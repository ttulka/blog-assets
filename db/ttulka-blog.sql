-- phpMyAdmin SQL Dump
-- version 4.7.7
-- https://www.phpmyadmin.net/
--
-- Host: localhost
-- Generation Time: Apr 14, 2020 at 11:28 AM
-- Server version: 10.3.18-MariaDB-0+deb10u1
-- PHP Version: 5.6.33-0+deb8u1

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `ttulka-blog`
--

-- --------------------------------------------------------

--
-- Table structure for table `Author`
--

CREATE TABLE IF NOT EXISTS `Author` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Author`
--

INSERT INTO `Author` (`id`, `name`) VALUES
(1, 'Tomas Tulka');

-- --------------------------------------------------------

--
-- Table structure for table `Category`
--

CREATE TABLE IF NOT EXISTS `Category` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(20) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Category`
--

INSERT INTO `Category` (`id`, `name`) VALUES
(1, 'Programming'),
(2, 'Miscellaneous');

-- --------------------------------------------------------

--
-- Table structure for table `Comment`
--

CREATE TABLE IF NOT EXISTS `Comment` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `createdAt` int(10) UNSIGNED NOT NULL,
  `author` varchar(50) DEFAULT NULL,
  `body` text DEFAULT NULL,
  `parentId` int(11) DEFAULT NULL,
  `postId` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `comment_parent_idx` (`parentId`),
  KEY `comment_post_idx` (`postId`)
) ENGINE=InnoDB AUTO_INCREMENT=19 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT;

--
-- Dumping data for table `Comment`
--

INSERT INTO `Comment` (`id`, `createdAt`, `author`, `body`, `parentId`, `postId`) VALUES
(1, 1563007000, 'Siddharth Zunjarrao', 'This page is indeed quite helpful for our case. We are looking at a scenario where we have two different Microsoft Active Directories and our application can talk to only one AD. I can see a scenario where one LDAP and one AD is tested. Has MyVD been tested for connecting two AD\'s and act as one single AD to provide authentication?', NULL, 35),
(2, 1564316000, 'ttulka', 'The best would be to ask the authors of MyVD directly: https://github.com/TremoloSecurity/MyVirtualDirectory/issues', 1, 35),
(3, 1574188024, 'daggerson', 'An Api will still be changed as the implementation provides a better insight into the problem. Up-front design doesn\'t work. ', NULL, 64),
(4, 1574228083, 'ttulka', 'I agree. API could be improved as more details are explored during implementation, that\'s why I still see *make it right* very important. The difference is how much. A well-defined API tends to be pretty stable while implementing right ahead requires a lot of refactoring afterwards. This could be problematic as not everybody enjoys deleting code.', 3, 64),
(5, 1575456507, 'Pablo', 'Using Spring MVC itself makes the application non-OO.  You\'re trying to solve unnecessary problems you actually caused.', NULL, 66),
(6, 1575474306, 'ttulka', 'I agree with the first part: Spring framework is not very object-oriented indeed. As it can\'t know your domain, it focuses on solving technical issues. MVC pattern heavily used by Spring is not an OO practice as it conceptually belongs to a higher level. MVC is an architectural pattern. Object-oriented is (should be!) in MVC only its \"M\" layer. In general, MVC works with anemic data rather than objects. On the other hand, MVC is well known and understood, and, when used with caution, brings more benefits than drawbacks.\n\nI do disagree with your conclusion. Even in a perfectly OO application, when crossing the boundaries you don\'t deal with objects but mere data. Consider an application which communicates with another one via JSON or XML. The same problem emerges. You don\'t save objects but data into the database, you don\'t print object but data on the screen, etc.', 5, 66),
(7, 1575917643, 'sleeping_dragon', 'You\'re mentioning messages. I see using DTOs much more practical here as the client can derive the structure inside the message from the DTO type declaration without any need to know the data structure in advance from the protocol specification or some kind of documentation. ', NULL, 66),
(8, 1575962652, 'ttulka', 'I would be careful with saying that the client doesn\'t need to know the data structure it consumes. The structure is a part of the contract which should be well defined, published and understood by each participant. Even with a strongly typed structure the client *must know* which data it consumes, otherwise it can\'t do anything useful with it. \n\nAs there are several kinds of messages, there is no general answer. Talking about domain events, it could be okay to have a reference to a domain object (better immutable) inside the message. DTOs are fine in a monolithic application where you can have a module with DTO definitions shared across the application (known as \"Shared Kernel\" in Domain-Driven Design). As far as the message is sent via network, it contains nothing but data. Using shared libraries in the microservice ecosystem is an antipattern called \"Distributed Monolith\" (and it\'s really an utter nightmare to deal with).', 7, 66),
(9, 1576326865, 'Gerald', 'Isn\'t putting the `save(...)` method into `Employee` actually against the SRP? I would expect it only in the repository: `Registry#save(Employee)`, and being called from the application code.', NULL, 68),
(10, 1576355891, 'ttulka', 'Partly, you might be right. Many authors really say that. But I always prefer not to follow principles blindly. Doing what you propose follows the SRP strictly, but leads to exposing object\'s internals (typically via getters) and breaks the principle of encapsulation. The Employee is no employee anymore, just employee\'s data. Encapsulation is for me a higher principle that the SRP; software design is about tradeoffs.\nAnother point is, that the implementation of `save` is actually not in the Employee class. The class provides only a facade for the actual implementation. This means, a change in the persistence mechanism, like using a different database, will not require a change in the Employee class. And that\'s what the SRP is all about. ', 9, 68),
(11, 1577819388, 'Bert', 'Hello! I am following this, but when trying to run I get the following error: \n\"User <user> is not authorized to create: topic://ActiveMQ.Advisory.Connection\"\nThis only happens when I add the authorizationPlugin.\nAny pointers? Thanks.', NULL, 2),
(12, 1579470944, 'skapral', '> The biggest problem with the blue book I have is that it focuses a lot on tactical design and building blocks like Entities, Services and Repositories. At the end of the day that is the only thing some readers get from the text ignoring the real value: the importance of communication with domain experts and understanding the business being built.\n\nYes, I had exactly the same feeling about \"the blue book\" back in the days. Loved the idea behind ubiquitous language and \"talking with business\" a lot. But later... repositories, services, entities... made me sad quickly. These procedural dinosaurs just ruin the idea of orientation on domain. People get concerned more on patterns then the business purpose.\n\nI like the post.', NULL, 69),
(13, 1579472842, 'skapral', 'I.... wouldn\'t be so straight on that. IMO the subject is more complicated.\n\n> Sharing code among different requirements is a bad idea, because business is volatile and the rules might change independently.\n\nIt\'s true that business is volatile. And it\'s deadly right to avoid reusing volatile things. But there is one part of a typical object (or unit) under test that is (supposed to be) stable --- it\'s API. And API is stable when it is bound not to software requirements but to the purpose, the mission of software. And the purpose, if correctly defined, is stable.\n\nWhy it matters? Stable API can be a basis of reusable assertions. For example: if we consider that Account interface from your post is stable (as its purpose is to check whether account could be logged in), one can define a reusable assertion, like \"assertAccountCanBeLoggedIn(account, password)\". And this assertion would be pretty reusable for tests related to each and every current and future implementations of Account.', NULL, 65),
(14, 1579472882, 'skapral', 'More on this idea: https://www.pragmaticobjects.com/chapters/003_reusable_assertions.html', 13, 65),
(15, 1579547023, 'ttulka', '> But there is one part of a typical object (or unit) under test that is (supposed to be) stable --- it\'s API.\n\nI fully agree on that. The test `registered_user_account_is_in_the_registry` is working with the fact, that `Account::register` is stable.\n\nWhat I mean with \"sharing code\" is sharing implementation code, concretely the implementation of the test. Creating a shared object in the test suite and using it among tests, for example.', 13, 65),
(16, 1579547097, 'ttulka', '> Stable API can be a basis of reusable assertions.\n\nUsing reusable assertions, as you proposed in your block post, is an interesting idea, but being really practical it requires some kind of framework (like https://github.com/pragmatic-objects/oo-tests) or a lot of boilerplate code. As the one important purpose of a test is to provide a documentation, any unnecessary code serves against this purpose. See the 1. point (It\'s not obvious how the test is set up) in my post: implementing the test as a composition of assertions requires knowledge of those assertions. And only from the API providers, but especially from the users (clients). \n\nUnfortunately, I see a long way to go. My attempt was about to make the first move in that direction.', 13, 65),
(17, 1586597478, 'Andy', 'Why not just to have `Products.cheaperThan(..)`? It\'d make the interface simpler and more straightforward...', NULL, 72),
(18, 1586614018, 'ttulka', 'Having the find method on the Products object is okay for small projects where is no need for Domain Collections. You can have methods like cheaperThan, byType, byCategory etc. returning list List<Product> or similar. \nIn bigger codebasis I see at least two problems with this approach:\n- One can\'t combine the find methods with Domain Collections methods. The find methods belong to a stateless service and a Domain Collection represents a stateful collection of objects. So this call would be valid but completely nonsense without calling any find method before: products.sortByPrice(). Such an object has two meanings, which is never a good idea. \n- Adding a new use-case for removing products, the same interface would have to be used (to be consistent). The object would then have too many responsibilities, which is bad for maintainability and usability of such an API.', 17, 72);

-- --------------------------------------------------------

--
-- Table structure for table `Post`
--

CREATE TABLE IF NOT EXISTS `Post` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `url` varchar(100) NOT NULL,
  `createdAt` int(10) UNSIGNED NOT NULL,
  `title` varchar(100) NOT NULL,
  `summary` text DEFAULT NULL,
  `body` text DEFAULT NULL,
  `isMenu` enum('true','false') NOT NULL DEFAULT 'false',
  `isDraft` enum('true','false') NOT NULL DEFAULT 'true',
  `authorId` int(11) NOT NULL,
  `categoryId` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `url` (`url`),
  KEY `post_author_idx` (`authorId`),
  KEY `post_category_idx` (`categoryId`)
) ENGINE=InnoDB AUTO_INCREMENT=74 DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Post`
--

INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(1, 'about', 1392920000, 'About me', '<p>Ahoy, I&nbsp;am <a href=\"http://ttulka.com\" target=\"_blank\">Tomas Tulka</a>, a software developer based in a&nbsp;small village in Germany. I&nbsp;write mostly Java, and I&nbsp;love JavaScript. Currently,  I\'m working for a big corporation on a&nbsp;medical software. I&nbsp;wrote my first production code when I&nbsp;was fourteen; unfortunately, I&nbsp;can\'t find it anymore.</p>\r\n<p>Welcome to my blog of wonderfully random thoughts, which came to&nbsp;my mind over the years. Feel free to&nbsp;look around, stay awhile, and make yourself at&nbsp;home.</p>\r\n<p>You can catch me on&nbsp;<a href=\"https://www.linkedin.com/in/tomas-tulka-15240158\" target=\"_blank\">LinkedIn</a>.</p>', '<p>Please feel free to comment and discuss!</p>', 'false', 'false', 1, 2),
(2, 'activemq-jaas-custom-login-module', 1393000000, 'ActiveMQ + JAAS Custom Login Module', '<p>It is pretty easy to find how to run built-in JAAS plugin (<code>jaasAuthenticationPlugin</code>), but what shall you do when you want to your own JAAS LoginModule implementation for ActiveMQ broker authentication (for instance when you have the module already written)?</p> \r\n<p>This article will help with this task.</p>', '<h2>ActiveMQ + JAAS <br /></h2> \r\n<p>ActiveMQ messaging broker has several options how to deal with authentication and authorization. </p> \r\n<p>The easiest one is to use the <code>simpleAuthenticationPlugin</code>, but it is not very flexible.</p> \r\n<p>Another option is the <code>jaasAuthenticationPlugin </code>using\r\n two property files (users and groups), quite effective solution but \r\nstill very clumsy when you consider that all the information about users\r\n and groups are stored into files.</p> \r\n<p>Following text will describe how to use customer-defined JAAS LoginModule as an ActiveMQ plugin.</p> \r\n<h2>JAAS Custom Login Module for ActiveMQ</h2> \r\n<h3>Authentication</h3> \r\n<p>Once we have developed a JAAS login module we can build it and pack as a JAR library. To use it in the broker we need to put the JAR on the classpath of a running ActiveMQ instance. There are two ways how to do it:</p> \r\n<ol> \r\n<li> Copy the JAR into lib folder of the broker -<code>ACTIVEMQ_HOME/lib/extra</code> or <code>ACTIVEMQ_HOME/lib/optional</code></li> \r\n<li>Put a path to the JAR to the java classpath of the broker - edit <code>ACTIVEMQ_HOME/bin/activemq</code> (Unix) or <code>ACTIVEMQ_HOME/bin/activemq.bat</code> (Win) and extend the <code>ACTIVEMQ_CLASSPATH</code> parameter:<br /> \r\n<ul> \r\n<li><code>ACTIVEMQ_CLASSPATH=&quot;${ACTIVEMQ_CLASSPATH};/var/lib/auth-test.jar&quot;</code> (Unix)</li> \r\n<li><code>set ACTIVEMQ_CLASSPATH=%ACTIVEMQ_CONF%;%ACTIVEMQ_BASE%/conf;%ACTIVEMQ_HOME%/conf;%ACTIVEMQ_CLASSPATH%;c:/Develop/Java/lib/auth-test.jar</code> (Win)</li> \r\n</ul> \r\n<ul> </ul> \r\n</li> \r\n</ol> \r\n<h4>\r\n\r\nBroker Setting to use the Plugin\r\n</h4> \r\n<p>Edit the <code>ACTIVEMQ_HOME/conf/login.config</code> property file to use the plugin as following (consider the plugin class is <code>cz.net21.ActiveMqLoginModule</code>):</p> \r\n<p> </p> \r\n<pre class=\"brush: plain\">MyLoginModule { \r\n  cz.net21.ActiveMqLoginModule&nbsp; required debug=true;\r\n};\r\n</pre> \r\n<p> </p> \r\n<p>Edit the <code>ACTIVEMQ_HOME/conf/activemq.xml</code> configuration file as following:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;broker … &gt; \r\n  …\r\n  &lt;plugins&gt;\r\n    &lt;jaasAuthenticationPlugin configuration=\"MyLoginModule\" /&gt;\r\n  …\r\n  &lt;/plugins&gt;\r\n  …\r\n&lt;/broker&gt;\r\n</pre> \r\n<p> </p> \r\n<h4>Running the java code</h4> \r\n<p>Now we can run the java to see that the JAAS plugin is working:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"tcp://localhost:61616\");\r\nConnection connection = connectionFactory.createConnection(username, password);\r\nconnection.start();\r\n</pre> \r\n<p> </p> \r\n<p>If the method <code>login()</code> from the module class <code>cz.net21.ActiveMqLoginModule</code> returns <em>false </em>for added credentials, the code will return by en exception:</p> \r\n<pre class=\"brush: plain\">java.lang.SecurityException: User name [testuser] or password is invalid.\r\nCaused by: javax.security.auth.login.LoginException: Login Failure: all modules ignored</pre> \r\n<h3>Authorization</h3> \r\n<p>In this part we will look at a simple way how to authorize an user for broker\'s resources (queues/topics).</p> \r\n<p>Via <code>authorizationPlugin </code>we can setup all the rights for queues/topics and users groups. <br /></p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;authorizationPlugin&gt;\r\n  &lt;map&gt;\r\n    &lt;authorizationMap&gt;\r\n      &lt;authorizationEntries&gt;\r\n        &lt;authorizationEntry queue=\"&gt;\"\r\n          read=\"admins\"&nbsp; write=\"admins\"&nbsp; admin=\"admins\" /&gt;\r\n  …\r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>We can leave this settings and control the access via our custom JAAS module. All we need is to add an <code>UserPrincipal </code>object for the broker user and\r\na <code>GroupPrincipals </code>object  for the broker groups.</p> \r\n<p>Let\'s extend our module to consume a file with groups used in the authorizationPlugin:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">MyLoginModule { \r\n  cz.net21.ActiveMqLoginModule required \r\n    debug=true\r\n    org.apache.activemq.jaas.properties.group=\"groups.properties\";\r\n};</pre> \r\n<p> </p> \r\n<p>Here we use <code>org.apache.activemq.jaas.properties.group</code>  as a name of an option and <code>groups.properties</code> as a name of the property file with a list of groups. </p> \r\n<p> </p> \r\n<pre class=\"brush: java\">public void initialize(Subject subject, CallbackHandler handler, Map&lt;String, ?&gt; state, Map&lt;String, ?&gt; options) {\r\n  this.subject = subject;\r\n  groupsFile = options.get(\"org.apache.activemq.jaas.properties.group\") + \"\";\r\n  …\r\n</pre> \r\n<p> </p> \r\n<p>But we can use for instance <code>groupListFile </code>as the name and instead of the property file use a comma-separated file with just a list of group names.</p> \r\n<p>We will process the file and bind the logged user with one of the groups. </p> \r\n<pre class=\"brush: java\">public boolean login() throws LoginException {\r\n  try {\r\n    File f = new File(baseDir, groupsFile);\r\n    groups.load(new java.io.FileInputStream(f));\r\n  } catch (IOException e) {\r\n    throw new LoginException(\"Unable to load group properties file \" + groupsFile);\r\n  }\r\n  …\r\n</pre> \r\n<p>Then we need to put the <code>UserPrincipal </code>and <code>GroupPrincipals </code>objects into the subject\'s principals:</p> \r\n<pre class=\"brush: java\">principals.add(new UserPrincipal(user));\r\nprincipals.add(new GroupPrincipal(groupName));\r\nsubject.getPrincipals().addAll(principals);\r\n</pre> \r\n<p>If the <code>groupName</code> match the destination we will get the access, otherwise the code will return by en exception:</p> \r\n<p> </p> \r\n<pre class=\"brush: plain\">java.lang.SecurityException: User guest is not authorized to read from: queue://TestQ\r\n</pre> \r\n<p> </p> \r\n<p> <br /></p> \r\n<p>Congratulation, we have a JAAS plugin for authentication and authorization of ActiveMQ broker!</p> \r\n<h2>Appendix <br /></h2> \r\n<p>I am working with Java 7, ActiveMQ 5.9.0</p> \r\n<p>Please see the discussed code <a href=\"/storage/ActiveMQ_JAAS_Custom_Login_Module.zip\" title=\"The example code\">in the attachment</a>. <br /></p> \r\n<p><br /></p>', 'false', 'false', 1, 1),
(3, 'activemq-hornetq-and-rabbitmq-performance-comparison', 1393268000, 'ActiveMQ, HornetQ and RabbitMQ Performance Comparison', '<p>Messaging could be a great solution for a lot of projects regarding an inter-systems (and components) communication. But which vendor to choose? Which one is the best? </p> \r\n<p>There is no proper answer for this question, because each and every provider has some pros and cons. Please read the first description of all of them and figure out only those matching your requirements. </p> \r\n<p>If your results include <strong>ActiveMQ</strong>, <strong>HornetQ </strong>and <strong>RabbitMQ</strong>, you are propably interested in performace and some practical observations now. And that is what is this article all about.\r\n\r\n</p>', '<h2>What is tested</h2> \r\n<p>We leave all the brokers in the default setting, no additional tuning was done yet. If you have some special requirement, you should probably look deeply at the features of the brokers and count in the final results.</p> \r\n<p>We run the same performance test for several scenarios:<br /></p> \r\n<ol> \r\n<li>One broker, one producer, one consumer on the only one server.</li> \r\n<li>One broker running on the server one, one producer, one consumer running on the server 2.</li> \r\n<li>Two brokers running on <strong>different </strong>servers (forwarding messages / bridging), one producer&nbsp; running on the server 1, one consumer running on the server 2.</li> \r\n</ol> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \r\n<tbody> \r\n<tr> \r\n<td><img alt=\"scenarios\" src=\"/storage/ActiveMQ_HornetQ_and_RabbitMQ_Performance_Comparison_2.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td style=\"text-align: center; padding-top: 3px;\"><em>Image 1:</em> Scenario 1, 2 and 3 architecture</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\nWe also run the test with different setting of clients:<br /> \r\n<ol> \r\n<li>queues, topics</li> \r\n<li>non-persistent, persistent massages</li> \r\n<li>one or ten threads for a queue/topic (one or ten message producers and consumers in parallel) </li> \r\n</ol>Because <strong>proportion of the results</strong> for the different scenarios is almost <strong>the same</strong>, we can focus only on the first scenario to invest the results and do the comparison. <br /> \r\n<h2>Results</h2>Parameters for the run:<br /> \r\n<ul> \r\n<li>non-persistent messages</li> \r\n<li>non-transacted</li> \r\n<li>sending messages in asynchronous mode</li> \r\n<li>receiving messages in asynchronous mode (AUTO_ACKNOWLEDGE)</li> \r\n<li>not encrypted</li> \r\n</ul> \r\n<p>Running on Linux system in virtual machine with 2 cores and 8 GB RAM.</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"3\" border=\"1\" class=\"resultsTable\" style=\"width: 701px;\"> <caption>Results table</caption> \r\n<tbody> \r\n<tr> \r\n<td> queues<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n</tr> \r\n<tr> \r\n<td> consumers (per queue)<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n</tr> \r\n<tr> \r\n<td>  producers (per queue)</td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n</tr> \r\n<tr> \r\n<td>message body size (bytes)<br /></td> \r\n<td> 128<br /></td> \r\n<td> 2048<br /></td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n</tr> \r\n<tr> \r\n<td><strong> ActiveMQ</strong> (msg/sec)</td> \r\n<td><em> 14590</em></td> \r\n<td><em> 14565</em></td> \r\n<td><em> 15700</em></td> \r\n<td><em> 15640</em></td> \r\n<td><em> 17920</em></td> \r\n<td><em> 17995</em></td> \r\n<td><em>  18180</em></td> \r\n<td><em>19220</em></td> \r\n</tr> \r\n<tr> \r\n<td><strong>HornetQ</strong> (msg/sec)</td> \r\n<td><em> 46605</em></td> \r\n<td><em> 47830</em></td> \r\n<td><em> 10140</em></td> \r\n<td><em> 10802</em></td> \r\n<td><em> 16165</em></td> \r\n<td><em> 18885</em></td> \r\n<td><em> 19852</em></td> \r\n<td><em> 19905</em></td> \r\n</tr> \r\n<tr> \r\n<td><strong>RabbitMQ</strong> (msg/sec)</td> \r\n<td><em> 20550</em></td> \r\n<td><em> 14680</em></td> \r\n<td><em> 14422</em></td> \r\n<td><em> 13065</em></td> \r\n<td><em> 13770</em></td> \r\n<td><em> 12150</em></td> \r\n<td><em>  13800</em></td> \r\n<td><em> 11930</em></td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \r\n<tbody> \r\n<tr> \r\n<td><img alt=\"results\" src=\"/storage/ActiveMQ_HornetQ_and_RabbitMQ_Performance_Comparison_1.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td style=\"text-align: center; padding-top: 3px;\"><em>Image 2:</em> Results chart</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<h2>My point of view</h2> \r\n<p><strong>ActiveMQ </strong>is very good tool with lot of documentation, examples, support and a huge community. Is it not the best regarding the performance, but it is announced to be rapidly improved by the version 6 using Appolo module. We will see...<br /> <br /></p> \r\n<p><strong>HornetQ </strong>really shines in the performance and it is pretty useful with a good support as well, but a lot of features (for instance the management console) is tightly&nbsp; bound with running as a JBoss module. Running in a stand-alone mode (as I did) has several disadvantages and I would not recomment it. But if you are using (or going to use) the JBoss Application Server for running you web application, HornetQ could the right option for you. </p> \r\n<p>There is a bit mess in versioning of libraries, especially if you want to work with different vendors of components involved in the integration (Netty, Spring JMS, ...).</p> \r\n<p>Queues and topics must be declared in hornetq-jms.xml before and cannot be created and deleted dynamically using JMS API. <br /> <br /></p> \r\n<p><strong>RabbitMQ</strong> does not have a good performance and it is not very human-friendly (all the setting are written in Erlang, which makes longer configurations very unclear for a programmer). </p> \r\n<p>There is only a commercial implementation of JMS. You can use pure RabbitMQ libraries or the Spring AMQP, but it is a different approach from JMS and has several limitations, for instance AMQP 0-9-1 protocol used as native doesn\'t support durable subscribers.</p> \r\n<p> <br /></p>', 'false', 'false', 1, 2),
(4, 'principals-from-jaas-through-cas-to-spring-security', 1394041000, 'Principals from JAAS through CAS to Spring Security', '<p>This article is about a custom integration of:</p> \r\n<ul> \r\n<li><strong>Java Authentication and Authorization Service (JAAS)</strong></li> \r\n<li><strong>Central Authentication Service (CAS)</strong></li> \r\n<li><strong>Spring Security</strong> module</li> \r\n</ul> \r\n<p>Let\'s imagine a situation we have  a developed username-password based JAAS Login Module implementing an authentication process.<br />The JAAS Login Module is used by CAS server as a authentication handler.<br />The CAS server is used by Spring Security via its CAS authentication provider plugin.</p> \r\n<p>So far it is easy to implement by a lot of existing manuals and HOWTOs using just provided libraries with no need to extend or modify a code.</p> \r\n<p>But what if the <strong>JAAS module contains also an authorization</strong>, implemented for instance by putting principals (roles) to a subject. To <strong>transmit the roles</strong> from JAAS module through CAS server to the Spring context we need to do some additional work as following.<br /></p>', '<p><em>Principal</em> is a term used by JAAS and CAS in a different meaning. Spring Security module uses rather a term <em>role</em>. We will use the term <em>a principal </em>or <em>a role </em>in the meaning of <strong>user\'s group</strong>.</p> \r\n<h2>Collaboration</h2> \r\n<p> The collaboration among nodes is shown in the picture:</p> \r\n<p><img src=\"/storage/Principals_from_JAAS_through_CAS_to_Spring_Security_1.png\" /><br /></p> \r\n<h2>JAAS Login Module</h2> \r\n<p> There is no big deal with JAAS, we need just to implement the <code>javax.security.auth.spi.LoginModule</code> interface and put our logic to authenticate a user.</p> \r\n<p>For authorization we will use our custom class <strong><code>CommonGroupPrincipal </code></strong>implementing the <code>java.security.Principal</code> interface.</p> \r\n<p>By some logic we assign roles to the user (as in the following very simple example) optimally in the commit method:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">principals.add(new CommonGroupPrincipal(\"ROLE_USER\"));\r\nif (\"admin\".equals(user)) {\r\n    principals.add(new CommonGroupPrincipal(\"ROLE_ADMIN\"));\r\n}\r\nsubject.getPrincipals().addAll(principals);</pre> \r\n<p> Now we can use the modul in the CAS server. All we need to do is run an application server (Tomcat for instance) containing the CAS WAR file with a Java option defining the JAAS configuration file location.<br />Modify the run script of the application server as following (for Tomcat, Windows):</p> \r\n<pre class=\"brush: plain\">set JAVA_OPTS=%JAVA_OPTS% -Djava.security.auth.login.config=%CATALINA_HOME%/conf/jaas.config</pre> \r\n<p>and create the configuration file: <br /></p> \r\n<pre class=\"brush: plain\">CAS {\r\n    ttulka.test.auth.jaas.module.CasLoginModule required;\r\n};</pre> \r\n<p>Alternatively the Java default java.security.auth.login.config could be used, or the Java option could be set directly from the code of <code>CustomJaasAuthenticationManager </code>(see below):</p> \r\n<pre class=\"brush: java\">System.setProperty(\"java.security.auth.login.config\", \"/path/to/jaas/config/file\");</pre> \r\n<h2>Spring Security CAS plugin</h2> \r\n<p>We can setup the Spring Security CAS plugin in a usual way, but we will implement our custom <code>authenticationUserDetailsService </code>property from the CAS authentication provider bean:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"casAuthenticationProvider\" class=\"org.springframework.security.cas.authentication.CasAuthenticationProvider\"&gt;\r\n	&lt;property name=\"authenticationUserDetailsService\"&gt;\r\n		&lt;bean class=\"ttulka.test.auth.spring.CustomAuthenticationUserDetailsService\" /&gt;\r\n	&lt;/property&gt;\r\n	...</pre> The class <code>CustomAuthenticationUserDetailsService </code>implements the <code>org.springframework.security.core.userdetails.AuthenticationUserDetailsService</code> interface and its only one method deals with an object representing a CAS response token:&nbsp;\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n<pre class=\"brush: java\">public class CustomAuthenticationUserDetailsService implements AuthenticationUserDetailsService&lt;CasAssertionAuthenticationToken&gt; {\r\n&nbsp;&nbsp; &nbsp;@Override\r\n&nbsp;&nbsp; &nbsp;public UserDetails loadUserDetails(CasAssertionAuthenticationToken token) throws UsernameNotFoundException {\r\n\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;final String groupList = (String)token.getAssertion().getPrincipal().getAttributes().get(\"CommonGroupPrincipal\");\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;...\r\n</pre> \r\n<p>The variable <code>groupList</code> is holding a <strong>roles list transmitted from the CAS server</strong> got from the JAAS module. We will use to <strong>build a list of the granted authorities</strong> returned as a part of the <code>UserDetails</code> object.</p> \r\n<h2>CAS server extension</h2> \r\n<p> To push a CAS to consume and transmit additional attributes as roles we need to implement our custom alternative of an <strong>authentication handler</strong> and its standard implementation by the class <strong><code>JaasAuthenticationHandler</code></strong> class. Unfortunately the desired method <code>authenticateUsernamePasswordInternal</code> is in the <code>JaasAuthenticationHandler</code> class defined as final, so we cannot just simply extend the class.</p> \r\n<p>To work with our custom handler we need to extend the CAS <strong>authentication manager</strong>, too. As in the previous class, the standard implementation <strong><code>AuthenticationManagerImpl</code></strong> is set as a final class, so we have to create a new one.</p> \r\n<h3>JAAS Authentication Handler</h3> \r\n<p>Alike the <code>JaasAuthenticationHandler </code>we will extend the class <code>org.jasig.cas.authentication.handler.support.AbstractUsernamePasswordAuthenticationHandler</code> and implement just the method <code>authenticateUsernamePasswordInternal</code>.</p> \r\n<p>Using the standard JAAS process we log in via credentials and get principals from the JAAS subject represented by <code>CommonGroupPrincipal</code> objects (see above).<br />From the list we create a comma-separated string and put it into a principals map by a key as the class name (a public accessible property of the handler class):</p> \r\n<pre class=\"brush: java\">public class CustomJaasAuthenticationHandler extends AbstractUsernamePasswordAuthenticationHandler {\r\n    ...\r\n    private Map&lt;String, Object&gt; principals = new HashMap&lt;&gt;();\r\n    ...\r\n    protected boolean authenticateUsernamePasswordInternal(final UsernamePasswordCredentials credentials)...\r\n        ...\r\n        loginContext = new LoginContext(realm, handler);\r\n        loginContext.login();\r\n	   \r\n        processPrincipals(loginContext.getSubject().getPrincipals());\r\n        ...\r\n    ...\r\n    private void processPrincipals(Set&lt;Principal&gt; principalsSet) {\r\n        ...\r\n        for (Principal p : principalsSet) {\r\n            if (p instanceof CommonGroupPrincipal) {\r\n                sb.append(p.getName());\r\n                ...\r\n            }\r\n        }\r\n        principals.put(CommonGroupPrincipal.class.getSimpleName(), sb.toString());\r\n        ...\r\n</pre> \r\n<p>The handler will be used directly by our new manager (see below) so needs no additional setting. <br />Alternatively we can define it in the CAS webapp configuration file (<code>WEB-INF/deployerConfigContext.xml</code>) as a new bean and then inject into the manager.</p> \r\n<p> </p> \r\n<h3>JAAS Authentication Manager</h3> \r\n<p>Alike the <code>AuthenticationManagerImpl</code> we will extend the class <code>org.jasig.cas.authentication.AbstractAuthenticationManager</code> and implement just the method <code>authenticateAndObtainPrincipal</code>.</p> \r\n<p>After the authentication succeeds we build the principal\'s attributes from the handler\'s principals and return:</p> \r\n<pre class=\"brush: java\">handler = new CustomJaasAuthenticationHandler();\r\n...\r\nPrincipal principal = new SimplePrincipal(((UsernamePasswordCredentials)credentials).getUsername(), handler.getPrincipals());\r\n\r\nreturn new Pair&lt;AuthenticationHandler,Principal&gt;(handler, principal);</pre> \r\n<p>In the CAS webapp configuration file (<code>WEB-INF/deployerConfigContext.xml</code>) we need to set the manager:</p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"authenticationManager\" class=\"ttulka.test.auth.cas.CustomJaasAuthenticationManager\" /&gt;</pre> \r\n<p>To include the attributes with roles in the response to the Spring Security plugin we need to <strong>allow the attribute name <code>CommonGroupPrincipal</code> </strong>for a registered service in <code>WEB-INF/deployerConfigContext.xml</code>:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"serviceRegistryDao\" class=\"org.jasig.cas.services.InMemoryServiceRegistryDaoImpl\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;property name=\"registeredServices\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;list&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;bean class=\"org.jasig.cas.services.RegexRegisteredService\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ... \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;property name=\"allowedAttributes\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;list&gt;\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;value&gt;CommonGroupPrincipal&lt;/value&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/list&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/property&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/bean&gt; \r\n            ...\r\n</pre> \r\n<p>The last step is to modify the response renderer JSP page <code>WEB-INF/view/jsp/protocol/2.0/casServiceValidatorSuccess.jsp</code> to contain the attributes: <br /></p> \r\n<pre class=\"brush: xml\">&lt;cas:serviceResponse xmlns:cas=\'http://www.yale.edu/tp/cas\'&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;cas:authenticationSuccess&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;cas:user&gt;${fn:escapeXml(assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.id)}&lt;/cas:user&gt;&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;     &lt;c:forEach var=\"attr\" items=\"${assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes}\"&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &lt;cas:${fn:escapeXml(attr.key)}&gt;${fn:escapeXml(attr.value)}&lt;/cas:${fn:escapeXml(attr.key)}&gt;\r\n    &nbsp;&nbsp;&nbsp; &lt;/c:forEach&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;/cas:attributes&gt; \r\n        ... \r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p> </p> \r\n<p>The response will then looks like: <br /></p> \r\n<pre class=\"brush: xml\">&lt;cas:serviceResponse xmlns:cas=\'http://www.yale.edu/tp/cas\'&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:authenticationSuccess&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:user&gt;admin&lt;/cas:user&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:CommonGroupPrincipal&gt;ROLE_USER,ROLE_ADMIN&lt;/cas:CommonGroupPrincipal&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/cas:authenticationSuccess&gt;\r\n&lt;/cas:serviceResponse&gt;\r\n</pre> \r\n<p>The responsed XML is then processed by the Spring\'s <code>CustomAuthenticationUserDetailsService</code> (see above).</p> \r\n<p>All the CAS-related classes must be packed as a JAR library and copied to <code>/WEB-INF/lib</code> folder of the CAS WAR application.<br /></p> \r\n<p> </p> \r\n<p>And the integration is ready to use!</p> \r\n<h2>Appendix</h2> \r\n<p>I am working with Java 7, CAS 3.5.2, Spring 3.2.4. and Spring Security module 3.2.0.</p> \r\n<p>Please see the discussed code <a href=\"/storage/Principals_from_JAAS_through_CAS_to_Spring_Security.zip\" title=\"The example code\">in the attachment</a>. </p>', 'false', 'false', 1, 1),
(5, 'synchronization-with-modification-of-the-lock-reference', 1395765000, 'Synchronization with Modification of the Lock Reference', '<p>...is very <strong>very bad practice</strong>, nevertheless it is not so rare to meet it:<br /></p> \r\n<p>In <strong>legacy sources</strong> I have found a really tricky code causing an occasional error.</p> \r\n<p>Well, you can say all around synchronization is tricky, but good understanding is a clue to eliminate the magic - this article might help a bit.</p>', '<p>Let\'s consider a very basic logic of the code: </p> \r\n<p><em>Thread-agents are dealing with a shared data of the singleton system.</em></p> \r\n<p>Alright, easy, the <strong>code working with the shared data must be synchronized</strong>.\r\n Yes, but don\'t forget that the threads (the agents) are working with \r\ndata of another class (the system) - we can not use synchronized methods\r\n as they are using the instance of an agent as a mutex instead of the \r\nclass the data belongs to. Well, let\'s use the shared system\'s object as\r\n a lock and... we are ready... wait a minute: This is exactly the \r\nsolution implemented in the legacy code!</p> \r\n<p>Where\'s the problem - this must work! Sure, but not in any case... <br /></p> \r\n<p>In a very simplified way the original code looks like this:</p> \r\n<pre class=\"brush: java\">public class SystemApp {\r\n \r\n&nbsp;&nbsp;&nbsp; static Integer count = 0;\r\n\r\n&nbsp;&nbsp;&nbsp; static class AgentThread extends Thread {\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; public void run() {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; try {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Thread.sleep(ThreadLocalRandom.current().nextInt(100));\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } catch (InterruptedException e) { }&nbsp;&nbsp;&nbsp; \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // do something\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // ...\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; synchronized (count) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; count ++;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // do something else\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // ...\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n \r\n&nbsp;&nbsp;&nbsp; public static void main(String[] argv) {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // run agents\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for (int i = 0; i &lt; 1000; i ++) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; new AgentThread().start();\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // sleep for a while to let agents finish their work\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; try {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Thread.sleep(5000);\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } catch (InterruptedException e) { }&nbsp;&nbsp;&nbsp; \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // print the result\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; System.out.println(count);\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<p>What is the expected result printed on the screen? 1000? Right!</p> \r\n<p>But guess what we get by running it five times:</p> \r\n<p>955<br />979<br />973<br />959<br />963</p> \r\n<p>Of course, the pain is in here:</p> \r\n<pre class=\"brush: java\">synchronized (count) {\r\n&nbsp;&nbsp;&nbsp; count ++;\r\n}</pre> \r\n<p>We are using as a lock an object referenced by the variable <em>count</em>. But this reference is changed inside the critical section. This means there is no guarantee that two or more parallel threads get the same lock object. And so sometimes happens this scenario: </p> \r\n<ol> \r\n<li>The thread A and the thread B are trying to enter the critical section.</li> \r\n<li>The thread A gets the access and the lock referenced by the variable <em>count </em>(currently for instance with a value of integer=2) gets locked up.</li> \r\n<li>The thread B is waiting for the release of the lock (integer=2).</li> \r\n<li>The thread A changes the reference of the variable <em>count </em>to the new value (integer=3) and releases the lock (integer=2).</li> \r\n<li>The thread B waiting for the lock (integer=2) gets that lock, enters the critical section and locks up the lock object (integer=2).</li> \r\n<li>The parallel thread C tries to enter the critical section with the lock object referenced by the variable <em>count </em>(integer=3 - changed by the thread&nbsp;A).</li> \r\n<li>The tread C gets the access to the critical section because its lock is now the object integer=3 while the thread B is being in the critical section as well (with the lock object integer=2).</li> \r\n</ol> \r\n<p>As you can see, by the seventh point we have <strong>two threads in the critical section</strong>.\r\n</p> \r\n<p>The lesson is: <u>never ever use a variable object as a lock!</u><br /></p> \r\n<p> <br /></p> \r\n<p> </p> \r\n<p>Synchronization is tricky and it is always good to think twice. </p> \r\n<p>By the way, using the API from the <em>java.util.concurrent</em> package is a good idea, too.\r\n</p> \r\n<p> <br /></p>', 'false', 'false', 1, 1),
(6, 'rmi-meets-jms', 1403112000, 'RMI meets JMS', '<p>A simple lightweight library for <strong>calling remote services via JMS</strong>.</p> \r\n<p>Library is using only JMS 1.1 API and it\'s Requestor and bytes messages.</p> \r\n<p>As simple as it can be, ready to use!<br /></p>', '<h2>How to use</h2> \r\n<p>Put the maven dependency into your POM file:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n   &lt;groupId&gt;cz.net21.ttulka&lt;/groupId&gt;\r\n&nbsp;&nbsp; &lt;artifactId&gt;rmi-meets-jms&lt;/artifactId&gt;\r\n&nbsp;&nbsp; &lt;version&gt;1.0.1&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Alternatively you can <a href=\"storage/rmi-meets-jms-1.0.1.jar\">download the library in the JAR file</a> and put it directly into your project classpath.</p> \r\n<h3>Remote service</h3> \r\n<p>The remote service is just a simple interface implemented by a class:</p> \r\n<pre class=\"brush: java\">public interface Service {\r\n \r\n&nbsp;&nbsp;&nbsp; Integer myMethod1(Integer i);\r\n&nbsp;&nbsp;&nbsp; void myMethod2(String str);\r\n}\r\n\r\npublic class ServiceImpl implements Service {\r\n\r\n&nbsp;&nbsp;&nbsp; public Integer myMethod1(Integer i) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; return i * 2;\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; public void myMethod2(String str) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; System.out.println(\"myMethod2 says: \" + str);\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<p>Methods of the service can be overloaded. <br /></p> \r\n<h3>Java code</h3> \r\n<p>To use the library directly from a java code you need to create a JMS factory and a destination queue:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">final QueueConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"tcp://localhost:61616\");\r\nfinal Queue queue = new ActiveMQQueue(\"MyQueue1\");</pre> \r\n<p>Then, on the <strong>server side</strong>, the remote service provider must be created with the service implementation as a parameter:</p> \r\n<pre class=\"brush: java\">final Service serviceImpl = new ServiceImpl();\r\n\r\nfinal RemoteServiceProvider provider = new RemoteServiceProvider(connectionFactory, queue, serviceImpl);</pre> \r\n<p>On the <strong>client side</strong> the remote service consumer must be created and then the remote service can be obtained:</p> \r\n<pre class=\"brush: java\">final RemoteServiceConsumer consumer = new RemoteServiceConsumer(connectionFactory, queue, Service.class);\r\n\r\nfinal Service service = (Service)consumer.getService();\r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>and the service can be called:</p> \r\n<pre class=\"brush: java\">Integer res1 = service.myMethod1(3);\r\nSystem.out.println(res1);               // will print \"6\"\r\n\r\nservice.myMethod2(\"Hello, server!\");    // will print \"Hello, server!\" on the server\'s console output</pre> \r\n<h3>Spring approach</h3> \r\n<p>The same can be achieved with the Spring framework very easily (as everything is easy with the Spring):</p> \r\n<pre class=\"brush: xml\">&lt;!-- Server side --&gt;\r\n&lt;bean id=\"server\" class=\"cz.net21.ttulka.rmimeetsjms.RemoteServiceProvider\"&gt;\r\n	&lt;constructor-arg ref=\"jmsFactory\" /&gt;\r\n	&lt;constructor-arg ref=\"queue\" /&gt;\r\n	&lt;constructor-arg ref=\"serviceImpl\" /&gt;\r\n&lt;/bean&gt;\r\n\r\n&lt;!-- Client side --&gt;\r\n&lt;bean id=\"client\" class=\"cz.net21.ttulka.rmimeetsjms.RemoteServiceConsumer\"&gt;\r\n	&lt;constructor-arg ref=\"jmsFactory\" /&gt;\r\n	&lt;constructor-arg ref=\"queue\" /&gt;\r\n	&lt;constructor-arg value=\"mypack.Service\" /&gt;\r\n&lt;/bean&gt;\r\n&lt;bean id=\"serviceProxy\" factory-bean=\"client\" factory-method=\"getService\" /&gt;\r\n\r\n&lt;!-- Service implementation --&gt;\r\n&lt;bean id=\"serviceImpl\" class=\"mypack.ServiceImpl\" /&gt;\r\n\r\n&lt;!-- JMS connection factory --&gt;\r\n&lt;bean id=\"jmsFactory\" class=\"org.apache.activemq.ActiveMQConnectionFactory\"&gt;\r\n	&lt;property name=\"brokerURL\" value=\"tcp://localhost:61616\" /&gt;\r\n&lt;/bean&gt;\r\n\r\n&lt;!-- JMS destination --&gt;\r\n&lt;bean id=\"queue\" class=\"org.apache.activemq.command.ActiveMQQueue\"&gt;\r\n	&lt;constructor-arg index=\"0\" value=\"MyQueue1\" /&gt;\r\n&lt;/bean&gt;</pre> \r\n<p>All you need to do is to initialize the Spring context:</p> \r\n<pre class=\"brush: java\">final ApplicationContext context = new ClassPathXmlApplicationContext(\"spring-context.xml\");</pre> \r\n<p>and get the service proxy on the client side:</p> \r\n<pre class=\"brush: java\">final Service service = (Service)context.getBean(\"serviceProxy\");</pre> \r\n<h2>Limitation <br /></h2> \r\n<p>There is only one limitation regarding the parameters of the remote service: all the <strong>parameters must be serializable</strong> (must implement <code>java.io.Serializable</code>).</p> \r\n<p><br /></p> \r\n<p>Have fun!</p> ', 'false', 'false', 1, 1),
(7, 'rmi-meets-jms-1-0-1-released', 1408898000, 'RMI meets JMS 1.0.1 Released!', '<p>New version of RMI meets JMS released! <br /></p>', '<p>The last release has a version number 1.0.1.</p> \r\n<p>It improves the library in meaning of <strong>performance </strong>(optimized process of serialization) and adds additional <strong>logging </strong>of error statuses.</p> \r\n<p> </p> \r\n<p> <br /></p>\r\n<p>Use and gain!</p> \r\n<p> <br /></p>', 'false', 'false', 1, 1),
(8, 'gradle-build-from-an-ant-script', 1415637000, 'Gradle Build from an Ant Script', '<p>Currently I am working on a redesign of a pretty complex project based on Ant builds. The task is to move whole concept into Gradle.</p> \r\n<p>I will continuously update this article how my work will be moving forward and put interesting issues and my solutions for them.</p> \r\n<p>If you have a better solution, please comment and discuss!<br /></p>', '<h2>Does a file exist?</h2> \r\n<p>In the Ant script I have found such a construct to check if a file exists:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;pathconvert property=\"MyFile.isPresent\" setonempty=\"false\"&gt;\r\n    &lt;path&gt;\r\n&nbsp;&nbsp;&nbsp;     &lt;fileset dir=\"src\" includes=\"MyFile-*.dat\" /&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/path&gt;\r\n&lt;/pathconvert&gt;\r\n</pre> \r\n<p>Obviously it checks the folder <em>src</em> for files with names of the filter <em>MyFile-*.dat</em>.</p> \r\n<p>We can do the same with Gradle\'s <code>FileCollection</code>:</p> \r\n<p> </p> \r\n<pre class=\"brush: groovy\">MyFile.isPresent = !fileTree(\"src\").include(\'MyFile-*.dat\').isEmpty();</pre> \r\n<p>\r\nThis will create a file filter on the folder and check if the match is empty, then set to a variable.</p> \r\n<h2>Substring </h2> \r\n<p>In the Ant script the macro was defined as a javascript:</p> \r\n<pre class=\"brush: xml\">&lt;scriptdef name=\"substring\" language=\"javascript\"&gt;</pre> \r\n<p>With Gradle you can easily implement it in java:</p> \r\n<pre class=\"brush: groovy\">def substring(String text, String regexp, int result) {\r\n&nbsp;&nbsp;&nbsp; java.util.regex.Pattern p = java.util.regex.Pattern.compile(regexp);\r\n&nbsp;&nbsp;&nbsp; java.util.regex.Matcher m = p.matcher(text);\r\n&nbsp;&nbsp;&nbsp; if (m.find( )) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; return m.group(result + 1);\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; return null;\r\n}</pre> \r\n<p>and use it:</p> \r\n<pre class=\"brush: groovy\">def version = substring(files.getAsPath(), \"executor-(.*)\\\\.msi\", 0)</pre> \r\n<p> </p> \r\n<h2> Custom builds</h2> \r\n<p>Because the result from the new Gradle build must be the same as from the old Ant build <strong>without changes of the project structure</strong>, we need to customize standard <code>java </code>plugin builds a bit.</p> \r\n<h3>Source sets</h3> \r\n<p>Java plugin provides a pretty high-level configuration element called <code>sourceSets</code>. You can change the sources location by setting the <code>sourceSets</code> up:</p> \r\n<pre class=\"brush: groovy\">sourceSets {\r\n&nbsp;&nbsp;&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; java {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; srcDirs = [\"mysource/mypkg\"]\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; exclude \"test/**\"\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<h3>Destination of the result classes</h3> \r\n<p>Compilation of the java classes in the <a target=\"_blank\" href=\"http://www.gradle.org/docs/current/userguide/java_plugin.html\"><em>java plugin</em></a> is done by the target called <code>compileJava</code>. If you want to change the destination of the compilation, can do it easily by rewriting the variable <code>destinationDir</code>:</p> \r\n<pre class=\"brush: groovy\">compileJava {\r\n&nbsp;&nbsp;&nbsp; destinationDir = file(\"myBuildDir\")&nbsp;&nbsp;&nbsp; // change the default dir for classes\r\n}\r\n</pre> \r\n<h3>Additional clean</h3> \r\n<p> If you want to put some additional actions into the standard clean task, do it easily:</p> \r\n<pre class=\"brush: groovy\">clean &lt;&lt; {\r\n&nbsp;&nbsp;&nbsp; // ... do some clean up\r\n}\r\n</pre> \r\n<p> </p> \r\n<h3>Compile task</h3> \r\n<p> In the old Ant script I have a compile task I need to keep. To make it work correctly together with the standard <code>java</code> plugin, make it dependent on the <code>classes</code> task, which is equivalent in the context:</p> \r\n<pre class=\"brush: groovy\">task compile(dependsOn: classes) {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // depends on the standard java task \"classes\"\r\n}\r\n</pre> \r\n<h3>Another build</h3> \r\n<p> If you want to have a build of different sources in the same Gradle file (of course in a different task), you can do it like this:</p> \r\n<pre class=\"brush: groovy\">sourceSets {\r\n&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; ...\r\n&nbsp; }\r\n&nbsp; compileApp2 {\r\n&nbsp;&nbsp;&nbsp; java {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDirs = [\"anothersource/mypkg2\"]\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"app/**\"\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}\r\n...\r\ntask compileApp2(type: JavaCompile, dependsOn: prepareApp2) {\r\n&nbsp;&nbsp;&nbsp; source = sourceSets.compileApp2.allSource.srcDirs\r\n&nbsp;&nbsp;&nbsp; destinationDir = file(\'app2BuildDir\')&nbsp;&nbsp;&nbsp; \r\n&nbsp; &nbsp; classpath = configurations.compile&nbsp; \r\n}\r\n</pre> \r\n<br>', 'false', 'false', 1, 1),
(9, 'memory-examiner', 1420478000, 'Memory Examiner', '<p>When you need to learn something by hear, you must repeat it again and again... and it is hard.</p> \r\n<p>This little program could help you!</p> \r\n<p> It is a simple test application for pairs of term - explanation, it could be used for technical definitions as well as words of a foreign language.<br /></p>', '<h2>Fill up the lexicon</h2> \r\n<p>First we need to define the lexicon of pairs term-explanation from which the application shall test us.</p> \r\n<p> We can do it in two easy ways:</p> \r\n<ol> \r\n<li>by the form in the bottom part of the application frame,</li> \r\n<li>putting the pairs as <strong>two lines of a text file</strong> (first line the term, second the explanation and so on) and import them via the application menu.</li> \r\n</ol> \r\n<p>The lexicon can be as well exported into a text file via the application menu, this will create two lines of text for each term-explanation pair. The file can be then edited and imported into the application and vise versa.</p> \r\n<h2>Test your memory!</h2> \r\n<p>The application will show you first the term and let you think. You can let show the explanation. Then you can move forward by clicking the green or red button if you had known the term or not.</p> \r\n<p>The application will server the most difficult words with priority. </p> \r\n<h3>Structure your tests</h3> \r\n<p>The application uses the file named <strong><code>lexicon.dat</code></strong> in the working directory as the storage. You can backup your lexicons in files and just rename the file with the demanding lexicon to <code>lexicon.dat</code> to push it into the application (the application has to be restarted to load the new lexicon file).</p> \r\n<p>Or you can use export - import functions to achieve this. But be aware that doing this you will loose you success-fail results!</p> \r\n<h2>Get it</h2> \r\n<p>You can download the <a href=\"/storage/memory-examiner-1.0.jar\">executable JAR archive</a> and run it by the command (if your OS is not configured to run JAR files on the JRE implicitly):</p> \r\n<p><code>java -jar memory-examiner-1.0.jar</code></p> \r\n<p> <br /></p> \r\n<p>And because this is a blog about programming, you can see the <a href=\"/storage/memory-examiner-1.0-sources.jar\">source codes</a> as well.</p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(10, 'managing-non-java-resources-from-gradle', 1424198000, 'Managing Non-Java Resources from Gradle', '<p>There are really complex projects which consist of a lot of heterogeneous parts, modules, batch scripts, binaries...</p> \r\n<p>Let\'s consider a project containing a module written in C... How to proceed in this case when we want to manage everything by the same way, compile from Gradle and version in a Maven repository?</p> \r\n<p>This article describes an easy way how to achieve this tough DevOps goal.<br /> </p>', '<h2>Building non-java resources with Gradle <br /></h2> \r\n<p>For building non-java sources there is a package of plugins for dealing with native binaries, this package is currently under incubating but seems to be pretty usable so far.</p> \r\n<p>Details can be found in the <a target=\"_blank\" href=\"https://gradle.org/docs/current/userguide/nativeBinaries.html\" title=\"Building native binaries\">Building native binaries</a> chapter of the official documentation. <a href=\"https://gradle.org/docs/current/userguide/nativeBinaries.html\" title=\"Building native binaries\"><br /></a></p> \r\n<p>There is a support for several languages like C, C++, Assembly, Windows resources, ...</p> \r\n<p>We will focus on C here, as the language differences are not the point of our issue.</p> \r\n<h3>C builds with Gradle</h3> \r\n<p>Applying the &quot;c&quot; Gradle plugin with a simple setting of source codes located in <code>src/main/c</code> and <code>src/main/c/headers</code> in the project folder:</p> \r\n<pre class=\"brush: groovy\">apply plugin: \'c\'\r\n\r\nexecutables {\r\n&nbsp; main { }\r\n}\r\n\r\nsources {\r\n&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; c {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDir \"src/main/c\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"**/*.c\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exportedHeaders { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDir \"src/main/c/headers\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"**/*.h\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}</pre> \r\n<p> This script will simply compile the sources in the source path and put the result (.exe file) into <code>build/binaries/mainExecutable</code> folder.</p> \r\n<p>More details can be found in the documentation (mentioned above).</p> \r\n<h2>Publish non-java resources into a Maven2 repository</h2> \r\n<p>For publishing we need to have an access to a Maven2 repository, for instance <a href=\"http://www.sonatype.org/nexus\" target=\"_blank\" title=\"Nexus server\">the Nexus server from Sonatype</a>.</p> \r\n<p>The goal here is to <strong>version the binaries in the repository</strong>, and what\'s more - we would like to make <strong>different variants of binaries</strong> (for instance each variant for a different platform).</p> \r\n<p>Using the Gradle <a title=\"Maven Publishing plugin\" target=\"_blank\" href=\"https://gradle.org/docs/current/userguide/publishing_maven.html\">Maven Publishing plugin</a> is a pretty straightforward way to do this:</p> \r\n<pre class=\"brush: groovy\">apply plugin: \'maven-publish\'\r\n\r\ngroup = \'cz.net21.ttulka\'\r\nversion = \'0.1\'\r\n\r\ntask myZip(type: Zip) { \r\n&nbsp; destinationDir = file(\'dist\') \r\n&nbsp; archiveName \'myC.zip\' \r\n&nbsp; from \'build/binaries/mainExecutable\' \r\n}\r\n\r\npublishing {\r\n&nbsp; publications {\r\n&nbsp;&nbsp;&nbsp; myPublicationName(MavenPublication) {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; artifact (myZip) {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classifier = \'win32\'\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}\r\n\r\nrepositories {\r\n&nbsp; maven {\r\n&nbsp;&nbsp;&nbsp; credentials { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; username \'xxx\' \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; password \'xyz\' \r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; url \'http://localhost:8081/nexus/content/repositories/MyMavenRep1/\'\r\n&nbsp; }\r\n}\r\n</pre> \r\n<p>Don\'t forget to set the project name in the <code>settings.gradle</code> configuration file, for instance:</p> \r\n<pre class=\"brush: groovy\">rootProject.name = \'my-project-with-native-binaries\'</pre> \r\n<p>In the example snippet we have used the <code>Zip </code>Gradle task to archive the binaries into a zip file, then we have used the archive as the input for the Maven publication and finally added the classifier (<code>classifier = \'win32\'</code>) to annotate this build variant as a Win32 platform-oriented. </p> \r\n<h2>Load the binaries as a dependency in an independent project</h2> \r\n<p>As far as we have compiled the sources and put the binaries into the repository, we want to deal with them from another project (or a different module).</p> \r\n<p> To distinguish the binary dependencies from the others, we can define a new Gradle configuration:</p> \r\n<pre class=\"brush: groovy\">configurations {\r\n&nbsp; binaries {}\r\n}</pre> \r\n<p>Dependencies are then marked by this configuration flag, don\'t forget that we used the zip archive and the classifier:</p> \r\n<pre class=\"brush: groovy\">dependencies { \r\n&nbsp; binaries \'cz.net21.ttulka:my-project-with-native-binaries:0.1:win32@zip\' \r\n}</pre> \r\n<p>That\'s it, now we can do whatever we want:</p> \r\n<pre class=\"brush: groovy\">// copy the dependencies into a \'deps\' folder\r\ntask getDeps(type: Copy) { \r\n&nbsp; from configurations.binaries \r\n&nbsp; into \'deps/\' \r\n}</pre> \r\n<p> <br /></p> \r\n<p>Hopefully this article gave you a little insight how to deal with different types of resources in one unified way with Gradle, and help you optimize your DevOps processes!<br /></p> \r\n<p> <br /></p>', 'false', 'false', 1, 1),
(11, 'xslt-multiple-xml-inputs', 1434551000, 'XSLT: Multiple XML Inputs', 'How to create one result document from more sources? It\'s easy with XSLT!', '<p>Let\'s imagine, that we have two (or more) XML documents as the data sources. In our example we will use a list of products as the first document and a list of attributes (colors, in this case) as the second document. Each product as a ID of a color as its attribute, the name of the color is in the list of colors. We would like to show all the products with their color names in a nice HTML table.</p> \r\n<h2>Data sources </h2> \r\n<p><em><strong>products.xml</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;products&gt;\r\n&nbsp; &lt;product id=\"1001\" color=\"A1\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Strawberry&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1002\" color=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Blueberry&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1003\" color=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Plum&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1004\" color=\"A3\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Green Apple&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&lt;/products&gt;</pre> \r\n<p><em><strong>colors.xml</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;colors&gt;\r\n&nbsp; &lt;color id=\"A1\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Red&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&nbsp; &lt;color id=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Blue&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&nbsp; &lt;color id=\"A3\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Green&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&lt;/colors&gt;</pre> \r\n<h2>XSLT Transformation</h2> \r\n<p><em><strong>transformation.xsl</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" exclude-result-prefixes=\"xsl\"&gt;&nbsp; \r\n \r\n&nbsp; &lt;xsl:param name=\"colorsFile\"/&gt;\r\n&nbsp; &lt;xsl:variable name=\"colorsDoc\" select=\"document($colorsFile)\"/&gt;\r\n \r\n&nbsp; &lt;xsl:template match=\"/\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;html&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;body&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;table border=\"1\" cellpadding=\"10\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;tr bgcolor=\"#accfff\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;th&gt;Name&lt;/th&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;th&gt;Color&lt;/th&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/tr&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:for-each select=\"products\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:apply-templates select=\"product\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/xsl:for-each&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &lt;/table&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/body&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/html&gt;\r\n&nbsp; &lt;/xsl:template&gt;\r\n \r\n&nbsp; &lt;xsl:template match=\"product\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;tr&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;td&gt;&lt;xsl:value-of select=\"name\"/&gt;&lt;/td&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;td&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:variable name=\"colorId\" select=\"@color\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:value-of select=\"$colorsDoc/colors/color[@id=$colorId]/name\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/td&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/tr&gt;\r\n&nbsp; &lt;/xsl:template&gt;\r\n \r\n&lt;/xsl:stylesheet&gt;</pre> \r\n<p>The parameter <code>colorsFile </code>takes its value from the processor (below), could be set to a file path like:</p> \r\n<pre class=\"brush: xml\">&lt;xsl:param name=\"colorsFile\" select=\"\'../input/colors.xml\'\" /&gt;</pre> \r\n<h2>Java processor</h2> \r\n<p>In this article we\'re using a Java processor, the solution can be but very easily converted to the pure XSLT solution and another processor (for instance an internet browser) can be used.</p> \r\n<p>All the classes are from the sub-packages of the package <code>javax.xml</code>. </p> \r\n<pre class=\"brush: java\">final DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\r\n\r\nfinal DocumentBuilder builder = factory.newDocumentBuilder();\r\nfinal Document document = builder.parse(\"products.xml\");\r\n\r\nfinal TransformerFactory tFactory = TransformerFactory.newInstance();\r\nfinal Transformer transformer = tFactory.newTransformer(new StreamSource(\"transformation.xsl\"));\r\n \r\ntransformer.setParameter(\"colorsFile\", \"colors.xml\");\r\n\r\ntransformer.transform(new DOMSource(document), new StreamResult(\"output.html\"));</pre> \r\n<p>And that\'s exactly what we wanted!</p> \r\n<p><strong><em>output.html</em></strong></p> \r\n<p> \r\n<table cellpadding=\"3\" border=\"1\"> \r\n<tbody> \r\n<tr bgcolor=\"#accfff\"> \r\n<th>Name</th> \r\n<th>Color</th> \r\n</tr> \r\n<tr> \r\n<td>Strawberry</td> \r\n<td>Red</td> \r\n</tr> \r\n<tr> \r\n<td>Blueberry</td> \r\n<td>Blue</td> \r\n</tr> \r\n<tr> \r\n<td>Plum</td> \r\n<td>Blue</td> \r\n</tr> \r\n<tr> \r\n<td>Green Apple</td> \r\n<td>Green</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> ', 'false', 'false', 1, 1),
(12, 'sql-null-tricky-equality', 1447269000, 'SQL NULL: tricky equality', '<p>Especially when creating a SQL quary from the code, to make the life easier we are using constructions like this:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE 1=1</pre> \r\n<p>Then we can easily add a new condition just joined with AND (or OR):</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE 1=1 AND ...</pre> \r\n<p>and it will be working perfectly fine.</p> \r\n<p>Working with constans can\'t bring any problem, but it\'s getting tricky when we\'re working with variables (columns) like this:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num=num</pre> \r\n<p>The problem is the special <em>value</em> NULL. <br /></p>', '<p>Why do we actually need <code>num = num</code> instead of <code>1 = 1</code>? Well it could be handy when we have a condition inside the query:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE val =&nbsp; IF(num &gt; 2, 3, val)</pre> \r\n<p>By this query we say: give me all the data in the <code>test</code> table where the value of the <code>val</code> column equals 3, in case that value of the <code>num</code> column is greater then 2, otherwise we don\'t care of the value of the <code>val</code> at all.</p> \r\n<p><em>Comment: the control flow function <code>IF</code> is RDBMS-related (<code>IF</code> is available in MySQL).</em></p> \r\n<p>In case when the val column contains NULL values, we can get unexpected behaviour (unexpected for us, but logic as we will see next).</p> \r\n<h2>Working with the NULL values</h2> \r\n<p>Let\'s consider a simple table:</p> \r\n<pre class=\"brush: sql\">CREATE TABLE test (num INT, str VARCHAR(100), str2 VARCHAR(100));</pre> \r\n<p>with different values containg NULL values:</p> \r\n<pre class=\"brush: sql\">INSERT INTO test VALUES (1, \'abc\', \'abc\'), (2, NULL, NULL), (NULL, \'3\', NULL), (NULL, NULL, NULL);</pre> \r\n<p>The table looks like following:</p> \r\n<p> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n<td style=\"width: 33%;\">&nbsp;3</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL <br /></td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n</tbody> \r\n</table><br /> \r\n</p> \r\n<p>Executing this query:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num = num;</pre> \r\n<p>will return</p> \r\n<p> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">abc <br /></td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<p>But it\'s very suspicious, because we have added four not two rows.</p> \r\n<p>NULL values can\'t be compared by the <code>=</code> equality operator. The same behaviour we can see in this example:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE str = str2;</pre> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">abc <br /></td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p>Instead of <code>=</code> operator we need to use <code>IS</code> operator:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num IS num;</pre> \r\n<p>will return the whole table.</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE str IS str2;</pre> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL <br /></td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p>&nbsp;</p> \r\n<p>NULL is an unknown value. As <code>NULL != NULL</code>, we can\'t do <code>num = NULL</code> nor <code>num != NULL</code> to get the expected results.</p> \r\n<p><strong>Always use <code>IS</code> (<code>IS NOT</code>) operators while working with NULL values.</strong><br /></p> ', 'false', 'false', 1, 1),
(13, 'javafx-2-simple-graphs', 1452103000, 'JavaFX 2: Simple Graphs', '<p>It\'s pretty easy to create a graphs in the JavaFX application, because there are a lot of neat libraries on the Internet.</p> \r\n<p>Trouble comes when you need to stick with JavaFX 2.</p> \r\n<p> This very very simple library will let you to create simple graphs like this one:</p> \r\n<p><img alt=\"Simple Graphs - preview\" src=\"/storage/simple-graphs-preview.png\" /><br /></p>', '<p>Let\'s consider a simple JavaFX 2 application:</p> \r\n<pre class=\"brush: java\">public class HelloWorld extends Application {\r\n\r\n&nbsp;&nbsp;&nbsp; private void init(Stage primaryStage) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // TODO\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; @Override\r\n&nbsp;&nbsp;&nbsp; public void start(Stage primaryStage) throws Exception {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; init(primaryStage);\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; primaryStage.show();\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; public static void main(String[] args) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; launch(args);\r\n&nbsp;&nbsp;&nbsp; }\r\n} </pre> \r\n<p>We will fill the init method to set up the scene showing the graph nodes from the picture above.</p> \r\n<p>First, we need the parent node:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">final BoxNode parent = new BoxNode(\"Parent\", 250, 0);</pre> \r\n<p> </p> \r\n<p>Then the children nodes:</p> \r\n<pre class=\"brush: java\">final BoxNode myNode1 = new BoxNode(\"MyNode 1\", 0, 200);\r\nfinal BoxNode myNode2 = new BoxNode(\"MyNode 2\", 300, 400);</pre> \r\n<p>Second, we connect the nodes with each other (the parent to children, the first child to the second one and also with itself):</p> \r\n<pre class=\"brush: java\">parent.addArrowTo(myNode1, \"arrow 1\");\r\nparent.addArrowTo(myNode2, \"arrow 2\");\r\nmyNode1.addArrowTo(myNode2, \"arrow 3\");\r\nmyNode1.addArrowTo(myNode1, \"arrow round\");<p></p></pre> \r\n<p>Finally, we all the nodes to a group (<code>javafx.scene.Group</code>) and the group to the scene:</p> \r\n<pre class=\"brush: java\">final Group root = new Group();\r\nprimaryStage.setScene(new Scene(root));\r\nroot.getChildren().addAll(parent, myNode1, myNode2);</pre> \r\n<p>That\'s all!</p> \r\n<p>The library is available as a <a href=\"/storage/simple-graphs-1.0.0.jar\">JAR</a> or <a href=\"https://github.com/ttulka/javafx2-simple-graphs\">source archive</a>.</p> ', 'false', 'false', 1, 1),
(14, 'javascript-menu-plugin-for-mobile-web-applications', 1454521000, 'JavaScript Menu Plugin for Mobile Web Applications', '<p>It\'s not easy to put all the navigation information in the small space of a website for mobile apps, especially when you have a lot of categories, departments etc...</p> \r\n<p>This JavaScript (<a href=\"https://jquery.com/download\" target=\"_blank\">jQuery</a>) based dynamic menu could help you with the challenge.</p> \r\n<p> It\'s small, compact, customizable, easy to deploy and free to use! <br /></p>', '<h2>Live Example</h2> \r\n<p>Example page optimized for the mobile devices with the menu is <a target=\"_blank\" href=\"http://www.net21.cz/mobile-menu\">here</a>.</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" style=\"width: 100%;\"> \r\n<tbody> \r\n<tr> \r\n<td style=\"width: 33%;\"> <img border=\"1\" alt=\"Menu for Mobile Web Apps #1\" src=\"/storage/mobilMenu_screenshot_1.png\" /></td> \r\n<td style=\"width: 33%;\"> <img border=\"1\" alt=\"Menu for Mobile Web Apps #2\" src=\"/storage/mobilMenu_screenshot_2.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td valign=\"top\"><br /></td> \r\n<td valign=\"top\"><br /></td> \r\n</tr> \r\n</tbody> \r\n</table><img border=\"1\" src=\"/storage/mobilMenu_screenshot_3.png\" alt=\"Menu for Mobile Web Apps #3\" /><br /> \r\n</p> \r\n<h2>Get started</h2> \r\n<p>Dowload the <a href=\"http://www.net21.cz/mobile-menu/mobileMenu.js\" target=\"_blank\">Menu jQuery Plugin</a> (<a href=\"http://www.net21.cz/mobile-menu/mobileMenu.min.js\" target=\"_blank\">compressed</a>) or the <a href=\"http://www.net21.cz/mobile-menu/mobileMenu.zip\">this whole example</a>.</p> \r\n<p>To deploy the menu on your website you need to include jQuery and the plugin scripts:</p> \r\n<pre class=\"brush: javascript\">&lt;script type=\"text/javascript\" src=\"http://code.jquery.com/jquery-1.12.0.min.js\"&gt;&lt;/script&gt;\r\n&lt;script type=\"text/javascript\" src=\"mobileMenu.min.js\"&gt;&lt;/script&gt;</pre> \r\n<p>Then to connect a start point on the page (typically a button or a text block) with the plugin:</p> \r\n<pre class=\"brush: javascript\">&lt;script type=\"text/javascript\"&gt;\r\n$(document).ready(function() {\r\n&nbsp; $(\"#mobileMenuStartButton\").mobileMenu(\"url/to/data.json\");\r\n});\r\n&lt;/script&gt;</pre> \r\n<h3>Data structure <br /></h3> \r\n<p>As the first parameter of the <code>mobileMenu</code> plugin you have to set an URL to a JSON file containing the menu data in the structure like following:</p> \r\n<pre>{\r\n&nbsp; \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.1\", \"link\" : \"link1.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2.1\", \"link\" : \"link1.2.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2.2\", \"link\" : \"link1.2.2\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.3\", \"link\" : \"link1.3\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 2\", \"link\" : \"link2\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3.1\", \"link\" : \"link3.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3.2\", \"link\" : \"link3.2\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]} \r\n&nbsp; ]\r\n}</pre> \r\n<p>As showed in the example data above, the structure starts with the <code>menu</code> record with a value of a list of menu items. Every menu item has a name and a link or a sub-menu of the same structure recursively.</p> \r\n<h3>Custom styling</h3> \r\n<p>The menu component contains several CSS classes which can be styled by the user.</p> \r\n<p> The easiest way is to download the <a href=\"view-source:http://www.net21.cz/mobile-menu/mobileMenu.css\" target=\"_blank\">example CSS style sheet</a> and adjust it.</p> \r\n<p>The important CSS are following:</p> \r\n<ul> \r\n<li><strong><code>mobileMenuButton </code></strong><br />The starter element will get this class additionaly. Together with the class <code>expanded</code> indicates the opened menu.<br /> <br /></li> \r\n<li><strong><code>mobileMenuMain</code></strong><br />The wrapper for the whole menu context. It\'s important to set atributte <code>position:absolute</code> for this to make the menu overflowing the context of the page below.<br /> <br /></li> \r\n<li><strong><code>mobileMenuBlock</code></strong><br />The container for the menu items context.<br /> <br /></li> \r\n<li><strong><code>mobileMenuItem</code></strong><br />The class for the menu items, has following additional classes:<br /> <br /></li> \r\n<ul> \r\n<li><code>title</code><br />The menu item showing the name of the currently expanded class.<br /> <br /></li> \r\n<li><code>first</code><br />The first item in the first menu list (without title item).<br /> <br /></li> \r\n<li><code>link</code><br />Indicates that the menu item has no sub-menu but it\'s only a simple link.<br /></li> \r\n</ul> \r\n</ul> \r\n<p> <br /></p>', 'false', 'false', 1, 2),
(15, 'upgrade-java-se-7-to-java-se-8-ocp-programmer-summary', 1468344000, 'Upgrade Java SE 7 to Java SE 8 OCP Programmer - Summary', '<p>Maybe you are updating your Oracle Professional Certification from Java 7 to Java 8 as I just did a few days ago.</p> \r\n<p>This summary contains everything important to learn to achive this goal.</p> \r\n<p>You can <a href=\"/storage/Upgrade_Java_SE_7_to_Java_SE_8_OCP_Programmer_-_Summary.pdf\">download this summary</a> as a PDF document.</p> \r\n<p>I wish you good luck!</p>', '<p> </p> \r\n<p> <br /></p> \r\n<p>Of course that this summary it\'s not enought, if you are beginning to learn all the new stuff in the Java 8.</p> \r\n<p>For deeper study I can recoment you a few things:</p> \r\n<ul> \r\n<li> OCP: Oracle Certified Professional Java SE 8 Programmer II Study Guide: Exam 1Z0-809<br /><a target=\"_blank\" href=\"https://www.amazon.de/OCP-Certified-Professional-Programmer-1Z0-809/dp/1119067901\">https://www.amazon.de/OCP-Certified-Professional-Programmer-1Z0-809/dp/1119067901<br /></a> <br /></li> \r\n<li>Mock Exams from Enthuware<br /><a target=\"_blank\" href=\"http://enthuware.com/index.php/mock-exams/oracle-certified-professional/ocpjp-8-1z0-810-questions\">http://enthuware.com/index.php/mock-exams/oracle-certified-professional/ocpjp-8-1z0-810-questions<br /></a><br /></li> \r\n<li>Pu-erh Dark Tea - good for the memory and whole body fitness<br /><a href=\"https://en.wikipedia.org/wiki/Pu-erh_tea\">https://en.wikipedia.org/wiki/Pu-erh_tea</a> <br /></li> \r\n</ul> \r\n<p> </p> \r\n<p><br />If you find some mistakes or have some comments to the summary, please feel free to discuss!</p> \r\n<p><br /></p>', 'false', 'false', 1, 2),
(16, 'pre-processing-spring-beans-of-prototype-scope', 1470586000, 'Pre-processing Spring Beans of Prototype Scope', '<p>Sometimes you have a type of bean in the <strong>Spring framework</strong> which you want to create by every asking for it - in the <strong>scope prototype</strong>.</p> \r\n<p>Pre-processing of those <strong>prototypes</strong> only one and in the beginning &nbsp;could be tricky.</p> \r\n<p>Let\'s consider an annotation which tell you someting about the setting of the beans. For example which alternative names is your GUI box connected to. You want to create a proper bean based on its alternative name of course without featching all the beans every time and looking for the right one (of course you can do it by lazy caching, but that\'s still not the best solution). Or what if you want to get a list of all the alternative names of course <u>withou the need to create the beans</u> first?</p> \r\n<p>This short article will tell you how to do it.&nbsp;</p>', '<p>Consider the following annotations:</p> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Documented\r\n@Component\r\n@Scope(value = SCOPE_PROTOTYPE)\r\npublic @interface Box {\r\n}</pre> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Documented\r\n@Component\r\n@Scope(value = SCOPE_PROTOTYPE)\r\npublic @interface AlternativeNames {\r\n&nbsp; &nbsp; AlternativeName[] value();\r\n}</pre> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Repeatable(AlternativeNames.class)\r\n@Documented\r\npublic @interface AlternativeName {\r\n&nbsp; &nbsp; String value();\r\n}</pre> \r\n<p>Then you have defined the base annotation <code>@Box </code>which tell the Spring framework that this class is a bean, <code>@AlternativeNames</code> which allows you to put the <code>@AlternativeName</code> annotation more times (<em>Repeating Annotations</em>).</p> \r\n<p>Based on the definitions you can create a class like this:</p> \r\n<pre class=\"brush: java\">@Box\r\n@AlternativeName(\"my-box\")\r\n@AlternativeName(\"box-of-mine\")\r\npublic class MyBox {\r\n&nbsp; &nbsp; // ... some code goes here\r\n}</pre> \r\n<p>Now it\'s time for the pre-processor:</p> \r\n<pre class=\"brush: java\">@Component\r\npublic class BoxContextListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; {\r\n\r\n&nbsp; &nbsp; private final ConfigurableListableBeanFactory factory;\r\n\r\n&nbsp; &nbsp; @Autowired\r\n&nbsp; &nbsp; public BoxContextListener(ConfigurableListableBeanFactory factory) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; this.factory = factory;\r\n&nbsp; &nbsp; }\r\n\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void onApplicationEvent(ContextRefreshedEvent event) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; String[] beanDefinitionNames = event.getApplicationContext().getBeanDefinitionNames();\r\n&nbsp; &nbsp; &nbsp; &nbsp; for (String beanDefinitionName : beanDefinitionNames) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BeanDefinition beanDefinition = factory.getBeanDefinition(beanDefinitionName);\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String originalClassName = beanDefinition.getBeanClassName();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClassName != null &amp;&amp; !originalClassName.isEmpty()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; final Class&lt;?&gt; originalClass = Class.forName(originalClassName);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClass.isAnnotationPresent(Box.class)) {\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClass.isAnnotationPresent(AlternativeNames.class) \r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; || originalClass.isAnnotationPresent(AlternativeName.class)) { \r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; AlternativeName[] mappings = originalClass.getAnnotationsByType(AlternativeName.class);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (AlternativeName alternativeMapping : mappings) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String alternativeName = alternativeMapping.value();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // do someting with the alternative name ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (ClassNotFoundException e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (BeansException e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>That\'s the way how to get values from the annotation of the Spring prototype beans in the pre-processing.</p> \r\n<p>I hope it helped!</p>', 'false', 'false', 1, 1),
(17, 'resolving-a-generic-type-with-the-spring-framework', 1472312000, 'Resolving a Generic Type with the Spring Framework', '<div>In the case of generic beans sometimes you need to get the <strong>generic type value</strong> for some specific reasons. A typical example could be parsing some data into the type.</div> \r\n<div><br /></div> \r\n<div>Following code shows you how to parse a JSON data into the specific structure defined by a generic class.</div>', '<pre class=\"brush: java\">import com.fasterxml.jackson.databind.ObjectMapper;\r\nimport org.springframework.core.GenericTypeResolver;\r\n// ...\r\n\r\n@Component \r\npublic class MyDataBean&lt;T extends MySuperData&gt; {\r\n\r\n    private final Class&lt;T&gt; genericType;\r\n\r\n    private final ObjectMapper objectMapper = new ObjectMapper();\r\n\r\n    public MyDataBean() {\r\n        this.genericType = (Class&lt;T&gt;) GenericTypeResolver.resolveTypeArgument(getClass(), MyDataBean.class);\r\n    }\r\n\r\n    public T parseData(String jsonData) {\r\n    	return objectMapper.readValue(jsonData, this.genericType);\r\n    }\r\n}\r\n</pre> \r\n<div> \r\n<p>Spring tool <code>GenericTypeResolver</code> will set the <code>genericType</code> class value which is later used as the parameter for the JSON parsing.</span></p> \r\n<p>It\'s easy!</p> \r\n<p> </p> \r\n</div>', 'false', 'false', 1, 1),
(18, 'ant-script-to-copy-a-snippet-of-a-xml-to-another-xml', 1475855000, 'Ant Script to Copy a Snippet of a XML to Another XML', '<p>I know, Ant is not the most modern technology, but there are still Ant-based systems we have to maintenance.</p> \r\n<p>Sometimes we need to cut or copy a piece of a XML document and save it to another.</p> \r\n<p>We will use <a href=\"http://www.oopsconsultancy.com/software/xmltask/\" target=\"_blank\">XmlTask</a> library by OOPS Consultancy to achive this goal.&nbsp;</p>', '<p>First of all, we have to define an Ant task:</p> \r\n<p>&lt;taskdef name=&quot;xmltask&quot; classname=&quot;com.oopsconsultancy.xmltask.ant.XmlTask&quot; /&gt;</p> \r\n<p>Don\'t forget to put the downloaded library into the Ant libs folder or onto the classpath:</p> \r\n<p><code>ant ... -lib xmltask.jar</code></p> \r\n<p>Let\'s consider two XML configuration files:</p> \r\n<p><code><strong>conf.xml</strong></code></p> \r\n<pre class=\"brush: xml\">&lt;configuration&gt;\r\n&nbsp; &lt;applications&gt;\r\n&nbsp; &nbsp; &lt;app id=\"MyCoolApp\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"AnotherApp\" /&gt;\r\n&nbsp; &lt;/applications&gt;\r\n&lt;/configuration&gt;&nbsp;</pre> \r\n<p><code><strong>settings.xml</strong></code></p> \r\n<pre class=\"brush: xml\">&lt;settings&gt;\r\n...\r\n&nbsp; &lt;apps&gt;\r\n&nbsp; &nbsp; &lt;app id=\"thisAppWasAlreadyHere\" /&gt;\r\n&nbsp; &lt;/apps&gt;\r\n&lt;/settings&gt;</pre> \r\n<p>And we want to merge those documents in the way we get this content of the <code>settings.xml</code>:</p> \r\n<pre class=\"brush: xml\">&lt;settings&gt;\r\n&nbsp; ...\r\n&nbsp; &lt;apps&gt;\r\n&nbsp; &nbsp; &lt;app id=\"thisAppWasAlreadyHere\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"MyCoolApp\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"AnotherApp\" /&gt;\r\n&nbsp; &lt;/apps&gt;\r\n&lt;/settings&gt;</pre> \r\n<p>We can use the xmltask\'s command call to get a path-defined snippet into the buffer and append it with the command insert.</p> \r\n<p>Here is the whole code:&nbsp;</p> \r\n<pre class=\"brush: xml\">&lt;xmltask source=\"conf.xml\"&gt;\r\n&nbsp; &lt;call path=\"configuration/applications/app\" buffer=\"apps_storedXml\"&gt;\r\n&nbsp; &nbsp; &lt;param name=\"id\" path=\"@id\" /&gt;\r\n&nbsp; &nbsp; &lt;actions&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;echo&gt;Merge the app id=\"@{id}\"&lt;/echo&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;xmltask source=\"settings.xml\" dest=\"settings.xml\"&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;insert path=\"settings/apps\" buffer=\"apps_storedXml\" /&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/xmltask&gt;\r\n&nbsp; &nbsp; &lt;/actions&gt;&nbsp;\r\n&nbsp; &lt;/call&gt;\r\n&lt;/xmltask&gt;</pre> \r\n<p>That\'s it!</p> \r\n<p> </p>', 'false', 'false', 1, 1),
(19, 'ant-sequential-tasks', 1479229000, 'Ant Sequential Tasks', '<p>This article will show you how to implement an Ant task which consume a sequence of sub-task and call them with a parameter of the result from the execution.</p> \r\n<p>As an example we can consider a task <strong>taking a path to a directory</strong> as a parameter, <strong>fetching files</strong> in the directory and <strong>running a sequence of sub-task for each file</strong>.&nbsp;</p> \r\n<p>The task will be implemented as a <strong>Java library</strong>.&nbsp;</p>', '<p>Considering the example, we will have in our Ant script something like:</p> \r\n<pre>&lt;fetch-files path=\"/Windows\" suffix=\".exe\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;echo&gt;My file print: @{file}&lt;/echo&gt;\r\n\r\n        &lt;antcall target=\"process-file\"&gt;\r\n            &lt;param name=\"filename\" value=\"@{file}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/fetch-files&gt;\r\n</pre> \r\n<p>This should process all the <code>.exe</code> files in the <code>/Windows</code> directory and print the name of the file before processing.&nbsp;</p> \r\n<h2>Implementation in Java&nbsp;</h2> \r\n<p>Let\'s create a new Maven project, all we need is a dependency to the Ant API:</p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.ant&lt;/groupId&gt;\r\n    &lt;artifactId&gt;ant&lt;/artifactId&gt;\r\n    &lt;version&gt;1.9.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n</pre> \r\n<h3>Sequential Task&nbsp;</h3> \r\n<p>First of all we will create an abstract class which allows us to put the <code>&lt;sequential&gt;</code> part into the task:</p> \r\n<pre class=\"brush: java\">abstract class SequentialTask extends Task {\r\n\r\n    private MacroDef macroDef;\r\n    private Target owningTarget;\r\n\r\n    abstract String getAttributeName();\r\n\r\n    @Override\r\n    public void setOwningTarget(Target owningTarget) {\r\n        this.owningTarget = owningTarget;\r\n    }\r\n\r\n    public Object createSequential() {\r\n        macroDef = new MacroDef();\r\n        macroDef.setProject(getProject());\r\n\r\n        MacroDef.Attribute attribute = new MacroDef.Attribute();\r\n        attribute.setName(getAttributeName());\r\n        macroDef.addConfiguredAttribute(attribute);\r\n\r\n        return macroDef.createSequential();\r\n    }\r\n\r\n    void executeSequential(String attrValue) {\r\n        MacroInstance instance = new MacroInstance();\r\n        instance.setProject(getProject());\r\n        instance.setOwningTarget(owningTarget);\r\n        instance.setMacroDef(macroDef);\r\n        instance.setDynamicAttribute(getAttributeName(), attrValue);\r\n        instance.execute();\r\n    }\r\n}</pre> \r\n<p>All the task exending the class must define the declared <code>getAttributeName()</code> method which returns the name of the attribute (<code>&quot;file&quot;</code> in our example).</p> \r\n<p>Then we cann call the method <code>executeSequential(String attrValue)</code> for each item in the task result.</p> \r\n<h3>Fetching Files Task&nbsp;</h3> \r\n<p>Our concrete task implementation could look like:</p> \r\n<pre class=\"brush: java\">public class FetchFilesTask extends SequentialTask {\r\n\r\n    private static final String ATTR_NAME = \"file\";\r\n\r\n    private String path;\r\n    private String suffix;\r\n\r\n    public void setPath(String path) {\r\n        this.path = path;\r\n    }\r\n\r\n    public void setSuffix(String suffix) {\r\n        this.suffix = suffix;\r\n    }\r\n\r\n    @Override\r\n    String getAttributeName() {\r\n        return ATTR_NAME;\r\n    }\r\n\r\n    @Override\r\n    public void execute() { \r\n        if (path == null || path.trim().isEmpty()) {\r\n            throw new BuildException(\"Parameter \'path\' must be specified.\");\r\n        }\r\n\r\n        List&lt;String&gt; filesList = getFilesList(Paths.get(path), suffix);\r\n\r\n        for (String file : filesList) {\r\n            executeSequential(file.toString());\r\n        }\r\n    }\r\n\r\n    List&lt;String&gt; getFilesList(Path sourcePath, String suffix) {\r\n        final List&lt;String&gt; toReturn = new ArrayList&lt;&gt;();\r\n\r\n        try (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(sourcePath)) {\r\n            for (Path file: stream) {\r\n                if (Files.isRegularFile(file)) {\r\n                    if (suffix == null || file.toString().endsWith(suffix)) {\r\n                        toReturn.add(file.toString());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        catch (IOException | DirectoryIteratorException e) {\r\n            throw new BuildException(\"Error by reading \'\" + sourcePath + \"\'.\", e);\r\n        }\r\n        return toReturn;\r\n    }\r\n}</pre> \r\n<p>The class extends the <code>SequentialTask</code>, defines its abstract method and use the <code>executeSequential</code> method. </p> \r\n<p>Additionaly defines the class two task parameter&nbsp;<code>path</code> and&nbsp;<code>suffix</code> by the defining the <em>getters </em>for them.&nbsp;</p> \r\n<h2>Using the task in an Ant script</h2> \r\n<p>To call the task in an Ant script as showned in the example we have to provide the JAR library and define the task by its class:</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"fetch-files\" classname=\"cz.net21.ttulka.ant.FetchFilesTask\" classpath=\"AntTasks-1.0.jar\" /&gt;</pre> \r\n<p> </p> \r\n<p>You can download the <a href=\"/storage/AntTasks-1.0.jar\">compiled JAR library</a> or whole <a href=\"https://github.com/ttulka/ant-sequential-tasks\">project sources</a>.</p> \r\n<p>Have fun!</p>', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(20, 'application-package-manager-with-ant-and-java', 1484074000, 'Application Package Manager with Ant and Java', '<p>\r\nEveryone knows the operation systems package manager like&nbsp;<em>dpkg </em>(from Debian) or&nbsp;<em>RPM Package Manager</em> (from RedHat).</p> \r\n<p>Sometimes there is a need of such a management system in our own use.</p> \r\n<p>The environment doesn\'t have to be an operation system, but for instance a<strong>&nbsp;web server</strong> or just a <strong>container application</strong>.</p> \r\n<p>The manager must be able to manage package <strong>dependencies</strong>, <strong>versions </strong>and <strong>installation </strong>to the environment system.</p>', '<p>All the information must be contained only inside the application itself, the system nor the manage <strong>must not</strong> have any additional data about a concrete package.</p> \r\n<p>To distinguish between a package and a package manager we will use in our context terms <em>Application Package</em> (<strong>AP</strong>) and <em>Application Package Manager</em> (<strong>APM</strong>).</p> \r\n<h2><em>Scenario One</em>: Container Application</h2> \r\n<p>We want to develop a container application which on the starup checks the application folder and install included application if not installed yet.</p> \r\n<p>Consider an application which simulates an operation system. It has a core, folder for configuration, run and so one. In the simplest case:</p> \r\n<pre>/applications/\r\n/system/\r\n       /conf/\r\n            /applications.conf\r\n       /core/\r\n            /lib/\r\n                /apm.jar\r\n                /system.jar\r\n            /apm.xml\r\n       /data/\r\n       /start.bat</pre> \r\n<p>The file <code>conf/applications.conf</code> contains pairs of <code>application-name=files-in-bin-directory-comma-separated</code>. It\'s empty with a brand new system installation or contains some initial system-provided application and files.</p> \r\n<p>The system calls the APM (<code>core/apm.xml</code>) after the <code>start.bat</code> script is executed. The APM takes care of the applications installation. Then a system program (from the library <code>core/lib/system.jar</code>) goes thru the <code>conf/application.conf</code> and run each application logic (in our case only prints the content of a file defined by the application).</p> \r\n<p>The <code>applications</code> directory stands outside the <code>system</code> directory, it means even when the system is completely updated remains the <code>applications</code> directory untouched and the included application are always ready for a new installation by the next startup.</p> \r\n<h3>AP Implementation</h3> \r\n<p>An AP is nothing more than a folder with a strict defined structure containing all the needed system-related information about itselft. We will put all the AP-related files into the <code>META-INF</code> sub-folder.</p> \r\n<p>Every AP must have a meta information file. We will create a ordinary property file and call it <code>meta.info</code>. The meta information file contains the <em>name</em>, <em>version </em>and <em>dependencies </em>(optionally) of the AP.</p> \r\n<p>For our example we need data to be executed (their content will be printed after the system start). We will put them into <code>data</code> folder.&nbsp;</p> \r\n<pre>META-INF/\r\n        /data/\r\n        /meta.info\r\n</pre> \r\n<p>For an application to be deployed must be the application folder named in the pattern name-version (e.g. code>MySuperAppA-1.0</code>) and placed in the <code>applications</code> folder. This is the default place for the APM to look for the new APs.</p> \r\n<h4>Demo APs&nbsp;</h4> \r\n<p>For the test purposes let\'s create three APs: <code>A</code>, <code>B</code>, <code>C</code> with the following dependencies:</p> \r\n<pre>A --&gt; B, C\r\nB --&gt; C\r\nC --&gt; (no dependency)</pre> \r\n<p>Each AP has two text files called be the pattern ap-name-1.txt and ap-name-2.txt with the content of the name of the AP and number of the data file.</p> \r\n<p>For example the AP named <code>B</code> contains in the folder <code>META-INF/data</code> a file <code>B-1.txt</code> with the content &quot;<code>B1</code>&quot; and a file <code>B-2.txt</code> with the content &quot;<code>B2</code>&quot;.</span></p> \r\n<p>After start of the demo system, we should see the following output:</p> \r\n<pre>C1\r\nC2\r\nB1\r\nB2\r\nA1\r\nA2&nbsp;</pre> \r\n<p>The order of the startups follows the dependencies definitions of the APs.</p> \r\n<h3>APM Implementation</h3> \r\n<p>First of all we need two Ant task for our APM script. The implementation of both is in the <code>core/lib/apm.jar</code> Java library and could be found in the source codes under the <code>ant-lib</code> project.</p> \r\n<h4>Dependencies order</h4> \r\n<p>This task takes a path as a parameter, goes thru this directory and gets all the folders inside it as an input.</p> \r\n<p>Then goes thru all of them and checks the dependencies from the <code>META-INF/meta.info</code>. So it creates a list of dependencies in the order of the need. For each application in the list a sequence inside the task will be executed. The sequence defines the installation procedure.</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"dependencies-order\" classname=\"cz.net21.ttulka.apm.ant.demo.DependenciesOrderTask\"\r\n         classpath=\"core/lib/apm.jar\" /&gt;\r\n...\r\n&lt;dependencies-order path=\"../applications\" relpathmetainfofile=\"META-INF/meta.info\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;antcall target=\"install-app\"&gt;\r\n            &lt;param name=\"appPath\" value=\"@{application}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/dependencies-order&gt;\r\n</pre> \r\n<h4>Configuration record&nbsp;</h4> \r\n<p>For every application we need to scan the <code>data</code> folder and create a list of the files inside. This list in comma-separated form will be appended into the applications configuration file.</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"create-conf-record\" classname=\"cz.net21.ttulka.apm.ant.demo.CreateConfRecordTask\"\r\n         classpath=\"core/lib/apm.jar\" /&gt;\r\n...\r\n&lt;create-conf-record path=\"${appPath}/META-INF/data\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;antcall target=\"write-conf-record\"&gt;\r\n            &lt;param name=\"record\" value=\"@{record}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/create-conf-record&gt;\r\n...\r\n&lt;target name=\"write-conf-record\"&gt;\r\n    &lt;echo file=\"conf/applications.conf\" append=\"true\"&gt;${line.separator}${appName}=${record}&lt;/echo&gt;\r\n&lt;/target&gt;&nbsp;</pre> \r\n<p> </p> \r\n<h2><em>Scenario Two</em>: Application (Web) Server&nbsp;</h2> \r\n<p>Such a APM could be used in the same way in the case of installing <strong>web applications into a server</strong>.&nbsp;</p> \r\n<p>Consider that the applications provide some functional services and their installation is <strong>dependent on one or more other</strong>.</p> \r\n<p>We can also install <strong>more versions of an application</strong> and enable only the latest version.</p> \r\n<p>For a proper run of an application the <strong>server needs to be configurated</strong> in a way known only to the application self.</p> \r\n<p>Application must be corretly <strong>re-installed after an update of the server</strong>, even when the old server data was completely removed.</p> \r\n<p>Even here helps the APM in all the above mentioned cases.&nbsp;</p> \r\n<h2>Downloads</h2> \r\n<p>You can download <a href=\"/storage/APM-demo.jar\">whole demo project</a>. After unpacking you can start the demo with <code>system/start.bat</code>.</p> \r\n<p>Alternatively you can download the <a href=\"https://github.com/ttulka/application-package-manager-demo\">source project</a>&nbsp;and addapt it to your needs and desires.</p> \r\n<p><br /></p>', 'false', 'false', 1, 1),
(21, 'json-mock-data-generator', 1493141000, 'JSON Mock Data Generator', '<p>It\'s easy to generate test mock JSON data for a small HTTP request or a similar use, because there is a planty of online tools providing this functionality for you.</p> \r\n<p>But what if you want to generate data for a database performance test for systems like MongoDB or Elasticsearch?</p> \r\n', '<p>Online tools can generate entities in tens, maybe in hunders, but in thousands or millions?&nbsp;</p> \r\n<p>In this case comes the&nbsp;<a href=\"https://github.com/ttulka/json-mock-data-generator\" target=\"_blank\" title=\"JSON Mock Data Generator\">JSON Mock Data Generator</a> on the scene.</p>', 'false', 'false', 1, 2),
(22, 'how-to-effectively-protect-critical-section', 1496339000, 'How to Effectively Protect Critical Section', '<div>Not everywhere could be <em>immutable objects</em> used to ensure thread-safe code. Sometimes we need to wait for some events (especially coming as user input) to initialize a variable. In the case this input can come from a concurrent environment we need to protect the initialization as critical section.</div> \r\n<div><br /></div> \r\n<div>Native Java approach is to use the keyword <code>synchronized</code> of the critical code and this will work... but is it <strong>effective </strong>as well?</div> \r\n<div> </div> \r\n<div>Let\'s discuss some more methods how to protect a critical section in the Java code.&nbsp;</div>', '<div>As usual we start as simple as possible, this is our code:</div> \r\n<pre class=\"brush: java\">class MyCriticalSection {\r\n\r\n    static Object toBeInitialized = null;   // mutable static object\r\n\r\n    public void callFromClient(Object... globalParams) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n\r\n        // so something with the toBeInitialized object\r\n        System.out.println(toBeInitialized.toString());\r\n    }\r\n\r\n    private Object initialize(Object... params) {\r\n        // do some better stuff here...\r\n        return new Object();\r\n    }\r\n}\r\n</pre> \r\n<div>Completely unsafe. So let\'s synchronize it:</div> \r\n<pre class=\"brush: java\">public synchronized void callFromClient(Object... globalParams) {\r\n    // ...\r\n}\r\n</pre> \r\n<div>Very ineffective. Actually we want to synchronize only the initialization part:</div> \r\n<pre class=\"brush: java\">public void callFromClient(Object... globalParams) {\r\n    synchronized (MyCriticalSection.class) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<div>Synchronizing on the class means a caller trying to access the section will be block even when another caller is inside another section synchronized on the class as well:</div> \r\n<pre class=\"brush: java\">void anotherCall() {\r\n    synchronized (MyCriticalSection.class) {\r\n        // ...\r\n    }\r\n}\r\n</pre> \r\n<div>The same for static methods:</div> \r\n<pre class=\"brush: java\">void static doSomething() {\r\n    synchronized (MyCriticalSection.class) {\r\n        // ...\r\n    }\r\n}\r\n</pre> \r\n<div>This could be okay when the initialization is distributed or static, but even for these cases we have a better option:</div> \r\n<pre class=\"brush: java\">private final Object initializationLock = new Object();     // could be static for static methods\r\n\r\npublic void callFromClient(Object... globalParams) {\r\n    synchronized (initializationLock) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<div>Looks nice, we\'re done...? Actually not, using the synchronized keyword is far not the most effectively way how to protect a critical section.</div> \r\n<div>Java 5 came with a package of atomic classes <code>java.util.concurrent.atomic</code> among then <code>AtomicBoolen</code>.</div> \r\n<div><code>AtomicBoolen</code> could be used to check atomically if a condition was already fulfilled or not:</div> \r\n<pre class=\"brush: java\">private final AtomicBoolean alreadyInitialized = new AtomicBoolean();\r\n\r\npublic void callFromClient(Object... globalParams) {\r\n    if (alreadyInitialized.compareAndSet(false, true)) {\r\n        toBeInitialized = initialize(globalParams);\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<div>This works the same way as the synchronized section above and is much more effective... but well, there is another little devil hidden in the code.</div> \r\n<div>Take a look at the code above including the synchronized variants. What happens when another caller calls the method in the same time as the first method entered the critical section to initialize the object?</div> \r\n<div>The second caller doesn\'t get the access to the critical section and continues in the code execution. But this means he works with a potentially uninitialized object and the last line of code will throw a <code>NullPointerException</code> trying to call <code>toString()</code> method on the <code>null</code> value.</div> \r\n<div> \r\n<p>Our job is not done here yet...</p> \r\n<p>In the package <code>java.util.concurrent</code> we can find a class<code>CountDownLatch</code> designed to synchronize concurrency of thread processing:</p> \r\n<pre class=\"brush: java\">private final AtomicBoolean initializationStarted = new AtomicBoolean();\r\nprivate final CountDownLatch initializationFinished = new CountDownLatch(1);\r\n\r\npublic void callFromClient(Object... globalParams) throws InterruptedException {\r\n    if (initializationStarted.compareAndSet(false, true)) {\r\n        toBeInitialized = initialize(globalParams);\r\n        initializationFinished.countDown();\r\n    } else {\r\n        initializationFinished.await();\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<div>So, what happens here? We use the atomic boolean to check if the object was already initialized or not. If not we access the critical section. Because the set and set of the atomic boolean is done atomically only one thread can access the initialization.</div> \r\n<div>The initialization is first ready when the first caller counts down the latch. Before the latch was count-downed all the concurrent callers wait on its <code>await()</code> method.</div> \r\n<div>All this doesn\'t look as nice and easy as with the <code>synchronized</code> keyword. So, is it really so effective?</div> \r\n<div> \r\n<p>I did performance tests using all the above mentioned techniques. I tried to access the critical section one million times for each of them:</p> \r\n<p> \r\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"3\"> \r\n<tbody> \r\n<tr> \r\n<td><strong>technique</strong></td> \r\n<td><strong>execution time</strong></td> \r\n</tr> \r\n<tr> \r\n<td>not thread-safe </td> \r\n<td style=\"text-align: right;\">2 ms</td> \r\n</tr> \r\n<tr> \r\n<td>synchronized class</td> \r\n<td style=\"text-align: right;\">32 ms</td> \r\n</tr> \r\n<tr> \r\n<td>synchronized object</td> \r\n<td style=\"text-align: right;\">26 ms</td> \r\n</tr> \r\n<tr> \r\n<td>atomic-latch</td> \r\n<td style=\"text-align: right;\">16 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n</div> \r\n<p> </p> \r\n<div> \r\n<p>Of course there a great penalty of synchronization but this shouldn\'t be a big surprise, I guess.</p> \r\n</div> \r\n<div> \r\n<p>The more important thing is the <em>AtomicBoolean-CountDownLatch</em> synchronization is over 40 % more effective. In environments like servers where the calls come for every request hundred thousand times in second this is a pretty nice optimization.</p> \r\n<p> </p> \r\n<p> </p> \r\n</div> \r\n</div>', 'false', 'false', 1, 1),
(23, 'managing-asynchronous-tests', 1500480000, 'Managing Asynchronous Tests', '<p>It\'s recommended to avoid any asynchrony within the scope of test, but this is unfortunaly not possible everywhere. In some systems the caller must wait for another thread or transaction to be completed.</p> \r\n<p>Testing asynchrony could be pretty tricky, not always is clear how long must a test wait for a result to be delivered - if it\'s too little the test fails event when the tested functionality works, if it\'s too long the test is just wasting time (so expensive especially in the commit stage).</p> \r\n<p>So, how to deal with this problem?&nbsp;</p>', '<p>Consider a simple test in JUnit:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldDeliverResult() {\r\n&nbsp; &nbsp; calculateResult();\r\n&nbsp; &nbsp; confirmResultWasDelivered(); &nbsp; &nbsp; &nbsp; &nbsp;\r\n}\r\n\r\nprivate void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; if (!resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; fail(\"No result was delivered!\");\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>In this unit test we test a state of an object after calling the method <code>calculateResult()</code> with the method <code>resultWasDelivered()</code>. If the method <code>calculateResult()</code> is designed in the asynchronous manner, the test will fail almost in 100 % cases. </p> \r\n<p>A naive solution is to <strong>add a timeout</strong>:</p> \r\n<pre class=\"brush: java\">private void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; sleep(TIMEOUT);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; if (!resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; fail(\"No result was delivered!\");\r\n&nbsp; &nbsp; }\r\n}\r\nprivate void sleep(long milliseconds) {\r\n&nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(milliseconds);\r\n&nbsp; &nbsp; } catch (InterruptedException ignore) {\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>But again, what is the <em>right </em>value of the timeout here?</p> \r\n<p>We want to be sure that our test passes always when a correct result is delivered, so we set the timeout to a maximal waiting period. On the other hand in some cases the result could be delivered quickly in a small amout of time: </p> \r\n<pre class=\"brush: java\">private void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; long testStarted = System.currentTimeMillis();\r\n&nbsp; &nbsp; do {\r\n&nbsp; &nbsp; &nbsp; &nbsp; if (resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return;\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; sleep(SMALL_AMOUT_OF_TIME);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; } while (TIMEOUT &gt; System.currentTimeMillis() - testStarted);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; fail(\"No result was delivered!\");\r\n}</pre> \r\n<p>The last approach is still based on the pull princip and active waiting, but it\'s much more efficient as the previous one.&nbsp;</p> \r\n<p>We try to check if a result was delivered, if not we try it again after a small amout of time till the timeout exceeds.</p> \r\n<p> </p>', 'false', 'false', 1, 1),
(24, 'synchronized-methods-vs-semaphore', 1501685000, 'Synchronized Methods vs Semaphore', '<div> \r\n<p>When one-thread-access is too restrictive and we want to allow more threads to our resource then come semaphores on the stage.</p> \r\n<p>But there is one important difference in these approaches.</p> \r\n</div>', '<div>Let\'s consider a code:</div> \r\n<pre class=\"brush: java\">private Object synch = new Object();\r\n\r\npublic void synchronizedMethod1() {\r\n    synchronized (synch) {\r\n        synchronizedMethod2();\r\n    }\r\n}\r\n\r\npublic void synchronizedMethod2() {\r\n    synchronized (synch) {\r\n        System.out.println(\"Access to the resource acquired.\");\r\n    }\r\n}</pre> \r\n<div>What happened by calling the <code>synchronizedMethod1()</code>?</div> \r\n<div>Of course the access to the resource will be acquired because the synchronized block is in Java shared in within the thread. In other words, if the thread is already in the synchronized block will get all the others as well (protected by the same object).</div> \r\n<div><br /></div> \r\n<div>Applying a semaphore (with the counter set to one) on the same code:</div> \r\n<pre class=\"brush: java\">private Semaphore semaphore = new Semaphore(1);\r\n\r\npublic void synchronizedMethod1() throws InterruptedException {\r\n    semaphore.acquire();\r\n    synchronizedMethod2();\r\n    semaphore.release();\r\n}\r\n\r\npublic void synchronizedMethod2() throws InterruptedException {\r\n    semaphore.acquire();\r\n    System.out.println(\"Access to the resource acquired.\");\r\n    semaphore.release();\r\n} </pre> \r\n<div>And calling <code>synchronizedMethod1()</code> will freeze forever.</div> \r\n<div><br /></div> \r\n<div>In this case acquire the <code>synchronizedMethod1</code> the semaphore but <code>synchronizedMethod2</code>&nbsp;fails on trying to acquire it again.</div> \r\n<div><br /></div> \r\n<div>In general, this problem is caused by a weak design and can be solved by a simple refactoring:</div> \r\n<pre class=\"brush: java\">public void synchronizedMethod1() throws InterruptedException {\r\n    semaphore.acquire();\r\n    resourceMethod();\r\n    semaphore.release();\r\n}\r\n\r\npublic void synchronizedMethod2() throws InterruptedException {\r\n    semaphore.acquire();\r\n    resourceMethod();\r\n    semaphore.release();\r\n}\r\n\r\nprivate void resourceMethod() {\r\n    System.out.println(\"Access to the resource acquired.\");\r\n}</pre>', 'false', 'false', 1, 1),
(25, 'thistledb-simple-json-database-released', 1505935000, 'ThistleDB - Simple JSON Database - Released!', '<p>Simple JSON database based on the <strong>file-access</strong> and <strong>server-client</strong> approach with the non-blocking server and the reactive (asynchronous non-blocking) client.</p>', '<p>All about:&nbsp;<a href=\"http://thistledb.net21.cz\"><strong>http://thistledb.net21.cz</strong></a></p> \r\n<p>Have fun!</p>', 'false', 'false', 1, 2),
(26, 'pitfalls-of-processing-a-stream-from-an-external-program', 1506962000, 'Pitfalls of Processing a Stream from an External Program', '<div>Let\'s image an external standalone program producing a big amount of binary data. Good example is a files converter (images, mp3s, documents, etc).</div> \r\n<div>How to design such a program and what are the pitfalls of the approach?</div>', '<h2>Standalone Producer Application</h2> \r\n<div>There is many ways how to create a standalone application and one of the easiest and the most straight-forward approaches is Spring-Boot (<code>pom.xml</code>):</div> \r\n<pre class=\"brush: xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\r\n&nbsp; &nbsp; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\r\n&nbsp; &nbsp; &lt;parent&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;!-- Your own application should inherit from spring-boot-starter-parent --&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;/parent&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.eval&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;StandaloneBytesProducer&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;properties&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;java.version&gt;1.8&lt;/java.version&gt;\r\n&nbsp; &nbsp; &lt;/properties&gt;\r\n&nbsp; &nbsp; &lt;dependencies&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;commons-logging&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.2&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;lombok&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.16.18&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;scope&gt;provided&lt;/scope&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &lt;/dependencies&gt;\r\n&nbsp; &nbsp; &lt;build&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;plugins&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;3.7.0&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;source&gt;1.8&lt;/source&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;target&gt;1.8&lt;/target&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugins&gt;\r\n&nbsp; &nbsp; &lt;/build&gt;\r\n&lt;/project&gt;</pre> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<p><code>StandaloneBytesProducerApplication.java</code>:</p> \r\n<pre class=\"brush: java\">package cz.net21.ttulka.eval.bytesproducer;\r\n\r\nimport org.springframework.boot.ApplicationArguments;\r\nimport org.springframework.boot.ApplicationRunner;\r\nimport org.springframework.boot.SpringApplication;\r\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\r\nimport lombok.extern.apachecommons.CommonsLog;\r\n\r\n@SpringBootApplication\r\n@CommonsLog\r\npublic class StandaloneBytesProducerApplication implements ApplicationRunner {\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void run(ApplicationArguments args) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; log.info(\"StandaloneBytesProducerApplication started.\");\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; int bytesAmount = 1000;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (args.containsOption(\"bytes\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bytesAmount = Integer.parseInt(args.getOptionValues(\"bytes\").get(0));&nbsp; &nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; bytesAmount; i++) {&nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.write(i % Byte.MAX_VALUE);&nbsp; &nbsp;// we\'re writing on the standard output stream\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(\"Unexpected error.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; System.exit(0);\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; public static void main(String[] args) throws Exception {\r\n&nbsp; &nbsp; &nbsp; &nbsp; SpringApplication.run(StandaloneBytesProducerApplication.class, args);\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<p> </p> \r\n<div> \r\n<p>Compile it and run:</p> \r\n</div> \r\n<pre>mvn clean package\r\nmvn spring-boot:run</pre> \r\n<div>It looks good. Of course a consumer will run it direct from a JAR:</div> \r\n<pre>java -jar target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar</pre> \r\n<div>The result is what we expected, Spring Boot ASCII logo, some log messages and our bytes stream.</div> \r\n<div>And this is exactly one pitfall because all this junk destroys our result, actually all and only we need is the bytes stream.</div> \r\n<div><br /></div> \r\n<div>Spring Boot uses it own logging (based on <code><a href=\"https://commons.apache.org/proper/commons-logging/\" target=\"_blank\">commons-logging</a></code>) hidden in the artifact <code>spring-boot-starter-logging</code>. To get rid of it we can exclude this artifact from the build:</div> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n&nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;exclusions&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;exclusion&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/exclusion&gt;\r\n&nbsp; &nbsp; &lt;/exclusions&gt;\r\n&lt;/dependency&gt;</pre> \r\n<div>When we now run the program, the log messages look different. After excluding the Spring Boot logging the <code>commons-logging</code> uses its default fall-back implementation <code>SimpleLog</code>.</div> \r\n<div><code>SimpleLog</code> then sends all messages, for all defined loggers, to <code>stderr</code>. We can prove it by forwarding the standard output into a file:</div> \r\n<pre>java -jar target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar &gt; out.dat</pre> \r\n<div>Indeed, the log messages are still written in the console and the file includes only the Spring Boot logo and our bytes.</div> \r\n<div><br /></div> \r\n<div>To get rid of the logo is easy, just put the <code>application.yml</code> into the <em>resources </em>directory:</div> \r\n<pre>spring:\r\n&nbsp; main:\r\n&nbsp; &nbsp; banner-mode: \"off\"</pre> \r\n<div> \r\n<p>Now the standard output contains only the result bytes. It\'s time to implement a consumer...</p> \r\n</div> \r\n<h2>Standalone Consumer Application</h2> \r\n<div>Consumer could be done in the same manner, this time we don\'t case about logging much (<code>pom.xml</code>):</div> \r\n<pre class=\"brush: xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\r\n&nbsp; &nbsp; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\r\n&nbsp; &nbsp; &lt;parent&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;!-- Your own application should inherit from spring-boot-starter-parent --&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;/parent&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.eval&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;StandaloneBytesConsumer&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;properties&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;java.version&gt;1.8&lt;/java.version&gt;\r\n&nbsp; &nbsp; &lt;/properties&gt;\r\n&nbsp; &nbsp; &lt;dependencies&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;commons-logging&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.2&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;lombok&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.16.18&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;scope&gt;provided&lt;/scope&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &lt;/dependencies&gt;\r\n&nbsp; &nbsp; &lt;build&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;plugins&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;3.7.0&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;source&gt;1.8&lt;/source&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;target&gt;1.8&lt;/target&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugins&gt;\r\n&nbsp; &nbsp; &lt;/build&gt;\r\n&lt;/project&gt;&nbsp;</pre> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<div>Important pitfall here to be aware about: <strong>all (stdout, stderr) the streams must be consumed</strong>. If you forget to consume the <code>stderr</code> stream the program will freeze forever.</div> \r\n<div><br /></div> \r\n<div>The error log can be either consumed and forgotten or consumed and print into the log:</div> \r\n<pre class=\"brush: java\">package cz.net21.ttulka.eval.bytesconsumer;\r\n\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport java.util.Scanner;\r\nimport org.springframework.boot.ApplicationArguments;\r\nimport org.springframework.boot.ApplicationRunner;\r\nimport org.springframework.boot.SpringApplication;\r\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\r\nimport lombok.extern.apachecommons.CommonsLog;\r\n\r\n@SpringBootApplication\r\n@CommonsLog\r\npublic class StandaloneBytesConsumerApplication implements ApplicationRunner {\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void run(ApplicationArguments args) {&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; String pathToJar = System.getProperty(\"PATH_TO_JAR\");\r\n&nbsp; &nbsp; &nbsp; &nbsp; log.info(\"StandaloneBytesConsumerApplication started: \" + pathToJar);\r\n&nbsp; &nbsp; &nbsp; &nbsp; ProcessBuilder builder = new ProcessBuilder(\"java\", \"-jar\", pathToJar);&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Process process = builder.start();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; processErrors(process.getErrorStream());&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; processStream(process.getInputStream());&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(\"Unexpected error.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; System.exit(0);\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; private void processStream(InputStream stream) throws IOException {\r\n&nbsp; &nbsp; &nbsp; &nbsp; int b;\r\n&nbsp; &nbsp; &nbsp; &nbsp; while ((b = stream.read()) != -1) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // TODO do something with the stream\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; stream.close();\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; private void processErrors(final InputStream in) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; new Thread(new Runnable() {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public void run() {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; int logLevel = 3;&nbsp; &nbsp;// 0 - ERROR, 1 - WARN, 2 - INFO, 3 - DEBUG\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Scanner scanner = new Scanner(in);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (scanner.hasNextLine()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String line = scanner.nextLine();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"ERROR\") || line.startsWith(\"FATAL\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 0;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"WARN\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 1;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"INFO\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 2;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"DEBUG\") || line.startsWith(\"TRACE\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 3;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; switch (logLevel) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 0:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 1:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.warn(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 2:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.info(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; default:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.debug(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; }).start();\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; public static void main(String[] args) throws Exception {\r\n&nbsp; &nbsp; &nbsp; &nbsp; SpringApplication.run(StandaloneBytesConsumerApplication.class, args);\r\n&nbsp; &nbsp; }\r\n}&nbsp;</pre> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<div> </div> \r\n<div>Compile and run it:</div> \r\n<pre>mvn clean package\r\nmvn spring-boot:run -DPATH_TO_JAR=..\\StandaloneBytesProducer\\target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar</pre> \r\n<div> \r\n<p>Source codes: <a href=\"https://github.com/ttulka/eval/tree/master/StandaloneBytesProducer\" target=\"_blank\" title=\"StandaloneBytesProducer\">StandaloneBytesProducer</a> and <a href=\"https://github.com/ttulka/eval/tree/master/StandaloneBytesConsumer\" target=\"_blank\" title=\"StandaloneBytesConsumer\">StandaloneBytesConsumer</a>.&nbsp;</p> \r\n<p>Happy byting!</p>\r\n</div>', 'false', 'false', 1, 1),
(27, 'process-watch-dog-released', 1507387000, 'Process Watch Dog Released!', 'It\'s a good idea to work with external processes to prevent the system from failure especially when the process is very resources-demanding. \r\n<br/>But what if something goes wrong?\r\n<br/>Let the <em>watch dog</em> out!', '<pre class=\"brush: xml\">&lt;dependency&gt;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.exec&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;process-watch-dog&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Consider a very simple tooling class for executing new processes. Put the watch dog to kill processes after 2 minutes:</p> \r\n<pre class=\"brush: java\">import cz.net21.ttulka.exec.ProcessWatchDog;\r\n\r\npublic class ProcessExecutorTool {\r\n\r\n&nbsp; &nbsp; private static int TIMEOUT = 2 ✱ 60 ✱ 1000; // 2 minutes\r\n\r\n&nbsp; &nbsp; private static ProcessWatchDog watchDog = new ProcessWatchDog();\r\n\r\n&nbsp; &nbsp; public static int executeExternalProcess(String... cmd) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProcessBuilder builder = new ProcessBuilder(cmd);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Process process = builder.start();\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; watchDog.watch(process, TIMEOUT);   // release the dog!</pre> \r\n<pre class=\"brush: java\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; process.waitFor();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return process.exitValue();</pre> \r\n<pre class=\"brush: java\">&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; throw new RuntimeException(\"Exception by executing \'\" + cmd + \"\'.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>All about:&nbsp;<a href=\"https://github.com/ttulka/process-watch-dog\"><strong>https://github.com/ttulka/process-watch-dog</strong></a> </p> \r\n<p>Happy watching!</p>', 'false', 'false', 1, 2),
(28, 'boundary-io-streams-released', 1509558000, 'Boundary I/O Streams Released!', 'New Java library for working with <strong>boundary I/O streams</strong> was released.\r\n<br/> \r\nThis library provides classes for processing multiple streams withing a single stream bounded by a specific boundary content.\r\n<br/> \r\nLet\'s take a deeper look at its functionality...', '<p>First of all, put the Maven dependency into your <code>pom.xml</code>:</p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.io&lt;/groupId&gt;\r\n    &lt;artifactId&gt;boundary-io-streams&lt;/artifactId&gt;\r\n    &lt;version&gt;1.0.0&lt;/version&gt;\r\n&lt;/dependency&gt;&nbsp;</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>Consider a simple use-case: a files streaming service into the <code>stdout</code>.</p> \r\n<pre class=\"brush: java\">public class FileStreamService {\r\n\r\n    public static void main(String[] args) throws IOException {\r\n        try (BoundaryOutputStream bos = new BoundaryOutputStream(System.out)) {\r\n\r\n            for (String filename : args) {\r\n                try (InputStream is = new FileInputStream(filename)) {\r\n\r\n                    org.apache.commons.io.IOUtils.copy(is, bos);  // see https://commons.apache.org/io\r\n                    bos.boundary();                               // separate the stream with the boundary\r\n                }\r\n            }\r\n        }\r\n    }\r\n}&nbsp;</pre> \r\n<p>When you run the service from the command line</p> \r\n<pre>java FileStreamService /data/text1.txt /data/text2.txt&nbsp;</pre> \r\n<p>You get something similar like:</p> \r\n<pre>Content of the text1.txt-----StreamBoundary-----Content of the text2.txt-----StreamBoundary-----&nbsp;</pre> \r\n<p>Notice that the class&nbsp;<code>BoundaryOutputStream</code> is nothing more than a convenient class, you don\'t need this class to create such a stream with standard Java I/O classes.</p> \r\n<h2>Reading a Multiple Stream&nbsp;</h2> \r\n<p>For reading a multiple stream you can use the class&nbsp;<code>BoundaryInputStream</code> direct or wrap it into a convenient iterable class <code>IterableBoundaryInputStream</code>. We show the second approach:</p> \r\n<pre class=\"brush: java\">InputStream in = callFileStreamingService();\r\n\r\ntry (BoundaryInputStream bis = new BoundaryInputStream(in)) {\r\n    IterableBoundaryInputStream ibis = new IterableBoundaryInputStream(bis);\r\n\r\n    for (InputStream is : ibis) {                \r\n        // do something with the stream\r\n    }\r\n}</pre> \r\n<p>The method&nbsp;<code>callFileStreamingService</code> calls the service above and returns its stream as the input for the <code>BoundaryInputStream</code>.</p> \r\n<p>More details at&nbsp;<a href=\"https://github.com/ttulka/boundary-io-streams\" target=\"_blank\" title=\"ttulka/boundary-io-streams\">https://github.com/ttulka/boundary-io-streams</a>.</p> \r\n<p>Happy streaming!</p> \r\n<p> </p> \r\n<p> </p>', 'false', 'false', 1, 2),
(29, 'process-watch-dog-with-heartbeat-released', 1510336000, 'Process Watch Dog with Heartbeat Released!', 'It\'s nice to watch a process execution and kill frozen processes after timeout, but when a process is supplying data is not frozen even when it takes a long time.\r\n<br/> \r\nRelease 1.1.0 introduces a new process wrapper to send a <strong>heartbeat to reset the timeout</strong> explicitly or automatically with every read byte.', '<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.exec&lt;/groupId&gt;\r\n    &lt;artifactId&gt;process-watch-dog&lt;/artifactId&gt;\r\n    &lt;version&gt;1.1.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Putting a process into watching returns a process wrapper:</p> \r\n<pre class=\"brush: java\">Process process = ...\r\n\r\nProcessWatchDog watchDog = new ProcessWatchDog();\r\n\r\nWatchedProcess watchedProcess = watchDog.watch(process, 1000);</pre> \r\n<p>Because the class <code>WatchedProcess</code> extends stadard Java class Process, we can do simply:</p> \r\n<pre class=\"brush: java\">process = watchDog.watch(process, 1000);</pre> \r\n<p>Now, a heartbeat will be sent every time we read from the process input stream:</p> \r\n<pre class=\"brush: java\">InputStream is = process.getInputStream();\r\nint b;\r\nwhile ((b = is.read()) != -1) {\r\n    // heartbeat is sent implicitly with every successful call of `read()` \r\n}</pre> \r\n<p>To send a heartbeat explicitly, we can do it via the WatchedProcess object:</p> \r\n<pre class=\"brush: java\">wp.heartBeat();</pre> \r\n<p>Or simply tell the Watch Dog to do it:</p> \r\n<pre class=\"brush: java\">watchDog.heartBeat(process);   // we don\'t need the `WatchedProcess` object here</pre> \r\n<p>Previous post about the Watch Dog: <a href=\"/process-watch-dog-released\">Process Watch Dog Released!</a></p> \r\n<p>All about:&nbsp;<a href=\"https://github.com/ttulka/process-watch-dog\"><strong>https://github.com/ttulka/process-watch-dog</strong></a></p> \r\n<p>Happy heartbeating!</p> \r\n<p> </p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(30, 'stop-boundary-io-streams-released', 1512587000, 'Stop Boundary I/O Streams Released!', 'The <a href=\"https://github.com/ttulka/boundary-io-streams\" title=\"boundary-io-streams\">library for boundary I/O streams</a> brought the possibility to work with multiple stream written in a single stream.\r\n<br/> \r\nBut what if the stream continues further without containing any more sub-stream? In this case we can use the new released <strong>Stop Boundary Stream</strong> feature from the same library to ignore the rest of the stream after a boundary was reached.', '<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.io&lt;/groupId&gt;\r\n    &lt;artifactId&gt;boundary-io-streams&lt;/artifactId&gt;\r\n    &lt;version&gt;1.2.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>To stop consuming a stream after a boundary was reached it is possible to use the <code>StopBoundaryInputStream</code> class and the convenience class <code>StopBoundaryOutputStream</code> to generate such a stream. The stop boundary streaming is still using a boundary to separate sub-streams, but when a stop boundary occurs the rest of the input stream is ignored.</p> \r\n<p>Let\'s take a look at a simple example:&nbsp;</p> \r\n<pre class=\"brush: java\">String[] values = {\r\n    \"abcde\", \"ABCDE\", \"12345\"\r\n};\r\n\r\nbyte[] boundary = \"|\".getBytes();\r\nbyte[] stopBoundary = \"#\".getBytes();\r\n\r\nByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n\r\nStopBoundaryOutputStream out = new StopBoundaryOutputStream(bytes, boundary, stopBoundary);\r\n\r\nfor (String s : values) {\r\n    out.write(s.getBytes());\r\n    out.boundary();\r\n}\r\nout.stopBoundary();\r\nout.write(\"xyz\".getBytes());    // some junk at the end\r\n\r\n//System.out.println(bytes);    // prints `abcde|ABCDE|12345|#xyz`\r\n\r\nStopBoundaryInputStream in = new StopBoundaryInputStream(\r\n        new ByteArrayInputStream(bytes.toByteArray()), boundary, stopBoundary);\r\n\r\nfor (InputStream is : in) {\r\n    int b;\r\n    while ((b = is.read()) != -1) {\r\n        System.out.print((char) b);\r\n    }\r\n    System.out.println();\r\n}\r\n\r\n// close streams...\r\n</pre> \r\n<p>The code above prints:</p> \r\n<pre>abcde\r\nABCDE\r\n12345\r\n</pre> \r\n<p>Previous post about the library: <a href=\"/boundary-io-streams-released\">Boundary I/O Streams Released!</a></p> \r\n<p>All about: <a href=\"https://github.com/ttulka/boundary-io-streams\"><strong>https://github.com/ttulka/boundary-io-streams</strong></a></p> \r\n<p> </p> \r\n<p>Happy streaming!</p>', 'false', 'false', 1, 2),
(31, 'debugging-jasmine-ts-in-intellij-ide', 1522242000, 'Debugging jasmine-ts in IntelliJ IDE', 'TypeScript application wants tests to be written in TypeScript as well, right?<br/> \r\nThere is a great library to achive this: <a href=\"https://www.npmjs.com/package/jasmine-ts\" target=\"_blank\">jasmine-ts</a> <br/> \r\nBut how to debug such tests within your&nbsp;IntelliJ IDE?', '<p>You need to use the Node.js plugin and configure it to run/debug the <code>jasmine-ts</code> like following:</p> \r\n<p><img src=\"/storage/jasmine-ts-debug.png\" alt=\"jasmine-ts debug from IntelliJ\" width=\"100%\" /></p> \r\n<p>I\'m developing under Windows, but for Mac or Linux would be the situation almost the same.</p> \r\n<p>The meaning of the settings is following:</p> \r\n<p> </p> \r\n<ul> \r\n<li><em>Working directory</em> is the location of my TypeScript project, generally it the directory where your Jasmine configuration is located.</li> \r\n<li><em>JavaScript file</em> is the <code>jasmine-ts</code> bootstrap file. My local <code>npm</code>&nbsp;repository is located in my home ordner under&nbsp;<code>AppData\\Roaming\\npm\\node_modules</code>, this is the folder where <code>jasmine-ts</code> is installed into.</li> \r\n<li><em>Application parameters</em> are used to debug a concrete test suite. If you want to debug all the Jasmine tests, leave this field empty. In my case, all my Jasmine test files are located under the <code>test</code> sub-folder in my project directory.</li> \r\n</ul> \r\n<p>Then just set up some breakpoints and click the Run/Debug button.</p> \r\n<p>Here we go...&nbsp;</p> \r\n<p>Happy debugging!</p> \r\n<p> </p>', 'false', 'false', 1, 2),
(32, 'meaning-of-queues-and-topics-in-aws', 1523468000, 'Meaning of Queues and Topics in AWS', '<em>Queues</em> and <em>topics</em> are standard communication channels in messaging systems.\r\n<br/> \r\nHow to use then in <strong>AWS</strong>, for example to implement the&nbsp;<a href=\"http://www.enterpriseintegrationpatterns.com/patterns/messaging/CompetingConsumers.html\" target=\"_blank\" title=\"Competing Consumers\">Competing Consumers</a> pattern?', '<p>Let\'s consider a following scenario:</p> \r\n<p>A REST endpoint (Amazon API Gateway‎) initiates a time expensive processing served by a serverless function (AWS Lambda). Not only because of the max timeout 30 seconds on the gateway it is not a good idea to process the request synchronously (see <a href=\"https://www.reactivemanifesto.org\" title=\"The Reactive Manifesto\">The Reactive Manifesto</a>).&nbsp;</p> \r\n<p>Let\'s build a queue of working tasks into the middle between the gateway and the worker function:</p> \r\n<pre>Request --&gt; Queue --&gt; Worker&nbsp;</pre> \r\n<p>Immediately after a request comes it is put into the queue and taken by a worker function to the processing.</p> \r\n<p>To implement this in AWS the tasks queue is not a queue (SQS) at all, but a topic (SNS). Let\'s&nbsp;explain why.</p> \r\n<p>In a standard non-serverless implementation will be workers listening on the queue and distributing the tasks among each other. It means the queue is a <em>pull</em> mechanism.</p> \r\n<p>Serverless service creates as many worker instances as needed. So there is no need to keep tasks in a queue while they are processed immediately as put into the queue. Initialization of the worker function must be event-driven (a lambda function must never&nbsp;idle!) - implementing a <em>push</em> mechanism.</p> \r\n<p>There is no change to register a lambda function to a queue, for such a usage there is another concept - topics:</p> \r\n<pre>Request --&gt; Topic --&gt; Worker</pre> \r\n<p>As the request comes a task is put into the topic and processed by the lambda function, elastically created on demand.</p> \r\n<p>So, that\'s the meaning of queues and topics in AWS.</p> \r\n<p>Happy clouding!</p>', 'false', 'false', 1, 2),
(33, 'php-restful-api-microservices', 1525189000, 'PHP Restful API Microservices', 'How difficult is to properly implement the <strong>microservices design pattern in PHP</strong>? It will definitely need some thinking and maybe to leave some convince behind. At the end of the day we should have an autonomous loosely coupled service with all the benefits (and drawbacks) of microservices.<br/>In this article we do a little walk-through of a development process of a small RESTful microservice in PHP and we take a look at some Domain-Driven Design (DDD) theory as well.', '<div>Consider a simple <a href=\"https://en.wikipedia.org/wiki/Create,&#95;read,&#95;update&#95;and&#95;delete\" target=\"_blank\" title=\"CRUD\">CRUD</a> service for a blog articles management. Via this <a href=\"https://martinfowler.com/articles/richardsonMaturityModel.html\" target=\"_blank\" title=\"REST\">REST</a> API you can list, create, update and delete articles in the database (or whatever the persistence is).</div> \r\n<div> \r\n<p>We will implement this service following the <a href=\"https://martinfowler.com/articles/microservices.html\" target=\"_blank\" title=\"Microservices\">microservices design pattern</a> so we create a single REST endpoint (<code>/articles</code>) serving all the request.&nbsp;<br />Similarly we would create endpoint for the categories and authors management.</p> \r\n<p>First of all, let\'s prepare the infrastructure.</p> \r\n<h2>Database Schema and Test-Data</h2> \r\n<p>Traditionally, we use a <strong>MySQL database</strong> with a following schema:&nbsp;</p> \r\n<pre class=\"brush: sql\">CREATE SCHEMA `blog` DEFAULT CHARACTER SET utf8;\r\n\r\nCREATE TABLE `blog`.`categories` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `name` VARCHAR(50) NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`));\r\n&nbsp;&nbsp;\r\nCREATE TABLE `blog`.`authors` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `name` VARCHAR(50) NOT NULL,\r\n&nbsp; `email` VARCHAR(50) NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`));\r\n\r\nCREATE TABLE `blog`.`articles` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `title` VARCHAR(100) NOT NULL,\r\n&nbsp; `summary` TEXT(1000) NOT NULL,\r\n&nbsp; `body` TEXT NOT NULL,\r\n&nbsp; `createdAt` DATE&nbsp; NOT NULL,\r\n&nbsp; `categoryId` INT NOT NULL,\r\n&nbsp; `authorId` INT NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`),\r\n&nbsp; INDEX `categoryId&#95;idx` (`categoryId` ASC),\r\n&nbsp; INDEX `authorId&#95;idx` (`authorId` ASC),\r\n&nbsp; CONSTRAINT `category&#95;fk`\r\n&nbsp; &nbsp; FOREIGN KEY (`categoryId`)\r\n&nbsp; &nbsp; REFERENCES `blog`.`categories` (`id`),\r\n&nbsp; CONSTRAINT `author&#95;fk`\r\n&nbsp; &nbsp; FOREIGN KEY (`authorId`)\r\n&nbsp; &nbsp; REFERENCES `blog`.`authors` (`id`));&nbsp;</pre> \r\n<p>Then, let\'s put one author, two categories and three articles into the database.</p> \r\n<pre class=\"brush: sql\">INSERT INTO `blog`.`authors` (`id`, `name`, `email`)\r\n&nbsp; VALUES (0, \'Tomas Tulka\', \'tomas.tulka@gmail.com\');\r\n\r\nINSERT INTO `blog`.`categories` (`id`, `name`)\r\n&nbsp; VALUES (0, \'PHP\'), (0, \'Java\');\r\n\r\nINSERT INTO `blog`.`articles` (`id`, `title`, `summary`, `body`, `createdAt`, `categoryId`, `authorId`)\r\n&nbsp; VALUES \r\n&nbsp; &nbsp; (0, \'Sample PHP blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Lorem ipsum dolor sit amet.\',\r\n&nbsp; &nbsp; \'Sed vitae tincidunt magna. Sed pretium neque commodo mauris lobortis, quis finibus dolor malesuada.\',\r\n&nbsp; &nbsp; \'2018-05-01\', 1, 1),&nbsp;\r\n&nbsp; &nbsp; (0,&nbsp; \'Another PHP blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Donec id pellentesque elit, sit amet accumsan mi.\',\r\n&nbsp; &nbsp; \'Duis molestie tellus quis orci venenatis, ac pretium quam malesuada. Vivamus congue justo nulla, sit amet pharetra purus condimentum at.\',\r\n&nbsp; &nbsp; \'2018-05-02\', 1, 1),&nbsp;\r\n&nbsp; &nbsp; (0,&nbsp; \'Java blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Praesent porta sagittis diam non interdum.\',\r\n&nbsp; &nbsp; \'Semper a nunc nec dapibus. Sed tristique vel ipsum vitae euismod. Aenean vel nibh ac diam ullamcorper porta id at purus.\',\r\n&nbsp; &nbsp; \'2018-05-02\', 2, 1);</pre> \r\n<p>After creating the database schema and putting some test data, we can discuss the service architecture design.&nbsp;</p> \r\n<h2>REST API&nbsp;</h2> \r\n<p>We will use the <a href=\"https://martinfowler.com/articles/richardsonMaturityModel.html#level2\" title=\"REST Verbs\">Richardson Maturity Model</a> approach of <strong>HTTP verbs</strong>, simply:&nbsp;</p> \r\n<p> </p> \r\n<ul> \r\n<li><code>GET endpoint</code> - listing items</li> \r\n<li><code>GET endpoint/{id}</code> - item detail</li> \r\n<li><code>POST endpoint</code> - creating a new item</li> \r\n<li><code>PUT endpoint/{id}</code> -&nbsp;updating an existing item</li> \r\n<li><code>DELETE&nbsp;endpoint/{id}</code> - deleting an existing item</li> \r\n</ul> \r\n<p> </p> \r\n<h2>Microservices and Domain-Driven Design</h2> \r\n<p>We will model our microservice around a <a href=\"https://martinfowler.com/bliki/DDD&#95;Aggregate.html\" target=\"_blank\" title=\"DDD Aggregate\">DDD Aggregate</a>, in this case around the&nbsp;<em>Article</em>.</p> \r\n<p>For sake of simplicity we model our Article aggregate as an <a href=\"https://martinfowler.com/bliki/AnemicDomainModel.html\" target=\"_blank\" title=\"Anemic Domain Model\">Anemic Domain Entity</a>. With a maturer domain model the entity should be modeled as immutable object, created only via a constructor or a Factory, containing only getters and behavior methods. Such a domain object shouldn\'t be definitely exposed to the client\'s view (like we are doing), but should be transformed into a <a href=\"https://en.wikipedia.org/wiki/Data&#95;transfer&#95;object\" target=\"_blank\" title=\"DTO\">DTO</a>.</p> \r\n<p><code><strong>domain/Article.php</strong></code> </p> \r\n<pre class=\"brush: php\">class Article { \r\n    public $id;\r\n    public $title;\r\n    public $summary;\r\n    public $body;\r\n    public $createdAt;\r\n    \r\n    public $categoryId;\r\n    \r\n    public $author;         \r\n}\r\n\r\nclass ArticleAuthor {  \r\n    public $id;\r\n    public $name;\r\n    public $email;\r\n}&nbsp;</pre> \r\n<p>Why is there the class <code>ArticleAuthor</code>, why don\'t we create a separate class <em>Author</em> for that purpose? Well, we just don\'t need all the data of the author, an article needs only few of them. If, in the future, more attributes will be added to the <em>Author</em> entity, the Article structure should stay untouched. The object of the class <code>ArticleAuthor</code> is an <a href=\"https://martinfowler.com/bliki/ValueObject.html\" target=\"_blank\" title=\"Value Object\">Value Object</a> and is accessible only thru the aggregate\'s root. So remains the Article consistent event when the Author entity changes its structure. Instead of ArticleAuthor we can use the name Author within different namespaces (<code>articles</code>&nbsp;and&nbsp;<code>authors</code>).</span></p> \r\n<h2>Persistence</h2> \r\n<p>According the DDD theory, <strong>each aggregate has a matching repository</strong>.&nbsp;The repository is the mechanism you should use to retrieve and persist aggregates. Obviously, we will persist the data into a database, but the <strong>persistence is a point of decision</strong> which could be (and should be) made at the latest possible point. And this is exactly what a repository makes possible.</p> \r\n<p><code><strong>domain/ArticleRepo.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/Article.php\';\r\n\r\ninterface ArticleRepo { \r\n    public function fetchAll($categoryId, $authorId, $start, $limit);    \r\n    public function fetchOne($id);    \r\n    public function create(Article $article);    \r\n    public function update($id, Article $article);    \r\n    public function delete($id);\r\n}</pre> \r\n<p>This <strong>interface decouples the client code</strong> (the service) from the persistence decision. It can be for example implemented as an in-memory storage in the early phases of the development.</p> \r\n<p>We create a <a href=\"http://php.net/manual/book.pdo.php\" title=\"PDO\">PDO</a>-based implementation:</p> \r\n<p><code><strong>infrastructure/ArticleRepoPDO.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/Article.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/ArticleRepo.php\';\r\n\r\nclass ArticleRepoPDO implements ArticleRepo {\r\n \r\n    private $conn;\r\n    \r\n    private $articles&#95;table = \'articles\';\r\n    private $articles&#95;categories&#95;table = \'categories\';\r\n    private $authors&#95;table = \'authors\';\r\n  \r\n    public function &#95;&#95;construct(PDO $conn){                                           \r\n        $this-&gt;conn = $conn;\r\n    }\r\n    \r\n    function fetchAll($categoryId = null, $authorId = null, $start = 0, $limit = 10) {   \r\n        $q = \"SELECT a.id, a.title, a.summary, a.createdAt, a.categoryId, a.authorId, au.name authorName, au.email authorEmail\r\n                FROM {$this-&gt;articles&#95;table} a\r\n                    LEFT JOIN {$this-&gt;authors&#95;table} au ON a.authorId = au.id\r\n                WHERE 1=1 \";\r\n                \r\n        $params = array(\'start\' =&gt; (int)$start, \'limit\' =&gt; (int)$limit);\r\n\r\n        if ($categoryId) {\r\n            $q .= \" AND a.categoryid = :categoryId\";\r\n            $params[\'categoryId\'] = (int)$categoryId;\r\n        }\r\n        if ($authorId) {\r\n            $q .= \" AND a.authorId = :authorId\";\r\n            $params[\'authorId\'] = (int)$authorId;\r\n        }                    \r\n                    \r\n        $q .=\"  ORDER BY a.createdAt DESC, a.id DESC\r\n                LIMIT :start,:limit\";\r\n        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);\r\n        \r\n        foreach ($params as $param =&gt; $value) {\r\n            $stmt-&gt;bindValue($param, $value, PDO::PARAM&#95;INT);\r\n        }\r\n        \r\n        $stmt-&gt;execute();\r\n        \r\n        $articles = array();  \r\n               \r\n        while ($row = $stmt-&gt;fetch(PDO::FETCH&#95;ASSOC)){\r\n          $article = new Article();\r\n          \r\n          $article-&gt;id = (int)$row[\'id\'];\r\n          $article-&gt;title = $row[\'title\'];\r\n          $article-&gt;summary = $row[\'summary\'];\r\n          $article-&gt;createdAt = $row[\'createdAt\'];\r\n          $article-&gt;categoryId = (int)$row[\'categoryId\'];\r\n          \r\n          $article-&gt;author = new ArticleAuthor();\r\n          $article-&gt;author-&gt;id = (int)$row[\'authorId\'];\r\n          $article-&gt;author-&gt;name = $row[\'authorName\'];\r\n          $article-&gt;author-&gt;email = $row[\'authorEmail\'];\r\n          \r\n          array&#95;push($articles, $article);\r\n        }\r\n             \r\n        return $articles;\r\n    }\r\n    \r\n    public function fetchOne($id) {\r\n        $q = \"SELECT a.id, a.title, a.summary, a.body, a.createdAt, a.categoryId, a.authorId, au.name authorName, au.email authorEmail\r\n                FROM {$this-&gt;articles&#95;table} a\r\n                    LEFT JOIN {$this-&gt;authors&#95;table} au ON a.authorId = au.id\r\n                WHERE a.id = :id \";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n        \r\n        $article = null;  \r\n               \r\n        if ($row = $stmt-&gt;fetch(PDO::FETCH&#95;ASSOC)){\r\n          $article = new Article();\r\n          \r\n          $article-&gt;id = (int)$row[\'id\'];\r\n          $article-&gt;title = $row[\'title\'];\r\n          $article-&gt;summary = $row[\'summary\'];\r\n          $article-&gt;body = $row[\'body\'];\r\n          $article-&gt;createdAt = $row[\'createdAt\'];\r\n          $article-&gt;categoryId = (int)$row[\'categoryId\'];\r\n          \r\n          $article-&gt;author = new ArticleAuthor();\r\n          $article-&gt;author-&gt;id = (int)$row[\'authorId\'];\r\n          $article-&gt;author-&gt;name = $row[\'authorName\'];\r\n          $article-&gt;author-&gt;email = $row[\'authorEmail\'];\r\n        }\r\n             \r\n        return $article;\r\n    }\r\n    \r\n    public function create($article) {\r\n        $q = \"INSERT INTO {$this-&gt;articles&#95;table} (id, title, summary, body, createdAt, categoryId, authorId)\r\n                VALUES (0, \r\n                  \'{$article-&gt;title}\', \r\n                  \'{$article-&gt;summary}\', \r\n                  \'{$article-&gt;body}\', \r\n                  \'{$article-&gt;createdAt}\', \r\n                  {$article-&gt;categoryId}, \r\n                  {$article-&gt;authorId}\r\n                )\";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;execute(); \r\n        \r\n        $article-&gt;id = $this-&gt;conn-&gt;lastInsertId();\r\n        \r\n        return $article;   \r\n    }\r\n    \r\n    public function update($id, $article) {\r\n        $q = \"UPDATE {$this-&gt;articles&#95;table}\r\n                SET title = \'{$article-&gt;title}\', \r\n                    summary = \'{$article-&gt;summary}\', \r\n                    body = \'{$article-&gt;body}\', \r\n                    createdAt = \'{$article-&gt;createdAt}\', \r\n                    categoryId = {$article-&gt;categoryId}, \r\n                    authorId = {$article-&gt;authorId}\r\n                WHERE id = :id\";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n                \r\n        $count = $stmt-&gt;rowCount();        \r\n        return $count &gt; 0;   \r\n    }\r\n    \r\n    public function delete($id) {\r\n        $article = $this-&gt;fetchOne($id);\r\n        if ($article === null) {\r\n            return false;\r\n        }\r\n        \r\n        $q = \"DELETE FROM {$this-&gt;articles&#95;table} WHERE id = :id \";\r\n        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);                                  \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n        \r\n        $count = $stmt-&gt;rowCount();        \r\n        return $count &gt; 0;\r\n    }\r\n}&nbsp;</pre> \r\n<p>Constructor of the repository class needs a PDO database connection. We deal with that with a help of the <em>Factory</em> pattern.</p> \r\n<p>We create a <code>Database</code> interface which must be implemented by all the persistence providers, MySQL in our case:</p> \r\n<p><code><strong>infrastructure/db/Database.php</strong></code></p> \r\n<pre class=\"brush: php\">interface Database { \r\n    public function getConnection();  // returns a PDO connection object\r\n}&nbsp;</pre> \r\n<p><code><strong>infrastructure/db/DatabaseMySql.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/Database.php\';\r\n\r\nclass DatabaseMySql implements Database {    \r\n    private $conn;\r\n    \r\n    function &#95;&#95;construct($host, $db, $username, $password) {\r\n        try {\r\n            $this-&gt;conn = new PDO(\"mysql:host={$host};dbname={$db}\", $username, $password);\r\n            $this-&gt;conn-&gt;setAttribute(PDO::ATTR&#95;ERRMODE, PDO::ERRMODE&#95;EXCEPTION);\r\n            $this-&gt;conn-&gt;setAttribute(PDO::MYSQL&#95;ATTR&#95;FOUND&#95;ROWS, true);\r\n            $this-&gt;conn-&gt;exec(\"set names utf8\");\r\n            \r\n        } catch(PDOException $e){\r\n            throw new Exception(\'Connection error: \' . $e-&gt;getMessage(), 0, $e);\r\n        }\r\n    }\r\n \r\n    public function getConnection() {\r\n        return $this-&gt;conn;\r\n    }\r\n}&nbsp;</pre> \r\n<p>Our factory has a static method for getting the right PDO connection:</p> \r\n<p><code><strong>infrastructure/db/DatabaseFactory.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/DatabaseMySql.php\';\r\n\r\nclass DatabaseFactory {  \r\n  public static function getDatabase($type, $host, $db, $username, $password) {\r\n    switch ($type) {         \r\n      case \'mysql\':\r\n        return new DatabaseMySql($host, $db, $username, $password);        \r\n      default:\r\n        throw new Exception(\'Unknown database type: \' . $type);\r\n    } \r\n  }\r\n}</pre> \r\n<p>The factory can be used as following:</p> \r\n<pre class=\"brush: php\">$db = DatabaseFactory::getDatabase(\'mysql\', \'localhost\', \'blog\', \'root\', \'1234\');</pre> \r\n<h2>Controller</h2> \r\n<p>The term comes from <a href=\"https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller\" target=\"_blank\" title=\"MVC\">MVC</a> design pattern. The responsibility of the controller in the application is to<strong> process the user input into output</strong>. This is the right place where the parameters should be validated (data types, range etc., the business validation is not meant here - that belongs to the domain services), proceeded and transformed back to the client.</p> \r\n<p>The important point here is the controller shouldn\'t know anything about the fact, that we\'re developing a web application. The web-specifics must be present only in the application itself, as we show in a moment.&nbsp;</p> \r\n<p><code><strong>application/ArticleController.php</strong></code> </p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/ArticleRepo.php\';\r\n\r\nclass ArticleController {\r\n\r\n  private $repo;\r\n\r\n  public function &#95;&#95;construct(ArticleRepo $repo){                                           \r\n    $this-&gt;repo = $repo;\r\n  }\r\n\r\n  public function detailRequest($id) {\r\n    $article = $this-&gt;repo-&gt;fetchOne((int)$id);\r\n    \r\n    return $article;\r\n  }\r\n  \r\n  public function listRequest($params) {\r\n    $limit = 10;\r\n    \r\n    $categoryId = $this-&gt;getIfSet($params, \'categoryId\');   \r\n    $authorId = $this-&gt;getIfSet($params, \'authorId\');   \r\n    $page = $this-&gt;getIfSet($params, \'page\', 0);\r\n    \r\n    $articles = $this-&gt;repo-&gt;fetchAll((int)$categoryId, (int)$authorId, $page &#42; $limit, $limit);\r\n    \r\n    return $articles;\r\n  }\r\n  \r\n  public function createRequest($params) {\r\n    $article = new Article();\r\n    $article-&gt;title = $params[\'title\'];\r\n    $article-&gt;summary = $params[\'summary\'];\r\n    $article-&gt;body = $params[\'body\'];\r\n    $article-&gt;createdAt = $params[\'createdAt\'];\r\n    $article-&gt;categoryId = (int)$params[\'categoryId\'];\r\n    $article-&gt;authorId = (int)$params[\'authorId\'];\r\n    \r\n    if (!$article-&gt;title || !$article-&gt;summary || !$article-&gt;body || !$article-&gt;createdAt || !$article-&gt;categoryId || !$article-&gt;authorId) {\r\n      return array(\'error\' =&gt; \'Incorrect payload.\');\r\n    }\r\n    \r\n    $article = $this-&gt;repo-&gt;create($article);\r\n    \r\n    return (int)$article-&gt;id;\r\n  }\r\n  \r\n  public function updateRequest($id, $params) {\r\n    $article = new Article();\r\n    $article-&gt;title = $params[\'title\'];\r\n    $article-&gt;summary = $params[\'summary\'];\r\n    $article-&gt;body = $params[\'body\'];\r\n    $article-&gt;createdAt = $params[\'createdAt\'];\r\n    $article-&gt;categoryId = (int)$params[\'categoryId\'];\r\n    $article-&gt;authorId = (int)$params[\'categoryId\'];\r\n    \r\n    return $this-&gt;repo-&gt;update($id, $article);\r\n  }\r\n  \r\n  public function deleteRequest($id) {  \r\n    return $this-&gt;repo-&gt;delete($id);\r\n  }\r\n  \r\n  // ///////// HELPER FUNCTIONS /////////////////////////////////////\r\n  \r\n  private function getIfSet($params, $var, $def = null) {\r\n    return isset($params[$var]) ? $params[$var] : $def;\r\n  }\r\n}</pre> \r\n<h2>Putting It All Together&nbsp;</h2> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/BlogArticlesDesign.png\" alt=\"Blog Articles Architecture\" width=\"100%\"/></p> \r\n<p>The picture above shows the service components, double-lines represent layers boundaries. As you can see, <strong>all the inter-boundary communication is through interfaces</strong>, which define an API of every layer.</p> \r\n<p>Finally we have all the blocks we need to build our service.</p> \r\n<p>To keep things clean, we create a database configuration in a separate file:</p> \r\n<p><code><strong>config/db.config.php</strong></code></p> \r\n<pre>define(\'DB&#95;TYPE\', \'mysql\'); \r\ndefine(\'DB&#95;HOST\', \'localhost\'); \r\ndefine(\'DB&#95;NAME\', \'blog\'); \r\ndefine(\'DB&#95;USER\', \'root\'); \r\ndefine(\'DB&#95;PASS\', \'1234\');&nbsp;</pre> \r\n<p>This configuration will be loaded withing the application code.</p> \r\n<p>The application&nbsp;code is here to <strong>serve the request</strong> and&nbsp;<strong>assembly the components</strong>, this the only &quot;dirty&quot; code doing all the injections; a dependency-injection framework could be used here as well:</p> \r\n<p><code><strong>articles.php</strong></code></p> \r\n<pre class=\"brush: php\">header(\"Access-Control-Allow-Origin: &#42;\");\r\nheader(\"Content-Type: application/json; charset=UTF-8\");\r\n  \r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/config/db.config.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/application/ArticleController.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/infrastructure/ArticleRepoPDO.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/infrastructure/db/DatabaseFactory.php\';\r\n\r\n$db = DatabaseFactory::getDatabase(DB&#95;TYPE, DB&#95;HOST, DB&#95;NAME, DB&#95;USER, DB&#95;PASS);\r\n\r\n$repo = new ArticleRepoPDO($db-&gt;getConnection());\r\n\r\n$controller = new ArticleController($repo);\r\n\r\n$response = null;\r\n\r\nswitch ($&#95;SERVER[\'REQUEST&#95;METHOD\']) {  \r\n  case \'GET\':\r\n    if (isset($&#95;GET[\'id\'])) {\r\n      $response = $controller-&gt;detailRequest($&#95;GET[\'id\']);\r\n      \r\n    } else {\r\n      $response = $controller-&gt;listRequest($&#95;GET);\r\n    }\r\n    if ($response === null) {\r\n      http&#95;response&#95;code(404);\r\n      \r\n    } else {\r\n      echo json&#95;encode($response);\r\n    }\r\n    break;\r\n    \r\n  case \'POST\':\r\n    $&#95;DATA = parseRequestData();\r\n    $response = $controller-&gt;createRequest($&#95;DATA);\r\n    \r\n    if ($response === null) {\r\n      http&#95;response&#95;code(400);       \r\n    \r\n    } else {\r\n      http&#95;response&#95;code(201);\r\n      echo json&#95;encode($response);\r\n    }\r\n    break;                           \r\n  \r\n  case \'PUT\':\r\n    if (!isset($&#95;GET[\'id\'])) {\r\n      http&#95;response&#95;code(400);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Missing \"id\" parameter.\'));\r\n    }\r\n  \r\n    $&#95;DATA = parseRequestData();\r\n    $response = $controller-&gt;updateRequest($&#95;GET[\'id\'], $&#95;DATA);\r\n    \r\n    if ($response === false) {\r\n      http&#95;response&#95;code(404);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Article not found.\'));\r\n    } else {\r\n      http&#95;response&#95;code(204);\r\n    }\r\n    break;\r\n    \r\n  case \'DELETE\':\r\n    if (!isset($&#95;GET[\'id\'])) {\r\n      http&#95;response&#95;code(400);\r\n      header(\"Location: {$&#95;SERVER[\'REQUEST&#95;URI\']}/{$response}\");\r\n    }\r\n    \r\n    $response = $controller-&gt;deleteRequest($&#95;GET[\'id\']);\r\n    \r\n    if ($response === false) {\r\n      http&#95;response&#95;code(404);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Article not found.\'));\r\n    } else {\r\n      http&#95;response&#95;code(204);\r\n    } \r\n    break;\r\n    \r\n  case \'OPTIONS\':\r\n    header(\'Allow: GET POST PUT DELETE OPTIONS\');\r\n    break;\r\n        \r\n  default:\r\n    http&#95;response&#95;code(405);\r\n    header(\'Allow: GET POST PUT DELETE OPTIONS\');\r\n}\r\n\r\n// ///////// HELPER FUNCTIONS /////////////////////////////////////\r\n\r\nfunction parseRequestData() {\r\n  $contentType = explode(\';\', $&#95;SERVER[\'CONTENT&#95;TYPE\']);\r\n  $rawBody = file&#95;get&#95;contents(\'php://input\');\r\n  $data = array();\r\n  \r\n  if (in&#95;array(\'application/json\', $contentType)) {\r\n    $data = json&#95;decode($rawBody, true);\r\n    \r\n  } else {\r\n    parse&#95;str($data, $data);\r\n  }\r\n  \r\n  return $data;\r\n}</pre> \r\n<h2>Production</h2> \r\n<p>In production we want our API to be agnostic of the underlying technology. We can achieve that via setting rules of the HTTP server:</p> \r\n<p><code><strong>.htaccess</strong></code></p> \r\n<pre class=\"brush: plain\">RewriteEngine On\r\nRewriteRule ^articles$ articles.php [NC,L]\r\nRewriteRule ^articles/([0-9]+)$ articles.php?id=$1 [NC,L]&nbsp;</pre> \r\n<p>And now we can call the endpoint without the <code>.php</code> suffix.</p> \r\n<h2>Usage&nbsp;</h2> \r\n<p>The usage is pretty straight-forward:</p> \r\n<pre>curl http://localhost/articles\r\ncurl http://localhost/articles?categoryId=1\r\ncurl http://localhost/articles?authorId=1\r\ncurl http://localhost/articles?categoryId=1&amp;authorId=1\r\n\r\ncurl http://localhost/articles/1\r\n\r\ncurl http://localhost/articles -X POST -H \"Content-Type: application/json\" -d @data.json\r\n\r\ncurl http://localhost/articles/4 -X PUT -H \"Content-Type: application/json\" -d @data.json\r\n\r\ncurl http://localhost/articles/4 -X DELETE&nbsp;</pre> \r\n<p><code><strong>data.json</strong></code></p> \r\n<pre class=\"brush: plain\">{ \"title\": \"New article\", \r\n  \"summary\": \"Lorem ipsum\", \r\n  \"body\": \"Lorem ipsum dolor sit amet\", \r\n  \"createdAt\": \"2018-05-03\", \r\n  \"categoryId\": 1, \r\n  \"authorId\": 1 \r\n}&nbsp;</pre> \r\n<h2>Source Code&nbsp;</h2> \r\n<p>You can find the whole project source code on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/php-rest-articles\" title=\"Source code\">GitHub</a>.</p> \r\n<p>Enjoy!&nbsp;</p> \r\n</div>', 'false', 'false', 1, 1),
(34, 'building-a-simple-rest-api', 1525592000, 'Building a simple REST API', 'In my previous article I showed how to build a simple RESTful microservice. For sake of simplicity I skipped some good practices for building REST APIs and I feel a bit guilty about that. I think that topic deserves its own article. And here we go... we take a look at the previous API and try to do things better.', '<h2>API Requirements</h2> \r\n<p>What is the business case of our API? It\'s a simple <strong>public</strong> API for a blog (actually this blog). The public here means, it allow the unauthorized uses to read the content. Not more, not less.</p> \r\n<ul> \r\n<li>information about the blog (name, description, menu items, ...)</li> \r\n<li>collection of articles (posts)</li> \r\n<li>filtered collection of articles&nbsp;(by the category and author)</li> \r\n<li>pagination of the articles (first, last, next and previous page)</li> \r\n<li>detail of an article (the whole text content)</li> \r\n</ul> \r\n<p>The administration API is built out of this scope and I will maybe talk about that in another article later. For the administration API we can expect:</p> \r\n<ul> \r\n<li>user management</li> \r\n<li>blog management</li> \r\n<li>articles management&nbsp;</li> \r\n</ul> \r\n<h2>Problems with the Old API</h2> \r\n<p>When we don\'t talk about the missing AuthN/AuthZ, we find a lack of <strong>hyper-navigation in the API</strong>. Without knowing how the endpoints look like (even it follows the REST recommendations) there is no way <strong>how to navigate the client automatically</strong>.</p> \r\n<p>Calling the API self results to the response:</p> \r\n<pre>GET /api/ HTTP/1.1\r\nHTTP/1.1 403 Forbidden</pre> \r\n<p>This is not so bad, because we built a microservice API only for the articles management. In this case, we need a billboard endpoint. The billboard shows information about the blog and navigation to other endpoint.</p> \r\n<p>Another problem is with the navigation, resp. no navigation, within the API. When we call the <code>/api/articles</code> endpoint, the response look like:</p> \r\n<pre>[\r\n  {\r\n    \"id\" : 4,\r\n    \"title\" : \"New article\",\r\n    \"summary\" : \"Lorem ipsum\",\r\n    \"body\" : null,\r\n    \"createdAt\" : \"2018-05-03\",\r\n    \"categoryId\" : 1,\r\n    \"author\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\"\r\n    }\r\n  },\r\n  ... another articles come here\r\n]</pre> \r\n<p>First problem is with the structure itself. What does the attribute <code>body</code> there? It\'s always null because we expose the whole business object into the API even when some attributes are never set by the underlying service.</p> \r\n<p>Second, the response contains all the data, but we have no idea how to get the detail of an article, next page of the collection or how to filter the articles by the category ID.</p> \r\n<h2>New API&nbsp;</h2> \r\n<h3>The Billboad</h3> \r\n<p>To&nbsp;fulfill our needs we can design the response of the billboard endpoint <code>/api</code> as following:</p> \r\n<pre>{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api\",\r\n  \r\n  \"title\" : \"My Blog\",\r\n  \"description\" : \"A small blog about programming.\",\r\n  \r\n  \"categories\" : [ {\r\n      \"id\" : 1, \r\n      \"name\" : \"Programming\"\r\n    }, {\r\n      \"id\" : 2,\r\n      \"name\" : \"Another stuff\"\r\n    } ],\r\n\r\n  \"authors\" : [ {\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\",\r\n      \"id\" : 1 \r\n    } ],\r\n      \r\n  \"links\" : [ {\r\n      \"rel\" : \"articles\",\r\n      \"href\" : \"/api/articles\"\r\n    } ] }</pre> \r\n<p> </p> \r\n<p>The JSON document structure is pretty straightforward: the top-level entities are the blog content elements (<em>categories</em>&nbsp;and <em>authors</em>).</p> \r\n<p>The navigation is represented by the entity&nbsp;<code>links</code>, the&nbsp;<code>rel</code> attribute tells the logical name of the item and <code>href</code> attribute points to the resource URL.</p> \r\n<h3>The Articles Collection</h3> \r\n<p>Now, what we take a look at the <code>/api/articles</code> endpoint:&nbsp;</p> \r\n<pre>{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api/articles\",\r\n  \r\n  \"articles\" : [ {\r\n      \"href\" : \"/api/articles/4\",\r\n      \"data\" : {\r\n        \"id\" : 4,\r\n        \"title\" : \"New article\",\r\n        \"summary\" : \"Lorem ipsum\",\r\n        \"createdAt\" : \"2018-05-03\",\r\n        \"category\" : {\r\n          \"id\" : 1,\r\n          \"name\" : \"Programming\"\r\n        },\r\n        \"author\" : {\r\n          \"id\" : 1,\r\n          \"name\" : \"Tomas Tulka\",\r\n          \"email\" : \"tomas.tulka@gmail.com\"\r\n        }\r\n      }      \r\n    },\r\n    ... another articles    \r\n  ],\r\n  \r\n  \"queries\" : [ {\r\n      \"rel\" : \"page\",\r\n      \"name\" : \"page\"\r\n    }, {\r\n      \"rel\" : \"category\",\r\n      \"name\" : \"categoryId\",\r\n    }, {\r\n      \"rel\" : \"author\",\r\n      \"name\" : \"authorId\",\r\n    } ],\r\n  \r\n  \"links\" : [ {\r\n      \"rel\" : \"next\",\r\n      \"href\" : \"/api/articles?page=3\"\r\n    }, {\r\n      \"rel\" : \"previous\",\r\n      \"href\" : \"/api/articles?page=1\"\r\n    }, {\r\n      \"rel\" : \"first\",\r\n      \"href\" : \"/api/articles\"\r\n    }, {\r\n      \"rel\" : \"last\",\r\n      \"href\" : \"/api/articles?page=10\"\r\n    } ] }</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p> </p> \r\n<p>The endpoint have a set of query parameters listed in the <code>queries</code>. Practically it means you can filter the articles by calling the same endpoint with query parameters like <code>/api/articles?categoryId=1&amp;author=1</code>. The values for every query type are known already from the billboard response.</p> \r\n<p>The entity&nbsp;<code>links</code> brings the navigation (pagination).</p> \r\n<h3>The Article Detail&nbsp;</h3> \r\n<p>Last but not least is the <code>/api/articles/{id}</code> article detail endpoint:</p> \r\n<pre>{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api/articles/4\",\r\n  \r\n  \"data\" : {\r\n    \"id\" : 4,\r\n    \"title\" : \"New article\",\r\n    \"summary\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit...\",\r\n    \"body\" : \"Lorem ipsum\",\r\n    \"createdAt\" : \"2018-05-03\",\r\n    \"category\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Programming\"\r\n    },\r\n    \"author\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\"\r\n    }\r\n  } }</pre> \r\n<h2>Further Reading</h2> \r\n<p> </p> \r\n<ul> \r\n<li>The probably best source is the great book&nbsp;<a href=\"http://shop.oreilly.com/product/0636920028468.do\" target=\"_blank\" title=\"RESTful Web APIs book\">RESTful Web APIs</a> by Leonard Richardson.</li> \r\n<li>If you are to build a more sophisticated search API you should consider using&nbsp;<a href=\"https://graphql.org/\" target=\"_blank\" title=\"GraphQL\">GraphQL</a>.&nbsp;</li> \r\n</ul> \r\n<h2>Source Code</h2> \r\n<p>You can find the current implementation (PHP) on <a href=\"https://github.com/net21cz/blog-backend\" title=\"Source code\">GitHub</a>.</p> \r\n<p>Enjoy!&nbsp;&nbsp;</p> \r\n<p> </p>', 'false', 'false', 1, 1),
(35, 'multiple-ldap-servers-integration', 1528815000, 'Multiple LDAP Servers Integration', '<p>When an administative solution (like <a href=\"https://technet.microsoft.com/pt-pt/library/how-global-catalog-servers-work.aspx\" target=\"_blank\">Global Catalog</a>) is not possible or wanted and we have to integrate more LDAP servers under one hood there is a simple way how to do it with <a href=\"https://github.com/TremoloSecurity/MyVirtualDirectory\" taregt=\"_blank\">MyVirtualDirectory</a>.</p>', '<p>MyVirtualDirectory (<strong>MyVD</strong>) offers much more than an integration of multiple LDAP servers, actually <em>anything</em> could be exposed as a LDAP service via MyVD. In this tutorial we focus only on LDAP.</p> \r\n<h2>Simple LDAP Integration</h2> \r\n<p>We begin with a simple example of two LDAP servers integration.</p> \r\n<p>We have one LDAP running in <code>server1.com</code> network on the port <code>398</code> and another running in <code>server2.com</code> on the same port.</p> \r\n<p>We integrate the server in out local network on the port <code>50983</code> as shows the following picture:</p> \r\n<p align=\"center\"><img src=\"/storage/myvd-1.png\" alt=\"Simple LDAP Integration\" /></p> \r\n<p>First, let\'s set up the MyVD server (HTTP in our case).</p> \r\n<pre>server.listener.port=50983\r\n</pre> \r\n<p>That\'s it. Now the integration of the servers:</p> \r\n<pre>server.nameSpaces=server1,server2\r\n \r\nserver.server1.nameSpace=dc=server1,dc=com\r\nserver.server1.weight=100\r\nserver.server1.chain=ldap\r\nserver.server1.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server1.ldap.config.host=server1.com\r\nserver.server1.ldap.config.port=389\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=com\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=com\r\nserver.server1.ldap.config.proxyPass=123\r\n \r\nserver.server2.nameSpace=dc=server2,dc=com\r\nserver.server2.weight=100\r\nserver.server2.chain=ldap\r\nserver.server2.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server2.ldap.config.host=server2.com\r\nserver.server2.ldap.config.port=389\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=com\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=com\r\nserver.server2.ldap.config.proxyPass=123\r\n</pre> \r\n<h2>LDAP Client</h2> \r\n<p>As a non-trivial client example let\'s consider the <a href=\"https://spring.io/guides/gs/authenticating-ldap\" target=\"_blank\">Spring Security LDAP</a>.</p> \r\n<p>Simple said, the Spring Security LDAP does two search queries to the LDAP server:</p> \r\n<ul> \r\n<li>Get a user DN by its username (<code>(uid=&lt;username&gt;)</code>),</li> \r\n<li>get groups by user\'s DN (<code>(member=&lt;userDN&gt;)</code>).</li> \r\n</ul> \r\n<p>Spring allow the user to redefine the search bases and filters for different LDAP structures and uses placeholders (so the filter looks like <code>(uid={0})</code>). For the MyVD setting above let\'s set our client as following:</p> \r\n<p>\r\n<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\"> \r\n<tbody>\r\n<tr>\r\n<td>user search base</td>\r\n<td><code>dc=com</code></td>\r\n</tr> \r\n<tr>\r\n<td>user search filter</td>\r\n<td><code>(uid={0})</code></td>\r\n</tr> \r\n<tr>\r\n<td>groups search base</td>\r\n<td><code>dc=com</code></td>\r\n</tr> \r\n<tr>\r\n<td>groups search filter</td>\r\n<td><code>(uid={0})</code></td>\r\n</tr> \r\n</tbody>\r\n</table>\r\n</p>\r\n \r\n<h2>Namespaces Integration</h2> \r\n<p>In our first example we had two server with one base in common <code>dc=com</code>. But what happend when we have to integrate multiple server with different bases? This is what the property <code>server.&lt;server&gt;.nameSpace</code> is meant for.</p> \r\n<p align=\"center\"><img src=\"/storage/myvd-2.png\" alt=\"Namespaces Integration\" /></p> \r\n<pre>...\r\nserver.server1.nameSpace=dc=mycom,dc=com\r\n...\r\nserver.server1.ldap.config.host=server1.org\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=org\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=org\r\n...\r\nserver.server2.nameSpace=dc=mycom,dc=com\r\n...\r\nserver.server2.ldap.config.host=server2.net\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=net\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=net\r\n...\r\n</pre> \r\n<p>Now we can change our search base to <code>dc=mycom,dc=com</code>. Unfortunately this doesn\'t work. The problem is the DN of the result user entity is mapped to the integration namespace. It means for the username <code>testuser1</code> we get instead of <code>uid=testuser1,ou=people,dc=server1,dc=com</code> a DN <code>uid=testuser1,ou=people,dc=mycom,dc=com</code> and that doesn\'t match the value of the <code>member</code> attribute of group entities.</p> \r\n<p>MyVD brings a solution in <a href=\"https://github.com/TremoloSecurity/MyVirtualDirectory/blob/master/doc/myvd.asc#mapping-inserts\" target=\"_blank\">Mapping Inserts</a>:</p> \r\n<pre>...\r\nserver.server1.chain=dnMapper,ldap\r\n...\r\nserver.server1.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server1.dnMapper.config.dnAttribs=member\r\nserver.server1.dnMapper.config.remoteBase=dc=server1,dc=org\r\nserver.server1.dnMapper.config.localBase=dc=mycom,dc=com\r\n...\r\nserver.server2.chain=dnMapper,ldap\r\n...\r\nserver.server2.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server2.dnMapper.config.dnAttribs=member\r\nserver.server2.dnMapper.config.remoteBase=dc=server2,dc=net\r\nserver.server2.dnMapper.config.localBase=dc=mycom,dc=com\r\n...\r\n</pre> \r\n<p>The DN Attribute Mapper maps specified attributted back to the original namespace. Now the search works again.</p> \r\n<h2>Integration of Heterogeneous Services</h2> \r\n<p>As we mentioned in the beginning MyVD provides the possibility to integrate different services. So what happend when we integrate a standard LDAP server (e.g. <a href=\"https://www.openldap.org/\" target=\"_blank\">OpenLDAP</a>) and Active Directory (<strong>AD</strong>)? We are still on LDAP field but the details are different. For example AD\'s user entity holds the username in an <code>sAMAccountName</code> attribute. This means we have to integrate those heterogeneous attributes to be searchable with one client search query. \r\n</p>\r\n<p align=\"center\"><img src=\"/storage/myvd-3.png\" alt=\"Integration of Heterogeneous Services\" /></p> \r\n<p>Sure, we can compose the search filter like <code>(|(uid={0})(sAMAccountName={0}))</code>, which will work fine, but it <strong>exposes implementation details</strong> to the client and breaks so the encapsulation principle. The client shouldn\'t know anything about the backend server, it should treat the service as a <strong>single LDAP server</strong>.</p> \r\n<p>Fortunately, there is MyVD\'s Attribute Mapper:</p> \r\n<pre>...\r\nserver.server2.chain=uidMapper,dnMapper,ldap\r\n...\r\nserver.server2.uidMapper.className=net.sourceforge.myvd.inserts.mapping.AttributeMapper\r\nserver.server2.uidMapper.config.mapping=sAMAccountName=uid\r\n...\r\n</pre> \r\n<p>Now works everything fine. The whole MyVD configuration file:</p> \r\n<pre>server.listener.port=50983\r\n \r\nserver.nameSpaces=server1,server2\r\n \r\nserver.server1.nameSpace=dc=mycom,dc=com\r\nserver.server1.weight=100\r\nserver.server1.chain=dnMapper,ldap\r\nserver.server1.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server1.ldap.config.host=server1.org\r\nserver.server1.ldap.config.port=389\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=org\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=org\r\nserver.server1.ldap.config.proxyPass=123\r\n \r\nserver.server1.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server1.dnMapper.config.dnAttribs=member\r\nserver.server1.dnMapper.config.remoteBase=dc=server1,dc=org\r\nserver.server1.dnMapper.config.localBase=dc=mycom,dc=com\r\n \r\nserver.server2.nameSpace=dc=mycom,dc=com\r\nserver.server2.weight=100\r\nserver.server2.chain=uidMapper,dnMapper,ldap\r\nserver.server2.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server2.ldap.config.host=server2.net\r\nserver.server2.ldap.config.port=389\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=net\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=net\r\nserver.server2.ldap.config.proxyPass=123\r\n \r\nserver.server2.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server2.dnMapper.config.dnAttribs=member\r\nserver.server2.dnMapper.config.remoteBase=dc=server2,dc=net\r\nserver.server2.dnMapper.config.localBase=dc=mycom,dc=com\r\n \r\nserver.server2.uidMapper.className=net.sourceforge.myvd.inserts.mapping.AttributeMapper\r\nserver.server2.uidMapper.config.mapping=sAMAccountName=uid\r\n</pre> \r\n<p>Happy integrating!</p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(36, 'secure-communication-between-services-in-multitenant-systems', 1530465000, 'Secure Communication between Services in Multitenant Systems', '<p>Implementing a SaaS as a multitenant system brings a lot of benefits. As usual, there are some tradeoffs, too. For example security becomes more complex. Let\'s take a look at possible approaches when implementing security in a multitenant architectrure.</p>', '<p>Let\'s consider a trivial example of a communication between two independent services.</p> \r\n<p><img src=\"/storage/multitenant-security.png\" alt=\"communication between two independent services\" /></p> \r\n<p>Service A and Service B are in different realms untrusted to each other.</p> \r\n<h2>Security Levels</h2> \r\n<p>There are several levels of security when talking about a communication between systems:</p> \r\n<h3>1. Security Level: Don\'t trust anyone</h3> \r\n<p><img src=\"/storage/multitenant-security-level-1.png\" alt=\"1. Security level\" /></p> \r\n<p>This is the most secure model. It can be achive only by an active client establishing the communication.</p> \r\n<p>In this case, services don\'t know anything about each other - the client must know all the communication details and control the whole flow.</p> \r\n<p>If you don\'t trust anyone, you have to do everything by yourself...</p> \r\n<h3>2. Security Level:  In your name</h3> \r\n<p><img src=\"/storage/multitenant-security-level-2.png\" alt=\"2. Security level\" /></p> \r\n<p>In this model, a part of the communication is moved to the service code and the calling service acts partially on behalf of the client.</p> \r\n<p>Everytime the client sends its credentials to the Service A he manifests his trust to the service. The Service A uses client\'s identity to build trust between Service B. The identity can be implemented for example via a security token.</p> \r\n<p>The Service A can\'t communicate with the Service B without client\'s identity so it\'s unable to access any data but the client\'s one.</p> \r\n<h3>3. Security Level:  We\'re all friends</h3> \r\n<p><img src=\"/storage/multitenant-security-level-3.png\" alt=\"3. Security level\" /></p> \r\n<p>On this level you practically loose the multitenant-specific secuirity whatsoever. That could be okay in some cases like logging, collecting anonymous statistics etc., but we have to choose out friends carefully.</p> \r\n<p>If such a kind of trust is supported, the Service A can access any data of any client in the Service B. It can even make an identity up which doesn\'t exist (if the Service B is just blindly accepting the input).</p> \r\n<p>This model can be implemented using <em>API keys</em> or application certificates.</p> \r\n<h3>4. Security Level:  Whatever makes you happy</h3> \r\n<p>Pretty obvious one - the Service B has a public API accesible without any needed trust.</p> \r\n<p>There are definitely valid use-cases, but don\'t make your service public as long as you\'re not 100% sure!</p> \r\n<h2>Real-World Example</h2> \r\n<p>We will show a simple model of a multitenant system based on the <b>second security level</b> as described above.</p> \r\n<p>Let\'s consider a Service A consuming client\'s data and processing them somehow (e.g. saving into a database).</p> \r\n<p>The data can be later used for a post-procesing in the Service B.\r\n</p> \r\n<p>Because the Service A plays only the role of a data source and its business logic is completely independent on the post-processing and the Service B is a general service knowing nothing about its data sources we want to decouple both services as much as possible. Using <em>event-driven architecture</em> makes perfectly sense in this use-case.</p> \r\n<p>We build a plugin Adapter in the Service A to consume the data and forward them to the Service B. The Adapter is conceptually a part of the Service A, but it\'s loosly coupled to it and can be plugged-in/out completely independently on the life-cycle of the service.\r\n\r\n</p> \r\n<p><img src=\"/storage/multitenant-security-example-1.png\" alt=\"Real-World Example 1\" /></p> \r\n<p>When the Client sends data to the Service A (1) he uses a token to authorize himself. The Service A accepts the request, processes it and fires an event including the data (or some reference to them) and authorization token (2). The Adapter reacts on the event and uses the client\'s token to forward the data to the Service B (3).</p> \r\n<p>Gray lines separate three different realms of trust: Client\'s, Service A\'s and B\'s realms. Inside a realm everything trusted, outside the realm nothing is trusted.</p> \r\n<h3>Another example</h3> \r\n<p>If you are developing for the cloud, the following scenario should sound familier to you: You want to process data with a large file attachment. First, you make a request to the service with a security token, a data record is created and a signed upload URL is returned in the response. Second, the client puts the file data to the upload URL.</p> \r\n<p>The Adapter them must save the session information to be able to react to the event when the upload is finished.</p> \r\n<p>To save the token internally in the service is not enough in this situation because the upload can take longer than the token expiration time is. The Adapter have to get a temporary but long enough access to the Service B. This can be implemented using API keys.</p> \r\n<p><img src=\"/storage/multitenant-security-example-2.png\" alt=\"Real-World Example 2\" width=\"100%\" /></p> \r\n<p>When the Client sends metadata (like a file-name) to the Service A (1) he uses a token to authorize himself. The Service A accepts the request, processes it, fires an event including the metadata and security token (2), and returns a signed upload URL in the response. The Adapter reacts on the event, uses the client\'s token to demand an API key from the Service B (3A), then saves the API key internal with the metadata. In parallel the Client uses the upload URL to upload a file data to the storage of the Service A (3B). This fires an event containing the file metadata (4). On the event reacts the Adapter by looking up the API key connected to the metadata (5). The Adapter uses then the API key to upload the file data to the Service B (6).</p> \r\n<h4>What about asynchrony?</h4> \r\n<p>One problem could be the absense of order of the received events (that is natural for event-driven systems). It means, the event &quot;file uploaded&quot; can come before the API key retreiving is done or even (but very unlikely) before the &quot;upload requested&quot; event is caught by the Adapter. A solution here is not to consume the &quot;file uploaded&quot; event to be received again and again until the look-up is successful.</p> \r\n<p>Security first!</p>', 'false', 'false', 1, 2),
(37, 'how-aws-lambda-executes-node-js-functions', 1531317000, 'How AWS Lambda Executes Node.js Functions', '<p>Based on <a href=\"https://stackoverflow.com/questions/51037262/aws-lambda-javascript-sdk-async-handler\" target=\"_blank\">my question</a> on StackOverflow I did a bit investigation about how (likely) are Node.js functions called in AWS Lambda containers.</p>', '<p>First, I have to mention I don\'t have the actual implementation of the Node.js AWS Lambda container executors, so all following is only a simple thought experiment. But it seems to be very likely and can definitely help with understanding <em>how are Node.js handler functions executed in AWS Lambda</em>.</p>\r\n\r\n<p>The problem as described in the question is following:</p>\r\n<p>A simple function with an asynchrony inside is not correctly executed when the handler function is declared as <code>async</code>.</p>\r\n<pre class=\"brush: javascript\">\r\nexports.handler = async (event, context, callback) => {\r\n  setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n</pre>\r\n<p>The result of the execution of this Lambda is <code>null</code>.</p>\r\n<p>When the keyword <code>async</code> is removed from the handler everything works fine and the result is <code>resolved</code> as expected.</p>\r\n\r\n<p>So, what is the difference?</p>\r\n\r\n<p>According <a href=\"https://docs.aws.amazon.com/lambda\" target=\"_blank\">the documentation</a>, the asynchronous handlers are executed without using the callback function and the result of the <code>return</code> is used instead.</p>\r\n\r\n<p>I tried to simulate the executor code and came up with this:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar handler1 = async (event, context, callback) => {\r\n  setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n\r\nvar handler2 = async (event, context, callback) => {\r\n  return \"resolved\"\r\n}\r\n\r\nvar handler3 = (event, context, callback) => {\r\n    setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n\r\n// main thread function\r\n(async function main() {\r\n  var handler = handler1 || handler2 || handler3 // set the handler \r\n\r\n  var call = () => new Promise(async (resolve, reject) => {\r\n    var event = undefined\r\n    var context = undefined\r\n    \r\n    if (isAsync(handler)) {\r\n      try {\r\n        // await the result\r\n        var res = await handler(event, context, (err, succ) => {})\r\n        resolve(res)        \r\n      } catch (err) {\r\n        reject(err)\r\n      }      \r\n    } else {\r\n      // use the callback to get the result\r\n      handler(event, context, function(err, succ) {\r\n        if (err) reject(err)\r\n        else resolve(succ)\r\n      })\r\n    }\r\n  })\r\n\r\n  var res = await call()\r\n  console.log(\'RESULT\', res || null)\r\n\r\n})()\r\n\r\nfunction isAsync(fn) {\r\n   return fn.constructor.name === \'AsyncFunction\'\r\n}\r\n</pre>\r\n\r\n<p>When the handler is set to <code>handler1</code> the result <code>null</code> is returned, when <code>handler2</code> is used the result value is <code>resolved</code> as well as with <code>handler3</code> - exactly the same as observed.</p>\r\n\r\n<h2>Final Thoughts</h2>\r\n<p>Well, the simulation above doesn\'t behave 100%, for example when there is no promising (like <code>setTimeout(...)</code>) in an <code>async</code> handler, the AWS Lambda executor will still take account of the callback function.</p>\r\n<p>And this is exactly what I find so confusing - it\'s easy to overlook that and be suprised of the very new (and very wrong) results...</p>\r\n<p>I wrote the simulation code to understand its behaviour as a rule of thumb - keep in mind:</p>\r\n<ul>\r\n  <li><code>async</code> &rArr; <code>return</code></li>\r\n  <li>no <code>async</code> &rArr; <code>callback</code></li>\r\n</ul>\r\n\r\n<p>Happy serverlessing!</p>  ', 'false', 'false', 1, 1),
(38, 'testing-serverless-systems', 1536921000, 'Testing Serverless Systems', '<p>Designing a system architecture is always about making tradeoffs. Microservices resp. serverless architecture has a lot of benefits, but some drawbacks as well. One of them is testing. <strong>Testing serverless systems is hard</strong>. In this article I will discuss some practices which work for my project.</p>', '<h2>Serverless Systems</h2> \r\n<p>Because the view to serverless microservices can differ, let\'s start with a bit theory to define the terms we will use.</p> \r\n<p>A <em>serverless architecture</em> is built upon <strong>managed elastic simple components</strong>. AWS offers for example components like Lambda functions, DynamoDB NoSQL database, SNS message service etc.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-BigPicture.png\" /></p> \r\n<p>A <em>microservice</em> is my understanding a <strong>logical unit owning all the components</strong> to carry out its whole functionality. Microservices are built around a boundary context (domain-specific) and contains all the functions and resources to be able to run as an autonomous service. All the components are private for the microservice and the functionality is accessible only via an API. If you are adapting Infrastructure as code (and you should) you build a microservice as one stack (1:1). Typically you have an integration build pipeline per microservice. Microservices can communicate via domain events (preferred way) or API calls.</p> \r\n<p>A fictitious on-line store can have for example these microservices:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-StoreExample.png\" /></p> \r\n<p>Very important role in a serverless architecture have <em>functions</em>. Functions (like AWS Lambda or Google Cloud Functions) are the simplest deployable components, typically representing an action around an aggregate (what has to be processed within a transaction).</p> \r\n<p>Applications are clients outside the system using the system functionality via API calls.</p> \r\n\r\n<h2>What and How to Test</h2> \r\n<p>The most problematic term in testing theories is a <em>Unit Test</em>, resp. the <em>Unit</em>. Some articles say it\'s the smallest functional block. Some add that a unit test must run in isolation.</p> \r\n<p>As the best definition I consider: <strong>Unit is a functionality with well-defined boundaries (via API) which carries out a business requirement</strong>. This definition goes thru all the types of tests and should be always borne in mind by writing tests. Depending on the level of abstraction the unit can be a class, function, microservice, even the entire system.</p>\r\n<p>This definition is tricky because it doesn\'t match the general picture we have about what Unit Tesing is. Unfortunately, the name is so common that it\'s impossible to change it. Further in the text we will distinguish <em>test unit</em> (unit under test) from <em>Unit Testing</em> (testing on the lowest level of abstraction).</p>    \r\n\r\n<p>Another tricky term is <em>isolation</em>. Because in real system there are almost no truly isolated components - components work together as a system to provide a feature - so it makes no sense to try to isolate the components by stubbing or mocking them. Such tests have no big value. Consider following pseudo code:</p> \r\n<pre class=\"brush: java\">public class GetProductFunction {\r\n    private static final DbClient db = DbClient.build();\r\n\r\n    public Response handle(Request request, Context context) {\r\n        DbResult result = db.query(\"ProductTable\", \"name, description\", \"id = ?\", request.getProductId());\r\n        return result.found()\r\n               ? new Response(200, result.getSingleItem())\r\n               : new Response(400);\r\n    } }\r\n</pre> \r\n<p>Building a database client is an expensive operation so we keep the instance as a static variable while the function container is warm.</p> \r\n<p>Now we want to test this simple function in isolation. The problem is the variable <code>db</code> is private and final so if we want to stub it we propably end up with a code change like following:</p> \r\n<pre class=\"brush: java\">public class GetProductFunction {\r\n    static DbClient db; \r\n         \r\n    private static DbClient getDbClient() { return db == null ? db = DbClient.build() : db; }\r\n    \r\n    public Response handle(Request request, Context context) {\r\n        DbResult result = getDbClient().query(\"ProductTable\", \"name, description\", \"id = ?\", request.getProductId());\r\n        return result.found()\r\n               ? new Response(200, result.getSingleItem())\r\n               : new Response(404);\r\n    } }\r\n</pre> \r\n<p>Changing the code for sake of a test sucks. There is a lot of problems with this code, but at least we can mock our resource now:</p> \r\n<pre class=\"brush: java\">public class GetProductFunctionTest { \r\n    @Mock\r\n    private DbClient dbClient;\r\n    @Mock\r\n    private Request request;\r\n    @Mock\r\n    private Context context;\r\n\r\n    @Before\r\n    public void setup() { GetProductFunction.db = dbClient;  /* mock the expensive resource */ }\r\n    \r\n    @Test\r\n    public void testSuccess() {\r\n        when(request.getProductId()).thenReturn(\"test-id\");\r\n\r\n        when(dbClient.found()).thenReturn(true);\r\n        when(dbClient.getSingleItem()).thenReturn(\r\n                Result.builder()\r\n                        .put(\"name\", \"test-name\")\r\n                        .put(\"description\", \"test-description\")\r\n                        .build());\r\n\r\n        Response response = new GetProductFunction().handle(request, context);\r\n\r\n        assertEquals(\"test-name\", response.get(\"name\"));\r\n        assertEquals(\"test-description\", response.get(\"description\")); \r\n    }\r\n        \r\n    @Test\r\n    public void testNotFound() {\r\n        when(request.getProductId()).thenReturn(\"test-id\");\r\n\r\n        when(dbClient.found()).thenReturn(false);\r\n        when(dbClient.getSingleItem()).thenThrow(RuntimeException.class);\r\n\r\n        Response response = new GetProductFunction().handle(request, context);\r\n\r\n        assertEquals(404, response.getStatusCode());\r\n    } }\r\n</pre> \r\n<p>When we execute this test it\'s green - cool, great job! Wait, really? What do we actually test here? Apart from the trivial logic (found ⇒ 200, not found ⇒ 404) we test only the behavior we stubbed with the mocks. What\'s the value of testing mocks? - not a big one I guess...</p> \r\n<p>As we saw in the previous example, besides a few exceptions it makes only sense to test components in the system. <b>The unit of test isolation is the test</b>. It means no other test must have any impact on execution or results - tests should run isolated from each other (possibly in paralel). The unit of test isolation is <b>not</b> a class, function or component.</p> \r\n\r\n<p>We <strong>always test only the API (black-boxing), never the implementation (white-boxing)!</strong> There can be cases where implementation testing makes sense, or when we have to mock expensive resources in integration tests, but event in such situations coupling tests with the implementation should be used as little as possible.</p> \r\n\r\n<h2>Test Categories</h2> \r\n<p>Bases on the levels of abstraction of components under test we can distinguish several test categories and which APIs should be visible for the test:</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"1\"> \r\n  <tr> \r\n    <th>Test category</th> \r\n    <th>Component under test</th> \r\n    <th>API</th>\r\n  </tr> \r\n  <tr> \r\n    <td>Unit Testing</td>\r\n    <td>Function</td> \r\n    <td>Public methods / exported functions</td> \r\n  </tr> \r\n  <tr> \r\n    <td>Integration Testing</td> \r\n    <td>Microservice</td> \r\n    <td>Events (REST requests / messages)</td>\r\n  </tr> \r\n  <tr> \r\n    <td>End-to-End Testing</td> \r\n    <td>System</td> \r\n    <td>API Gateway</td>\r\n  </tr> \r\n</table> \r\n</p> \r\n<p>What about testing of the (client) applications? Well, an application is actually an individual system so we can apply the entire testing model on it.</p> \r\n<p>The test category determines how and where in the build pipeline the test is executed in:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-Pipeline.png\" /></p> \r\n<p>A software delivery strategy cloud look like following scenario (an example):</p> \r\n<ul> \r\n  <li>a commit triggers the pipeline</li>\r\n  <li>the code source is fetched</li> \r\n  <li>unit tests are being executed for each function (A, B, ... X)</li>\r\n  <li>the code is being built for each function (A, B, ... X)</li> \r\n  <li>the service is staged into a development environment, all resources are created</li>\r\n  <li>integration tests are being executed for the entire service</li> \r\n  <li>the component is deployed into a QA environment</li> \r\n  <li>end-to-end testing pipeline is triggered or scheduled to run tests with a new version of the system</li> \r\n  <li>application pipeline is triggered to be tested with a new version of the system</li> \r\n  <li>the system is ready to be released into production</li> \r\n</ul> \r\n\r\n<h2>Unit Testing</h2> \r\n<p><em>Unit Tests</em> operate on the lowest level. Unit Tests are parts of the codebase, having direct access to code they can test units in different layers without necessarily being deployed. As all the other tests, <strong>Unit Tests verify correct behavior only thru units API - interfaces or public functions -  they never test the implementation</strong>.</p>\r\n<p>Unit Tests verify business requirements. They are executed in the build phase, because it makes no sense to deploy code which doesn\'t match business rules.</p>\r\n<p>Unit Tests should be written by developers in the same programming language as the function and be a part of the function code base.</p> \r\n<p>As an example we have an AWS Lambda <code>create-product</code>* and the function <code>createProduct()</code> as the test unit; the function implements business rules saying the product price must be greater than zero and the product name must be included in the product description (probably for SEO reasons). Consider this Node.js code:</p> \r\n<pre class=\"brush: javascript\">// create-product.js\r\nconst uuidv1 = require(\'uuid/v1\')\r\n\r\nmodule.exports = ({name, description, price}) =&gt; {\r\n    if (price <= 0) throw new Error(\'Price must be greater than zero.\')\r\n    if (!description.includes(name)) throw new Error(\'Description must include name.\')\r\n    return { id: uuidv1(), name, description, price}\r\n}\r\n\r\n// handler.js\r\nconst AWS = require(\'aws-sdk\')\r\nconst dynamoDb = new AWS.DynamoDB.DocumentClient({apiVersion: \'2012-08-10\'})\r\n\r\nconst createProduct = require(\'./create-product\')\r\n\r\nexports.handler = async (event) =&gt; {\r\n    try {\r\n        const product = createProduct(event)\r\n        persistProduct(product)\r\n        return { statusCode: 201, body: product.id }\r\n    } catch (err) {\r\n      return { statusCode: 400, body: err.message } \r\n    }\r\n}\r\n\r\nasync function persistProduct(product) {\r\n    const params = {\r\n        TableName: \'ProductTable\',\r\n        Item: product\r\n    }\r\n    await dynamoDb.put(params).promise()\r\n}\r\n</pre>\r\n<p>And a test for it:</p> \r\n<pre class=\"brush: javascript\">describe(\'Unit test to create a product.\', () =&gt; {\r\n    const testProduct = {\r\n        name: \'Product 123\',\r\n        description: \'Product 123 Desc\',\r\n        price: 123.4\r\n    }                                              \r\n\r\n    const createProduct = require(\'../create-product\')\r\n      \r\n    it(\'Product should be created.\', () =&gt; {\r\n        const product = createProduct(testProduct)    \r\n        expect(product.id).toBeDefined()\r\n    })\r\n    \r\n    it(\'Product must be greater than zero.\', () =&gt; {\r\n        const product = { ...testProduct, price: 0 }\r\n        expect(() =&gt; createProduct(testProduct)).toThrow()\r\n    }) \r\n    \r\n    it(\'Description must include product name.\', () =&gt; {\r\n        const product = { ...testProduct, description: \'junk\' }\r\n        expect(() =&gt; createProduct(testProduct)).toThrow()\r\n    })\r\n})\r\n</pre>\r\n\r\n<h2>Integration Testing</h2> \r\n<p><em>Integration Testing</em> in serverless systems operates on level of <strong>microservices</strong>. We test the whole use-case by verifying communication and interactions between functions and other resources.</p> \r\n \r\n<p>Because <strong>Integration Tests are testing already deployed services</strong> there is no more the requirement to use the same programming language as the service is written in (we can all it \"polyglot microservices\"). But as well as Unit Tests the Integration Tests should be created and maintained by developers.</p> \r\n<pre class=\"brush: javascript\">const AWS = require(\'aws-sdk\')\r\nconst lambda = new AWS.Lambda({apiVersion: \'2015-03-31\'})\r\n\r\ndescribe(\'Integration test to create and persist a new product.\', () =&gt; {\r\n	  const testProduct = {\r\n        name: \'Product 123\',\r\n        description: \'Product Desc 123\',\r\n        price: 123.4\r\n    }       \r\n\r\n    it(\'Product should be available after created.\', async () =&gt; {\r\n        // create a new product\r\n        const createParams = {\r\n            FunctionName: \'create-product\',\r\n            Payload: JSON.stringify({\r\n                name: testProduct.name, \r\n                description: testProduct.description,\r\n                price: testProduct.price\r\n            })\r\n        }\r\n        const createResponse = await lambda.invoke(createParams).promise()\r\n    \r\n        expect(createResponse).toBeDefined()\r\n        expect(createResponse.statusCode).toBe(201)\r\n        expect(createResponse.body).toBeDefined()\r\n        \r\n        const productId = createResponse.body\r\n\r\n        // the product must be available now\r\n        const getParams = {\r\n            FunctionName: \'get-product\',\r\n            Payload: JSON.stringify({\r\n                id: testProduct.id\r\n            })\r\n        }\r\n        const getResponse = await lambda.invoke(getParams).promise()\r\n\r\n        expect(getResponse).toBeDefined()\r\n        expect(getResponse.statusCode).toBe(200)\r\n        expect(getResponse.body).toBeDefined()\r\n        \r\n        const foundProduct = JSON.parse(getResponse.body)\r\n        expect(foundProduct).toEqual(testProduct)\r\n    })\r\n\r\n    function sleep(ms) { /* ... */ }\r\n})\r\n</pre> \r\n\r\n<h2>End-to-End Testing</h2> \r\n<p>The <em>end-to-end tests</em> validate behavior of the <strong>entire system or its sub-systems</strong> and run apart from a feature (microservice\'s) build pipeline.</p> \r\n<p>They can have a form of <em>acceptance tests</em> where whole paths are tested (1. search a product, 2. order the product, 3. create an invoice, 4. delivery etc.) or <em>automated GUI tests</em> which are testing the system from the client\'s point of view.</p>\r\n<p>End-to-end tests could be generally written by someone else than by the developer. Especially GUI tests can be fully in hands of testers.</p> \r\n<p>Of course, the <em>manual testing</em> belongs to this category as well.</p> \r\n\r\n<p>In the previous integration-testing example both functions for saving and searching products were a part of one microservice - it means they lie in one build pipeline. If these functions were parts of different microservices the test would belong to the end-to-end testing category.</p>\r\n\r\n<p>As an example let\'s consider a Catalog microservice - after creating a new product via <code>create-product</code> function a domain event like <code>NEW_PRODUCT_CREATED</code> should be fired. The <strong>event is a part of the internal API</strong> of the service and can be consumed by any other service registered to it. Typically an indexing service reacts to the event by extracting meta-data from the product and provides a full-search upon them. There should be an integration test validating this behavior.</p>\r\n<p>End-to-end Testing doesn\'t access components directly but only via an API Gateway (e.g. REST).</p> \r\n<pre class=\"brush: java\">public class CreateProductServiceTestE2E {\r\n\r\n    import ...\r\n\r\n    private final String SERVICE_API = System.getenv(\"SERVICE_API\");\r\n    private final String PRODUCT_CREATE_ENDPOINT = SERVICE_API + \"/catalog/product\";\r\n    private final String SEARCH_ENDPOINT = SERVICE_API + \"/search\";\r\n\r\n    private HttpClient httpClient;\r\n\r\n    @Before\r\n    public void setup() { this.httpClient = createHttpClient(); }\r\n\r\n    @Test\r\n    public void testProductMetadataShouldIndexed() throws Exception {\r\n        Product product = new Product(\"Product 123\", \"Product Desc 123\", 123.4);\r\n\r\n        // create a new product\r\n        HttpPost createRequest = httpClient.post(PRODUCT_CREATE_ENDPOINT);\r\n        createRequest.setHeader(\"Content-type\", \"application/json\");\r\n        createRequest.setBody(product.toJson());\r\n\r\n        HttpResponse createResponse = httpClient.execute(createRequest);\r\n\r\n        assertEquals(201, createResponse.getStatusCode());\r\n        assertNotNull(createResponse.getBody());\r\n        \r\n        String productId = createResponse.getBody();\r\n\r\n        Thread.sleep(1000); // waiting necessary due to eventual consistency\r\n\r\n        // the product must be indexed and available for searching\r\n        HttpPost searchRequest = httpClient.post(SEARCH_ENDPOINT);\r\n        searchRequest.setHeader(\"Content-type\", \"application/json\");\r\n        searchRequest.setBody(\"{\\\"keyword\\\":\\\"\" + product.getName() + \"\\\"}\");\r\n\r\n        HttpResponse searchResponse = httpClient.execute(searchRequest);\r\n\r\n        assertEquals(200, searchResponse.getStatusCode());\r\n        assertNotNull(createResponse.getBody());\r\n        \r\n        Collection&#x3C;String&#x3E; foundProductIds = toList(createResponse.getBody());\r\n        assertTrue(foundProductIds.contains(productId));\r\n    }\r\n}\r\n</pre>\r\n\r\n<h2>Summary</h2> \r\n<p>In this article I wanted to recap my experience with functional testing of serverless systems and simplify the levels of tests. I reviewed terms Unit-, Integration- and End-to-end Testing to show what is under test and where such tests lie in the development process.</p> \r\n<p>Despite the fact that I focused only on functional testing doesn\'t mean that testing non-functional requirements like performance, security, scalability etc. are not important...:-)</p> \r\n\r\n<p>Happy testing!</p> <hr /> \r\n\r\n<p><em>* I use verbs to name functions because it sounds natural to me. A function is in fact an action.</em></p>', 'false', 'false', 1, 1),
(39, 'function-separation-in-a-microservice', 1538200000, 'Function Separation in a Microservice', '<p>Talking about serverless microservices, functions are the basic building blocks of the service functionality. How to design them from the code and deployment perspektive?</p>', '<h2>Microservices Anatomy</h2>\r\n<p>Let\'s simply define a microservice as an <strong>autonom service built from one or more resources</strong>. The resource mean functionality (functions*), storage (database), communication (message topic), etc. All the service needs must be included in the stack or given as a parameter (environment variable), API is exposed and besides are all the resource private for the microservice. One microservice &hArr; one stack &hArr; one deployment pipeline.</p>\r\n\r\n<p>According the Single Responsibility Principle a component should do one and only one thing. Applying this rule to the functions level we get a function for an action. Let\'s consider a simple CRUD microservice; all the CRUD operation are represented by simple functions:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-code-samples/master/nodejs-crud-microservice/doc/FunctionSeparation-Product-Service-Overview.png\" width=\"50%\" /></p>\r\n\r\n<p>How does this look like from the dev-ops point of view? We can create a new project artifact for every function. Such code is <strong>easy to reason about, easy to test and deploy</strong>. The last one could be written with a question mark, of course it\'s easy to deploy a function, but don\'t forget there must be pipeline actions for build, deploy and test execution which is really only doing, but it\'s a strenuous and boring job and brings additional complexity.</p>\r\n                                                                                                                                                                               \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/FunctionSeparation-pipeline.png\" width=\"100%\" /></p>\r\n\r\n<p>Similar for the codebase. Take a look at this simple AWS Lambda function in Node.js:</p>\r\n\r\n<pre>\r\n.gitignore\r\ngruntfile.js\r\njasmine.json\r\npackage.json\r\npackage-lock.json\r\nREADME.md\r\n</pre>\r\n\r\n<p>As you can see there several files only to set up the module. This must be multiplied for every function.</p>\r\n\r\n<p>So, a function for an action sounds well in theory but in practise it could be an overkill.</p>\r\n\r\n<h2>Modularization \'till the last step</h2>\r\n<p><strong>Decoupling</strong> is propably the most important concept in the design of software systems. So we never surrender this principle. Anyway we can distinguish between logical and physical coupling. <strong>Two modules can be perfectly decoupled even while physically existing in a same artifact</strong> - when they have no shared code and dependencies only to APIs (following principles of hexagonal architecture).</p>\r\n\r\n<p>Let\'s consider the following implementation of the CRUD microservice:</p>\r\n\r\n<pre>\r\nsrc/\r\n   index.js\r\n   list-products.js\r\n   get-product.js\r\n   create-product.js\r\n   update-product.js\r\n   remove-product.js\r\n</pre>              \r\n\r\n<p><code>index.js</code></p>\r\n<pre class=\"brush: javascript\">\r\nconst list = require(\'./list-products\').handler\r\nconst get = require(\'./get-product\').handler\r\nconst create = require(\'./create-product\').handler\r\nconst update = require(\'./update-product\').handler\r\nconst remove = require(\'./remove-product\').handler\r\n\r\nexports.handler = async function(event) {\r\n    if (!event || !event.resource || !event.httpMethod) {\r\n        return buildResponse(400, {error: \'Wrong request format.\'})\r\n    }\r\n\r\n    if (event.resource === \'/\') {\r\n\r\n        switch (event.httpMethod) {\r\n            case \'GET\':\r\n                return await list(event)\r\n            case \'POST\':\r\n                return await create(event)\r\n            default:\r\n                return buildResponse(405, {error: \'Wrong request method.\'})\r\n        }\r\n    }\r\n    else if (event.resource === \'/{id}\') {\r\n\r\n        switch (event.httpMethod) {\r\n            case \'GET\':\r\n                return await get(event)\r\n            case \'PUT\':\r\n                return await update(event)\r\n            case \'DELETE\':\r\n                return await remove(event)\r\n            default:\r\n                return buildResponse(405, {error: \'Wrong request method.\'})\r\n        }\r\n    } else {\r\n        return buildResponse(404, {error: \'Resource does not exist.\'})\r\n    }\r\n}\r\n\r\nfunction buildResponse(statusCode, data = null) {\r\n    return {\r\n        isBase64Encoded: false,\r\n        statusCode: statusCode,\r\n        headers: {\r\n            \'Content-Type\': \'application/json\'\r\n        },\r\n        body: JSON.stringify(data)\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>As you can see, the main handler is only a router to other functions, which look like following:</p>\r\n\r\n<p><code>get-product.js</code></p>\r\n<pre class=\"brush: javascript\">\r\nconst AWS = require(\'aws-sdk\')\r\n\r\nconst PRODUCT_TABLE = process.env.PRODUCT_TABLE\r\n\r\nconst dynamoDb = new AWS.DynamoDB.DocumentClient({apiVersion: \'2012-08-10\'})\r\n\r\nexports.handler = async function(event) {\r\n    if (!event || !event.httpMethod) {\r\n        return buildResponse(400, {error: \'Wrong request format.\'})\r\n    }\r\n    if (event.httpMethod !== \'GET\') {\r\n        return buildResponse(405, {error: \'Wrong request method - only GET supported.\'})\r\n    }\r\n    if (!event.pathParameters || !event.pathParameters.id) {\r\n        return buildResponse(400, {error: \'Wrong request - parameter product ID must be set.\'})\r\n    }\r\n\r\n    try {\r\n        const response = await dispatch(event.pathParameters.id)\r\n        if (response) {\r\n            return buildResponse(200, response)\r\n        } else {\r\n            return buildResponse(404, {error: \'Product was not found.\'})\r\n        }\r\n        \r\n    } catch (ex) {\r\n        console.error(\'ERROR\', JSON.stringify(ex))\r\n    }\r\n}\r\n\r\nasync function dispatch(id) {\r\n    const product = getProduct(id)\r\n    return product\r\n}\r\n\r\nasync function getProduct(id) {\r\n    const params = {\r\n        TableName: PRODUCT_TABLE,\r\n        Key: { productId: id }\r\n    }\r\n    const res = await dynamoDb.get(params).promise()\r\n    \r\n    return (res.Item) \r\n        ? { id: res.Item.productId, \r\n            name: res.Item.name, \r\n            description: res.Item.description, \r\n            price: res.Item.price }\r\n        : null\r\n}\r\n\r\nfunction buildResponse(statusCode, data = null) {\r\n    return {\r\n        isBase64Encoded: false,\r\n        statusCode: statusCode,\r\n        headers: {\r\n            \'Content-Type\': \'application/json\'\r\n        },\r\n        body: JSON.stringify(data)\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>The Single Responsibility Principle is applied here. The function has its own handler which takes care of the incoming HTTP request. All the HTTP-related checks and transformations must be done here. The handler is the only one place for such a dirty code. After the request is checked, dispatching is executed. This is something like a main method, an entry point to the business logic. In this case just getting the product details.</p> \r\n\r\n<p>And other functions in the same spirit...</p>\r\n\r\n<p>The actions are decoupled from each other and are ready to be separated into individual functions in the next last step. But staying here we still have a very clear code easy to test and reason about and we don\'t need to adapt the pipeline onto this level of granularity.</p>                                       \r\n  \r\n<h2>Test it!</h2>\r\n<p>It\'s very easy to test a service built in this way. We write a Unit Test Suite for every action (exported function), an Integration Test Suite for the service API (index.js) and End-to-End Test Suite for the Gateway API facade.</p> \r\n  \r\n<h2>Source Code</h2>\r\n<p>The whole stack could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/nodejs-crud-microservice\">GitHub</a>.</p>\r\n\r\n<p>Happy coding!</p>  \r\n\r\n<hr/>\r\n<p>* <em>Functions are sometimes called \"nanoservices\" as they are smaller than microservices. Actually, I don\'t even like the term \"microservice\" because it confuses people regarding its size (\"How small should a microservice be?\"), so I don\'t like to develop this terminology on in the same manner.</em>', 'false', 'false', 1, 1),
(40, 'rollback-and-microservices', 1538900000, 'Rollback and Microservices', '<p>Thinking about a deployment strategy for a microservices-based system it\'s natural to consider a <em>rollback</em>. Before looking for a technical solution, let\'s discuss this idea conceptually. <strong>Is it even possible to roll microservices back?</strong></p>', '<h2>Microservices</h2>\r\n<p>One of a plenty definitions of a microservice is an <strong>independently deployable</strong> component. This says actually a pretty lot. To understand this definition we have go back to the very first motivation for the microservices design pattern: <em>We want to deliver our product as soon as possible</em>. A microservice could be in hands of one team and have a completely different and unsynchronous delivery process. A team developing a service A don\'t want to be bothered by waiting for a next release of a service B if it doen\'t need its new functionality. The service A could be deployed into the production once a day and the service B once a month.</p>\r\n<p>For all this to be possible we have to follow a few basic principles, especially the most important one in our context: <strong>No Breaking Changes</strong>. We may find the same principle under different names like <a href=\"https://www.allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\" target=\"_blank\">\"APIs are forever\"</a> etc., but the idea is the same.</p>\r\n\r\n<h2>No Breaking Changes</h2>\r\n<p>How does this principle makes the microservices deployment independent? Let\'s consider two services, A and B. B is calling A via A\'s API. In the time of development of a version v1 of the service B, there is a released API of the service A in the version v1. So the service B v1 makes its call against the service A v1:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-1.png\" width=\"70%\" alt=\"Service Bv1 calls Av1\" /></p>\r\n\r\n<p>During the development of the service B v1 a new version of the service A - v2 - was released. An additional endpoint was added, but it\'s okay for the service B because A\'s API was not broken (in other words, A v2 is compatible with A v1). Actually, so far there are no breaking changes the service B doesn\'t care which newly realeased version of the service A is talking to:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-2.png\" width=\"70%\" alt=\"Service Bv1 calls Av2\" /></p>\r\n\r\n<h2>What about Rollbacks?</h2>\r\n<p>Now consider that the service B in some future version (let\'s say v2) is using an endpoint of the service A added in its release v2:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-3.png\" width=\"70%\" alt=\"Service Bv2 calls Av2\" /></p>\r\n\r\n<p>After some time the developers of the service A find an error in the v2. They decide to rollback to the previous version (v1). What happens to the system? The service B stops working because it keeps calling the endpoint of the service A v2 which doesn\'t exist anymore. The <strong>rollback to a previous version is a potential breaking change</strong>:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-4.png\" width=\"70%\" alt=\"Service Bv2 calls Av1\" /></p>\r\n\r\n<p>Because it can introduce a breaking change as the rest of the system depends on the current API a <strong>rollback should be avoided in microservices-based systems</strong>.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>As long as we can\'t rollback microservices, we have to find a confident way how to deploy them into the production. There are a lot good practises like separated deployment stages (development, testing, QA, ...), testing in a production-like environment, blue-green and canary deployment, and so on.</p>\r\n<p>It\'s also very important to resist the temptation of hotfixing a bug directly in the production environment. We alywas have to adapt the continuous-delivery principles: Every change, bug-fixes included, must be properly built and tested thru the standard deployment process (delivery pipeline).</p> \r\n<p>Microservices are living organisms and should be treated like that.</p>\r\n\r\n<p>Happy deploying!</p>', 'false', 'false', 1, 2),
(41, 'serverless-blue-green-deployment', 1540023000, 'Serverless Blue-Green Deployment', '<p>Blue-Green Deployment is a very good technique which has been successfully used for <strong>managing releases of cloud applications</strong>. Now it\'s time to rethink it a bit for serverless systems.</p>', '<h2>Blue-Green Deployment</h2>\r\n<p>The concept is pretty easy: We have <strong>two identical production environments</strong> (<i>blue</i> and <i>green</i>), the green is &quot;live&quot; as default. When we release a new version, we deploy first into the blue environment. There we can perform some tests and then switch routing. The blue enviroment becomes &quot;live&quot;. If something goes wrong we should be able to <strong>switch back in less than a second</strong>.</p>\r\n   \r\n<h2>Problems with Serverless</h2>\r\n<p>So, why don\'t we use the same strategy for serverless deployment? We have several challenges to solve:</p>\r\n<ul>\r\n  <li><strong>No database integration</strong> - a service should contain its own data storages as a part of the stack.</li>\r\n  <li><strong>Independent deployment lifecycles</strong> - there is no synchronization between deployments.</li>\r\n  <li><strong>Location transparency</strong> - the topology of serverless systems is dynamic.</li> \r\n</ul>\r\n<p>Is there any solution for those problems? Maybe yes, but the cost of complexity would be too high. On the other hand, this doesn\'t mean we have to abandon the blue-green deployment completely. We can still benefit from the idea, just on another level.</p>\r\n\r\n<h2>Blue-Green for Test-Isolation</h2>\r\n<p>Let\'s image the following scenario: We have a test stage where all the integration tests are executed in. This stage is shared by all the service deployment pipelines.</p>\r\n<p>What happens when a test of the service A runs at the same time as another test of the service B, which uses the service A as an external resource? When the new version of the service A doesn\'t work, it has a negative impact on the test results. And that\'s the point where the blue-green deployment comes to rescue!</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Serverless-BlueGreen-Deployment.png\" width=\"100%\" alt=\"Serverless Blue-Green Deployment\" /></p>\r\n\r\n<p>As you can see in the picture above, when a new version of the service A (Av2) is about to be tested, it is deployed parallel to the previous version stack (Av1). The <i>blue</i> version is integrated in the system as well as the <i>green</i> version. If another test comes along, it sees only the already tested stack Av1.</p>\r\n\r\n<h2>Source Code</h2>\r\n<p>You can find a simple implementation of the serverless blue-green development for test-isolation in my <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/serverless-blue-green-deployment\">Github account</a>. The example uses AWS CodePipeline.</p>\r\n\r\n<p>Be blue-green, be happy!</p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(42, 'use-case-driven-testing', 1541155000, 'Use-Case-Driven Testing', '<p>Why shouldn\'t we test the implemetation? How to decouple our tests from the code? What is the reason to add a new test? Why is mocking a code smell? In this article I will try to find answers to those questions.</p>', '<p>In <a href=\"/testing-serverless-systems\">my previous article</a> I opened the topic and defined some important terms and principles I find good to follow. Let me summarize some of them and continue in the theory how we should design our tests.</p>\r\n\r\n<h2>Contracts Testing vs. Implementation Testing</h2>\r\n<p>We can see a test as a friend of our application but never as its part (tests codebase is never a part of the production application). As a good friend <strong>a test must talk to the application via a contract</strong>. We can call that contract an API. Every well-defined component offers an API.</p>\r\n<p>If we couple our test with the implementation, we have to adapt the test everytime we refactor the application code, which is not only annoying and impractical but unnecessary and even rude (we don\'t snoop around in friend\'s wardrobe). After some time people get sick of double work and start to write fewer tests and/or ignore broken tests.</p>\r\n<p>Tests should <b>not</b> care <em>how</em> is a feature implemented, they <strong>should care only if the behavior is correct</strong>. When we implement a sorting function we should test if the sorting results are correct regardless which sorting algoritm is used.</p>\r\n<p>For example when we test if a record was saved we can use another service to retrieve the record, again via its API. When there is no API to retrieve a record there is no possibility to validate if the record was really save - but such a \"black hole\" system makes no sense.</p>\r\n\r\n<h2>Code-Drive Testing vs. Use-Case-Driven Testing</h2>\r\n<p>It\'s a very popular practice to have a test per class, in Java like following:</p>\r\n<pre>\r\n/src/main/java/Foo.java\r\n              /Bar.java\r\n/src/test/java/FooTest.java\r\n              /BarTest.java              \r\n</pre>\r\n<p>This is a typical example of code-driven testing design. Here is a new class a reason to add a new test. But a class is just an implementation detail. Maybe after some time you find out the class is too big and should be refactored into two classes. Then you have to refactor your test as well although there is no functional change and the test validates always the same behavior. But if you didn\'t do it your test wouldn\'t propably compile. An unhappy situation. <strong>Code-driven testing design makes maintenance hard, expensive and boring.</strong></p>\r\n<p>It is much better to <strong>drive your tests by use-cases</strong>. With this strategy not a class or a method is a reason to add a new test but a new requirement is. The structure looks like following:</p>\r\n<pre>\r\n/src/main/java/Foo.java\r\n              /Bar.java\r\n/src/test/java/UseCase1Test.java\r\n              /UseCase2Test.java              \r\n</pre>\r\n<p>When you decided to split a class into two or to remove one, the tests remain without any change:</p>\r\n<pre>\r\n/src/main/java/Foo1.java\r\n              /Foo2.java\r\n/src/test/java/UseCase1Test.java\r\n              /UseCase2Test.java              \r\n</pre>\r\n<p>In fact, there are only two reasons for a test change: 1. the requirement has changed, 2. there was a bug in the test self.</p>\r\n<p>The design of the test has the following pattern:</p>\r\n<pre class=\"brush: java\">\r\nclass UseCaseTest {\r\n    @Test\r\n    public void requirementOneTest() { \r\n        assert(/* acceptance criterium 1 */);\r\n        assert(/* acceptance criterium 2 */);\r\n        ...\r\n    }\r\n    @Test\r\n    public void requirementTwoTest() { \r\n        assert(/* acceptance criterium 1 */);\r\n        assert(/* acceptance criterium 2 */);\r\n        ...\r\n    }\r\n    ...\r\n}\r\n</pre>\r\n<p>The API is defined by an inteface (each public method of a class should implement an interface) and the implementation should be initialized via a factory, for example:</p>\r\n<pre class=\"brush: java\">\r\n/** src/main/java/MyUseCase.java */\r\npublic interface MyUseCase {\r\n    int func1(String param);\r\n    String func2(int param);\r\n}\r\n\r\n/** src/main/java/Foo.java */\r\nclass Foo implement MyUseCase {\r\n    private Bar bar = new Bar();\r\n    \r\n    public int func1(String param) {\r\n        ...\r\n    }\r\n    public String func2(int param) {\r\n        ...\r\n        return bar.func();\r\n    } \r\n}\r\n\r\n/** src/main/java/Bar.java */\r\nclass Bar {\r\n    String func(int param) {\r\n        ...\r\n    }\r\n}\r\n\r\n/** src/test/java/MyUseCaseTest.java */\r\nclass MyUseCaseTest {\r\n    private MyUseCase myUseCase;\r\n    \r\n    @Before\r\n    public void initialize() {\r\n        myUseCase = new MyDefaultUseCaseFactory().createMyUseCase();\r\n    }    \r\n    @Test\r\n    public void func1Test() { \r\n        assertEquals(/* expected */, myUseCase.func1(...));\r\n        assertEquals(/* expected */, myUseCase.func1(...));\r\n        ...\r\n    }\r\n    @Test\r\n    public void func2Test() { \r\n        assertEquals(/* expected */, myUseCase.func2(...));\r\n        assertEquals(/* expected */, myUseCase.func2(...));\r\n        ...\r\n    }\r\n    ...\r\n}\r\n</pre>\r\n \r\n<h2>Code Coverage vs. Use-Case Coverage</h2>\r\n<p>Another popular practice is code coverage. In some companies there is a code coverage check as a part of the build pipeline and a commit is rejected if its code coverage is less than a particular percentage. <strong>Code coverage is a wrong metric</strong> - you can have 100% code coverage without actually to test anything.</p>\r\n<p>Much more important is <strong>use-case coverage</strong>, telling us how much of our requirements are covered with test. <strong>We should always try to achive 100% use-case coverage</strong>, because satisfaction of use-cases determine the product quality.</p>  \r\n\r\n<h2>Test Categories</h2>\r\n<p>We distinguish several test categories based on the level of granularity the test operates on.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/TestingGranularity.png\" width=\"80%\" /></p>\r\n\r\n<p>As showned in the picture, the <strong>unit testing</strong> operates on the level of functions, it can access the functionality of the smallest components via their APIs defined by public methods or exported function. <strong>Integration testing</strong> sees a service as a black box as well as <strong>system testing</strong>* has no idea about the services structure hidden behind the system\'s facade (gateway).</p>\r\n\r\n<h3>Unit Testing</h3>\r\n<p>Unit tests operate on the level of functions and <strong>should test only business functionality</strong>. If there is no business logic and a requirement is implemented only with composition of another resources (like saving into a database or calling another function/service) there\'s no reason for a unit test and the testing should be done as a part of integration testing (otherwise we write unit test as integrations test where all resources are mocked - such tests have no value).</p>\r\n<p>To make unit testing possible we should always follow the Single Responsibility Principle by writting our code. If more responsibilities are mixed into one component, for example when a function transforms the input and saves it into a database, there is no other way how to test the transformation function but to mock the database. <strong>If we need a mock an dependency to be able to test our business logic in separation, the code probably needs refactoring</strong>.</p>\r\n<p>Another thing which makes writting tests much easier is to avoid side-effects. A function with side-effects cannot be easily tested as a black box with inputs and outputs (both can be absent) and need a knowledge about its interns resp. implementation details. Use pure functions as much as possible.</p>\r\n\r\n<h3>Integration Testing</h3>\r\n<p>Integration Testing operates on the level of services, it means autonomous components, and tests their interactions.</p>\r\n<p>Integration tests must run in the system environment so all the needed resources are available (and we don\'t have to mock anything).</p>\r\n<p>Integration tests as well as unit tests belong to a service and should access only the service\'s API. If a test needs to access more services we call it a system test and it should run accross services (e.g. in a separate testing pipeline).</p>\r\n\r\n<h3>System Testing</h3>\r\n<p>System testing sees the entire system as a black box with a facase (gateway) API. Whole scenarios accross services are under the test.</p>\r\n\r\n<h2>Mocking is a Code Smell</h2>\r\n<p>If you follow the principles above you don\'t need to mock anything in your tests. If you can\'t test without mocking there must be something \"smelly\" in the design.</p>\r\n<p>Of course, there could be exceptions like when a resource is too expensive or slow, but we should avoid mocking as much as possible, because it always removes a value from the tests.</p>\r\n<p>Mocks are coupled to the implementation which makes them brittle. If you really have to mock out an expensive dependency, consider to use a fake object (simple implementations with business behavior) instead. Fake objects, in contrast to mocks, contain domain logic and are a part of the domain. Read more about <a href=\"https://blog.cleancoder.com/uncle-bob/2014/05/14/TheLittleMocker.html\" target=\"_blank\">test doubles</a> and <a href=\"https://martinfowler.com/articles/mocksArentStubs.html\" target=\"_blank\">mocks vs. stubs</a> to understand the difference.</p>\r\n\r\n<h2>Example: Money</h2>\r\n<p>Do you remember the Money example from Kent Beck\'s <a href=\"https://www.oreilly.com/library/view/test-driven-development/0321146530/\" target=\"_blank\">Test-Driven Development By Example</a>? Let\'s use it as a base for our simple Money microservices system implemented with <a href=\"https://aws.amazon.com\" target=\"_blank\">AWS</a>.</p>\r\n<p>The Money service offers an API for multiplication of money amounts and reduction into different currencies. The Exchange service provides information about the actual rate; we can implement it for example as pulling the data from the New York Stock Exchange via its API.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/MoneyExampleMicroservices.png\" width=\"80%\" /></p>\r\n\r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/money\" target=\"_blank\">GitHub</a>. The implementation is far from perfect, but the idea is clear. Handlers (<code>index.js</code>) are separeted from the business (<code>money.js</code> and <code>exchange.js</code>), unit tests focus on the business, integration tests on handlers (which make APIs of the services) and system tests on the facade.</p>\r\n\r\n<p>Each tests have a different place in the build pipeline. Unit tests are executed for every function, integration tests for every service (a bunch of functions) and system tests for the entire system (a bunch of services).</p>\r\n<p>In our example there is only one function per service, but in a real-world scenario there would be much more. We can even think of a function per use-case. In such a case we would have two function in the Money service: <code>times</code> function and <code>reduce</code> function.</p>\r\n\r\n<p>Happy testing!</p>\r\n\r\n<hr />\r\n<p><strong>*</strong>) <em>System testing belongs together with GUI and manual testing into the group End-to-end testing which all operate on the same level of granularity. By system testing is in this article meant automatic testing of back-end functionality e.g. via system\'s REST API.</em></p> ', 'false', 'false', 1, 1),
(43, 'javascript-async-await-in-a-loop', 1541242000, 'JavaScript async/await in a Loop', '<p>Async/await syntax is a great technique how to deal with promises in modern JavaScript. Unfortunately it\'s not always easy to understand how it works which can lead to strange bugs. Let\'s investigate one of them.</p>', '<p>There is a service in our system cleaning up unused resources. Those resources are grouped upon an ID. So the request looks like \"remove all the resources for the ID 123\".</p>\r\n<p>In the service code I have found the following line:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nentryKeys.forEach(async key => await removeResource(key))\r\n</pre>\r\n\r\n<p>Why this doesn\'t work? Let\'s analyse the code a bit. The <code>forEach</code> is just a stream variant of the <code>for</code> loop and it\'s definitely synchronous. So, there shouldn\'t be the problem even when it\'s a bit ineffective. Inside the <code>forEach</code> there is a call of the <code>removeResource</code> function with <code>await</code>. The <code>await</code> tells us that the function returns a promise. The <code>await</code> can be used only within an <code>async</code> function which is fulfilled because the inline (lambda) function is really marked with the <code>async</code> keyword (we can rewrite the same function as <code>async function(key){ await removeReource(key) }</code>). The point is a function declared with <code>async</code> returns always a promise. It means, the <code>forEach</code> fires several <b>asynchronous</b> calls but <b>doesn\'t wait for them</b> (<code>await removeResource(key)</code> is another asynchronous call inside that asynchronous call).</p>\r\n\r\n<p>Now, when we understand why the code doesn\'t work, we can fix it:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nfor (const key of entryKeys) { await removeResource(key) } \r\n</pre>\r\n\r\n<p>This works fine, the only problem is, it\'s synchronous and slow. The loop is waiting for an execution to finish before starting a new one even when it is possible to run them all in parallel.</p>\r\n<p>Can we fix it? Sure!</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nawait Promise.all(entryKeys.map(key => removeResource(key))) \r\n</pre>\r\n\r\n<p>We have changed <code>forEach</code> to <code>map</code>, so we map an array of IDs into an array of promises. Then we pack the promises into a one big promise via <code>Promise.all()</code>. Now we wait for the big promise (and that\'s exactly what we missed before) to be completed. All the call are executed in parallel. Well done.</p>\r\n\r\n<p>You can play with the different variants via the following snippet:</p> \r\n\r\n<pre class=\"brush: javascript\">\r\n(async function(){\r\n    console.log(\"START\")\r\n    \r\n    // doesn\'t work\r\n    //[\"A\",\"B\",\"C\"].forEach(async key => await doSomethingAsync(key))\r\n    \r\n    await Promise.all([\"D\",\"E\",\"F\"].map(key => doSomethingAsync(key)))\r\n    \r\n    // synchronous\r\n    //for (const key of [\"G\",\"H\",\"I\"]) { await doSomethingAsync(key) }   \r\n    \r\n    console.log(\"END\")  \r\n})()\r\n\r\nfunction doSomethingAsync(key) {\r\n    return new Promise(function (resolve, reject) {\r\n        setTimeout(() => {\r\n            console.log(\"RESOLVED \" + key)\r\n            resolve(key)\r\n        }, 1000)\r\n    })\r\n}\r\n</pre>\r\n\r\n<h2>What about Reduce?</h2>\r\n\r\n<p>How could we reduce the value from the promises? Well, of course the reduce needs the already resolved values:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\n// \"JKL\"\r\nvar result = await [\"J\",\"K\",\"L\"]\r\n      .map(key => doSomethingAsync(key))\r\n      .reduce(async (sum,v) => await sum + await v)\r\n</pre>\r\n\r\n<p>Because the whole reduce function is <code>async</code> we have to use <code>await</code> as well to retrieve the value of <code>sum</code>.</p>\r\n\r\n<p>Happy promising!</p>', 'false', 'false', 1, 1),
(44, 'lesson-learned-principles-of-serverless-development', 1546126000, 'Lesson Learned: Principles of Serverless Development', '<p>After some time of developing <strong>serverless</strong> systems (especially on AWS) I take a look back and try to summarize what I have learned so far.</p>', '<p>First, I should explain what I mean by saying <em>serverless</em> as long as this became already a buzzword and could have different meanings.</p> \r\n<p>Let think about a serveless system simply as about <strong>a system with only managed, elastic (auto-scaling) resources</strong>. This means, hardware (CPU, memory, storage) and platform (OS, runtime) provisioning is always included as a part of the cloud-provider service.</p> \r\n<h2>Development</h2> \r\n<ol> \r\n<li>Everything is as serverless as possible, only managed, auto-scaling services are used.</li> \r\n<li>The development of a service is finished, once an infrastructure template exists and runs without error.</li> \r\n<li>Every client communication with the cloud services is possible only via an HTTPS facade API.</li> \r\n<li>Design of an API is use-case- and business-driven as well as the architecture-components.</li> \r\n<li>Implementation has no impact on an API and it\'s completely hidden behind it.</li> \r\n<li>In the implementation, the business-logic is decoupled from the cloud-provider.</li> \r\n<li>References to resources are handed over to services via environment variables.</li> \r\n<li>For each service there are unit + integration tests.</li> \r\n<li>Each service is accessible only via API and events.</li> \r\n<li>Multitenancy (when used) is a solid, mandatory part of every service.</li> \r\n</ol> \r\n<h3>Everything is as serverless as possible</h3> \r\n<p>On can spin-up thousands of functions in a second and doesn\'t have to take care of load-balancing or any other resource management - this all is done by the provider of the serverless service (by the definition). The natural consequence is that if only one part of the system is non-serverless, the whole system becomes failure-prone.</p> \r\n<p>Consider a serverless function connected to a non-serverless database. After spinning-up an amount of function greater than the access limit of the database, next calls of the function will freeze and eventually fail with a timeout.</p> \r\n<p>Isolation of a failure and fall-back strategy for dealing with a non-serverless resource in a severless system is crucial for success.</p> \r\n<h3>The development is finished, once an infrastructure template exists</h3> \r\n<p><em>Infrastructure as Code</em> is an important part of a serverless deployment. The template of the service infrastructure as an input for the deployment process encourages the so-called <em>DevOps</em> culture.</p> \r\n<h3>Client communicates with the services only via its HTTPS API</h3> \r\n<p>The client code should never use any provider-related SDKs. The communication must be agnostic and independent.</p>\r\n\r\n<p>This frees your clients from the vendor lock-in, which is definitely a good idea, because the client code is usually not in the hand of the services developers - freeing clients makes services more independent (for example, a service doesn\'t break any client by changing a cloud provider).</p> \r\n</h3> \r\n<h3>Design is use-case- and business-driven</h3> \r\n<p>Following principles of <em>Domain-Driven Design (DDD)</em> will provide a great insight of how to separate code into services.</p> \r\n<p>If helps to keep the API stable as domain seems to be more relevant for clients (customers) than technical aspects.</p> \r\n<h3>Implementation has no impact on an API</h3> \r\n<p>Connected to the previous one, not only an API must be domain-driven, but any implementation details must not leak into it.</p> \r\n<p>I guess this is not surprising as it counts to the very basic principles of software design in general.</p> \r\n<h3>Business-logic is decoupled from the cloud-provider</h3> \r\n<p>Serverless is a kind of deployment which means it should be implemented in the very outer layer of code calling domain logic as its dependency (never the other way around).</p> \r\n<h3>Resources are referenced via environment variables</h3> \r\n<p>The point III of <a href=\"https://12factor.net/\" target=\"_blank\">The Twelve Factors</a> in practice.</p> \r\n<p>The references should be resolved automatically in service templates in deployment time.</p> \r\n<h3>Each service has unit and integration tests</h3> \r\n<p>Test-driven Development (TDD) is a great method to build a software of high quality.</p> \r\n<p>Don\'t accept any service without tests as finished.</p> \r\n<p>For the whole product there should be a set of end-to-end test for testing whole scenarios from a client point of view on APIs.</p> \r\n<h3>Each service is accessible only via API and events.</h3> \r\n<p>All the service\'s internal resources (e.g. file storage, database etc.) are hidden and from outside denied for access.</p> \r\n<p>It has been already said that implementation must not have any impact on the API, this point says the same from the other side: no other service running in the same environment must access a service\'s internals (even if this is technically possible).</p> \r\n<h3>Multitenancy is a solid, mandatory part of a service.</h3> \r\n<p>When sharing services among customers (the opposite would be to build a &quot;silo&quot; stack for each customer), the multitenancy must be a part of every service - considered from design and the very beginning of development.</p> \r\n<h2>Deployment</h2> \r\n<ol> \r\n<li>Names of the deployment stacks follow the pattern &quot;&lt;service-name&gt;-&lt;stage&gt;&quot;.</li> \r\n<li>Automatic tests are executed in the development-stage DEV, acceptance and manual tests in the test-stage QA.</li> \r\n<li>Developers (stage Dev) and testers (stage QA) have no access to the production (stage Prod).</li> \r\n<li>The deployment of product artifacts is realized and implemented as a build pipeline (Continuous Delivery).</li> \r\n</ol> \r\n<h3>Names of the deployment stacks follow the pattern &quot;&lt;service-name&gt;-&lt;region&gt;-&lt;stage&gt;&quot;</h3> \r\n<p>It\'s important to bring order to the system resources and make the management of them human-friendly. In this case, similar as in code, the naming is very helpful.</p> \r\n<p>Each resource must be deployable in every region within the account, which is enabled by using the region name a a suffix.</p> \r\n<p>The name of the stage (<code>dev</code>, <code>test</code>, <code>prod</code>, etc.) as a suffix allows the developer to deploy multiple stages inside a single account (for test purposes or costs optimizing).</p> \r\n<h3>Developers and testers have no access to the production</h3> \r\n<p>This is possible thru multiple deployment stages strategy where different stages are deployed as a continuous process for different purposes till the last - production - stage.</p> \r\n<h3>Deployment implemented as a build pipeline</h3> \r\n<p>Building, testing, deploying from templates into stages is realized via a single (per product) pipeline.</p> \r\n<h2>Security</h2> \r\n<ol> \r\n<li><em>Principle of least privilege</em> is used for every resource.</li> \r\n<li>Encrypt everything.</li> \r\n</ol> \r\n<h3>Principle of least privilege is used for every resource</h3> \r\n<p>Don\'t give a service rights to do more than it actually should do. This could save you from an unpleasant surprise.</p> \r\n<p>Similar for multi-tenancy systems: the isolation of customer data must be enforced directly by underlying constrains, not only by the logic in code.</p> \r\n<h3>Encrypt everything</h3> \r\n<p>All the communication and all the data must be encrypted.</p> \r\n<p>It\'s a part of the contract where the keys are to be found.</p> \r\n<h2>Conclusion</h2> \r\n<p>Generally, the serverless development is not much different from a standard software development. The biggest difference is in the possibility (duty as well) for a developer to be an active part of the deployment process.</p> \r\n<p>Above I tried to summarize a few basic principles useful to follow in such a development.</p> \r\n<p>But the biggest lesson I have learned: (software) principles should never lead to dogma; they should provide a hint on unclear crossroads.</p> \r\n<p>Have a serverless day!</p>', 'false', 'false', 1, 2),
(45, 'package-by-component-with-clean-modules-in-java', 1546337000, 'Package by Component with Clean Modules in Java', '<p>Software architecting is about tradeoffs. Even when the theory is good the implementation details can break it. In this article I try to find the best from two architectural approaches: <strong>Package by component</strong> and <strong>Clean architecture</strong> (a variety of Ports and adapters).</p>', '<p>Package by component, as proposed by Simon Brown in his <a href=\"https://www.oreilly.com/library/view/clean-architecture-a/9780134494272/\" target=\"_blank\">Missing chapter</a>, is an architectural approach <strong>organizing code based by bundling together everything related to a component</strong>. If done properly (only the component entry point is marked as public) enforces architectural rules, like not bypassing the busines logic in the infrastructure layer - problematic in the Ports and adaptes -, using only the standard Java compiler mechanism.</p> \r\n<h2>Enforced Separation of Concepts</h2> \r\n<p>In my experience it is very important to have such a strict mechanism, because otherwise the temptation to skip the rules is too strong, especially when working under time pressure (don\'t forget - the stress-driven architecture is the <em>big ball of mud</em>).</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component.png\" alt=\"Package by component\" width=\"30%\" /> </p> \r\n<p>Package by component hides code under a package and so makes them inaccessible from another packages (components). There is no way to access the <code>OrderRepository</code> from the <code>OrderController</code> anymore and that\'s great. </p> \r\n<p>The problem here is that there is no mechanism to&nbsp;prevent the access the <code>OrderRepositoryJdbc</code> (infrastructure) from the <code>OrderServiceImpl</code> (domain). The ease of using the implementation directly instead of the interface is still too high. It seems even <strong>in the Package by component approach is the separation of concepts enforced not enough</strong>.</p> \r\n<h2>Modules to Rescue</h2> \r\n<p>Fortunately we have more than only Java packages - we can configure separate modules or projects in a build tool (eg. Maven, Gradle, ...). We can use these artifacts as dependencies and hide so the &quot;outside&quot; from the &quot;inside&quot;.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component_modules.png\" alt=\"Package by component with modules\" width=\"50%\" /> </p> \r\n<p>As you can see in the picture, there are <strong>no changes in the packages</strong> structure, we\'re still following the Package by component architecture, but this time the <strong>domain and infrastructure parts are both separated into modules</strong>. Because the domain artifact has no dependencies to the infrastructure artifact, accessing the <code>OrderRepositoryJdbc</code> from the <code>OderServiceImpl</code> would now cause a compilation error.</p> \r\n<p>We don\'t have to stick with only two modules and can create a finer structure, for example modules like <code>web</code>, <code>database</code>, etc.</p> \r\n<h2>Working Example</h2> \r\n<p>We extend the &quot;MyShop&quot; application shown in the pictures above. We create a simple web application with products to add into a shopping cart and then to order.</p> \r\n<p>As a glue we use the Spring framework (context) and for the web part the Spring Web/MVC all together with the Spring Boot.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component-with-clean-modules.png\" alt=\"Package by component with clean modules\" width=\"70%\" /></p> \r\n<p>Modules creates an <strong>onion structure</strong>:</p> \r\n<ul> \r\n<li>On the top there is the Spring Boot application layer - it represents a <strong>deployment scenario</strong> (alternatively there could be a WAR module for a web-container like WildFly etc.).</li> \r\n<li>Spring configurations, web UI and a database implementation create the second infrastructure layer - <strong>technical details</strong>.</li> \r\n<li>The domain layer sits always on the bottom - it provides the <strong>business API</strong> as well as implementation of the <strong>business logic</strong>.</li> \r\n</ul> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Clean-modules_MyShop.png\" alt=\"Clean modules - MyShop\" width=\"70%\" /></p> \r\n<p>Another benefit of using modules is that we can <strong>get rid of unnecessary dependencies in a declarative way</strong>. For example, if we want to add another implementation of the <code>OrderRepository</code>, like <code>OrderRepositoryInmem</code>, we can do it in two ways: 1. adding a new class into the <code>db</code> module, 2. creating two separate modules <code>db-jdbc</code> and <code>db-inmem</code>. Then we can exclude the dependencies in the <code>spring</code> module, or just declare one of them respectively. With the second approach the <strong>implementation can be easily changed just by adding/removing a dependency</strong>, on the other hand the maintenance of an additional module is needed and in some cases this could lead to an explosion of modules.</p> \r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/myshop\" target=\"_blank\" title=\"MyShop on Github\">GitHub</a>.</p> \r\n<p><i>Note on the implementation:</i> The purpose of the code is to show the Package by component with modules approach, nevertheless it is not a perfect Domain-driven design example. Following the DDD principles seriously, we should see a package <code>cz.net21.ttulka.myshop.cart</code>, classes like <code>ShoppingCart</code>, <code>OrderItem</code>, proper factories for domain objects and, of course, tests!</p>\r\n<p>Happy architecting!</p> \r\n<p> </p>', 'false', 'false', 1, 1),
(46, 'transducers-compose-top-to-bottom', 1548177000, 'Transducers Compose Top-to-Bottom', '<p>Transducers are composable reducers. A transducer takes a reducer and returns another reducer. Transducers compose via simple function composition. But, there is a tiny difference between function and transducer composition: functions compose bottom-to-top while transducers top-to-bottom.</p>', '<p><i>This text presumes you already have at least basic knowledge of <a href=\"https://medium.com/javascript-scene/transducers-efficient-data-processing-pipelines-in-javascript-7985330fe73d\" target=\"_blank\" title=\"Transducers\">transducers</a> and <a href=\"https://medium.com/javascript-scene/curry-and-function-composition-2c208d774983\" target=\"_blank\" title=\"Function composition\">function composition</a>.</i></p>\r\n\r\n<h2>Function Composition</h2>\r\n\r\n<p>First of all we discuss the basics: <em>function composition</em>.</p> \r\n<p>Function composition is the process of chaining the output of a function to the input of another function. In algebra, composition of functions <code>f</code> and <code>g</code> means <code>(f&nbsp;&sdot;&nbsp;g)(x)&nbsp;=&nbsp;f(g(x))</code>.</p> \r\n<p>In JavaScript we can write:</p> \r\n<pre>const inc = (x) =&gt; x + 1;\r\nconst double = (x) =&gt; x * 2; \r\n\r\nconst incAndDouble = (x) =&gt; double(inc(x)); // compound function\r\n\r\nincAndDouble(2); // 6\r\n</pre> \r\n<p>Of course, we can do better! Let\'s create a general function compose:</p> \r\n<pre>const compose = (...fns) =&gt; x =&gt; fns.reduceRight((y, f) =&gt; f(y), x);\r\n\r\nconst incAndDouble2 = compose(double, inc);\r\n\r\nincAndDouble2(2); // 6\r\n</pre> \r\n<p>As you can see, the functions are applied from bottom-to-top as in the definition <code>(f&nbsp;&sdot;&nbsp;g)(x)&nbsp;=&nbsp;f(g(x))</code>. It means, first <code>g(x)</code> is applied and, then, the output is used as the input for <code>f()</code>. If we want a composition applying from top-to-bottom, we can do it: this kind of composition is called a <em>pipe</em>:</p> \r\n<pre>const pipe = (...fns) =&gt; x =&gt; fns.reduce((y, f) =&gt; f(y), x);\r\n\r\nconst incAndDouble3 = pipe(inc, double);\r\n \r\nincAndDouble3(2); // 6\r\n</pre>\r\n \r\n<h2>Transducer Composition</h2> \r\n<p>Probably the most useful is the mapping transducer. Let\'s define it:</p> \r\n<pre>const map = f =&gt; step =&gt; (a, c) =&gt; step(a, f(c));\r\n</pre> \r\n<p>The <code>map</code> takes two curried parameters <code>f</code> and <code>step</code>. <code>f</code> is a mapping function and <code>step</code> is a reducer function to calculate (reduce) the result. We can peep at it with the following code:</p> \r\n<pre>const testReducer = map(double)((a, c)&nbsp;=&gt; console.log(c));\r\n[1,2,3].reduce(testReducer, 0); // prints 2, 4, 6 into the console\r\n</pre> \r\n<p>Now we can use the composite function from above:</p> \r\n<pre>const incAndDouble4 = compose(map(inc), map(double));\r\n\r\nconst concat = (a, c) =&gt; a.concat([c]);\r\n\r\nconst&nbsp;incAndDoubleReducer = incAndDouble4(concat);\r\n\r\n[1,2,3].reduce(incAndDoubleReducer, []); // 4, 6, 8\r\n</pre> \r\n<p>If you paid attention you maybe noticed one thing. Compare these two lines:</p> \r\n<pre>const incAndDouble2 = compose(double, inc);\r\nconst incAndDouble4 = compose(map(inc), map(double));\r\n</pre> \r\n<p>We didn\'t redefine the compose function, the result remains the same, but the order of functions for the composition is the other way around. How is this possible? Let\'s analyse it...</p> \r\n<p>The composite function is pretty straight-forward: it takes a function from bottom-to-top, applies it (first to the init value <code>x</code>) and uses the output as the input for the next function. The difference between <code>double</code> and <code>map(double)</code> is, that <code>map(double)</code> returns a transducer function, not a scalar value as <code>double</code> does. This transducer function takes a parameter <code>step</code>, which is a reducer. It means, <code>f(y)</code> from the compose function is <code>transducer(reducer)</code>; after evaluating it we get a <code>step</code> reducer function, which is then used as a new input <code>y</code> for the next transducer. Well, it means those two lines above define different kinds of function:</p> \r\n<pre>const incAndDouble2 = compose(double, inc); &nbsp;// function (x) =&gt; x\r\nconst incAndDouble4 = compose(map(inc), map(double)); &nbsp;// transducer (reducer) =&gt; reducer\r\n</pre> \r\n<p>Composition uses reducing in the bottom-to-top direction (<code>reduceRight</code>), the result of the composition is the top-most transducer function which contains the next (to-bottom direction) transducer\'s result reducer (in the <code>step</code> parameter). This next reducer is applied after the current reduction (mapping) is applied.</p> \r\n<p>We can write this particular composition function as:</p> \r\n<pre>const compose = (...reducers) =&gt; initReducer =&gt; reducers.reduceRight(\r\n&nbsp; &nbsp; (previousReducer, currectTransducer) =&gt; currectTransducer(previousReducer), initReducer);\r\n</pre> \r\n<p>Each reducer in the composition has a reference to the previous reducer (<code>step</code>). The final reduction starts with the top-most reducer which applies its functionality (mapping) and then executes the referenced reducer (<code>step(...)</code>). This brings the reducing in the opposite direction (top-to-bottom) from the transducers composition (bottom-to-top).</p> \r\n<p>We can break down our example execution as following:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Transducing.png\" alt=\"Transducing\" /></p>\r\n<p>As we can see, the result reducer applies functions in order: <code>inc</code>, then <code>double</code>, then <code>concat</code>. As expected.</p> \r\n<p>Neat.</p> \r\n<p>Happy transducing!</p>  \r\n<p> </p>', 'false', 'false', 1, 1),
(47, 'challenges-for-serverless-blue-green-deployment', 1549306000, 'Challenges for Serverless Blue-Green Deployment', '<p>I already blogged about the <a href=\"/serverless-blue-green-deployment\">serverless blue-green development</a> some time ago. I used it in practice a lot with very promising results. But there are challenges as well.</p>', '<p>First of all, let\'s briefly summarize the idea: We want to separately deploy a serverless service (stack of resources) and <strong>test it in isolation from another resources</strong>. Typically we can achieve this thru multi-account deployment, when several stages are separated via accounts (e.g. <code>dev</code>-<code>test</code>-<code>prod</code> standing for development, testing and production). We deploy changes into the <code>dev</code> stage, which is kind of playground for development, and then into the <code>test</code> stage where the tests are executed. If testing is successful, the service will be deployed into the <code>prod</code> account.</p> \r\n<p>This strategy works well untill we take a look at the bill. Continuous development teaches us to have all the stages as similar as possible, ideally identical. If you\'re using expensive resources, your bill could be three (or more) times bigger.</p> \r\n<p>Serverless blue-green development is a way how to <strong>save some money</strong> by <strong>deploying only those resources really needed</strong> for the test and use already deployed and tested dependencies within a single account.</p> \r\n<p>Deploying of changes results in a new stack of resources (blue), existing parallelly with the previous version (green). When tests succeed, the blue and the green stacks are switched, the blue becomes green and duplicated resources are removed.</p> \r\n<h2>Challenge: Unwanted Interactions</h2> \r\n<p>Consider a situation when a blue stack is triggered by some green resource. For example, a transformer is listening on a topic of upload events. Such &quot;green&quot; events could disturb our testing (out test emits &quot;blue&quot; test events) and devalue the test results. And what\'s more, if the transformation results are saved in a storage, like Amazon S3, the clean-up after the testing could be hard (it is no possible to delete a non-empty bucket).</p> \r\n<pre><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Challenges-for-Serverless-BlueGreen-Deployment.png\" alt=\"Unwanted Interactions\">\r\n</pre> \r\n<h3>Solution: Conditional Flags</h3> \r\n<p>Of course, it is not difficult to find a workaround for each such use-case (like to empty the whole bucket and distinguish &quot;green&quot; events from &quot;blue&quot; ones by an identifier, prefix or special attribute), but all those bring <strong>too much knowledge into tests</strong>. Knowledge we don\'t want to have and deal with.</p> \r\n<p>A solution is to use <strong>conditional flags</strong> in the stack template. For example, with AWS CloudFormation, it can look like this (Yaml):</p> \r\n<pre>Parameters:\r\n  GreenDeployment:\r\n    Type: String\r\n    Description: \"This is a green deployment\"\r\n    AllowedPattern: \"true|false\"\r\n    ConstraintDescription: \"A boolean value\"\r\n    Default: false\r\n\r\nConditions:\r\n  GreenDeploy: !Equals [ !Ref GreenDeployment, true ]\r\n\r\nResources:\r\n  # The subscription to the topic only when \"green\" deployed\r\n  UploadEventsSubscription:\r\n    Type: AWS::SNS::Subscription\r\n    Condition: [GreenDeploy]\r\n    ...\r\n</pre> \r\n<p>For our example, we simply don\'t subscribe to the upload events topic when the stack is not being deployed as green.</p> \r\n<h2>Challenge: Expensive and Slow Resources</h2> \r\n<p>Even when created only for the time of testing, some resources can be really expensive or extremely slow, which can slow testing and the whole deployment process down. A typical example for both of these characteristics is Elasticsearch in AWS. To create such a service is expensive and very slow. So, how to deal with this?</p> \r\n<h3>Solution: Green Resources</h3> \r\n<p>In many cases using green resources (index clusters, databases) does no harm, especially if we clean up afterwards.</p> \r\n<p>We can use a condition for <strong>not to create the expensive resource and inject a green dependency instead</strong> (AWS CloudFormation):</p> \r\n<pre>ResourceRef: !If [ GreenDeploy, !Ref MyExpensiveResource, !Ref GreenResourceRef ]\r\n</pre> \r\n<h3>Solution: Fake Resources</h3> \r\n<p>In testing we use <strong>test-doubles for expensive or slow services</strong> and we can do the same here:</p> \r\n<pre>ExpensiveFn:\r\n  Condition: [GreenDeploy]\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: expensive-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt ExpensiveFnRole.Arn\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; {\r\n          // do something very expensive and slow \r\n          // ...       \r\n          return \'success\'\r\n        }\r\n        \r\nFakeFn:\r\n  Condition: [BlueDeploy]\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: fake-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt FakeFnRole.Arn\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; \'success\'\r\n\r\nMyFn:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: my-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt MyFnRole.Arn\r\n    Environment:\r\n      Variables:\r\n        PROCESSING_FN: !If [ GreenDeploy, !Ref ExpensiveFn, !Ref FakeFn ]\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; {\r\n          // call the processing function\r\n          // ...\r\n        }\r\n</pre> \r\n<p>All the above listed solutions are implemented in the stack preparation phase, the <strong>tests stay completely agnostic</strong> without any additional knowledge.</p> \r\n<p>Happy deploying!</p>', 'false', 'false', 1, 2),
(48, 'how-to-manage-aws-cloudformation-stack-dependencies', 1549475000, 'How to Manage AWS CloudFormation Stack Dependencies', '<p>Automated infrastructure (Infrastructure as Code) is essential to succeed (not only) in the cloud. AWS provides its own service for managing resource stacks: AWS&nbsp;CloudFormation. What are the options to manage dependencies between stacks, how to use them and which pros&nbsp;&amp;&nbsp;cons they have?</p>', '<p>In general, we have three options how to link resources from different stacks:</p> \r\n<p> </p> \r\n<ul> \r\n<li><strong>hard-coded</strong> in template code&nbsp;</li> \r\n<li>via <strong>stack parameters</strong></li> \r\n<li>via <strong>exports/imports</strong></li> \r\n</ul> \r\n<p> </p> \r\n<h2>Hard-Coded References</h2> \r\n<p>This is the most trivial variant as well as the most disadvantageous one. Let\'s say, the stack&nbsp;B needs a resource from the stack&nbsp;A:</p> \r\n<pre>// stackA.yml\r\n\r\nServiceA:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: \"service-A\"\r\n    ...\r\n    \r\n// stackB.yml\r\n\r\nServiceB:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: \"service-B\"\r\n    Environment:\r\n      Variables:\r\n        SERVICE_A: \"service-A\"\r\n    ...</pre> \r\n<p>Well, at least the dependency is set via an environment variable (it could be worse: the reference could be hard-coded direct in the function code), but it\'s still very impractical. The value of the variable must be changed either via a template code change, or manually, which breaks principles of Continuous Delivery. The service B is not informed about a potential change in the stack&nbsp;A, there is no validation that the dependency actually exists and is correct. A system built in this way is obviously brittle and can stop working anytime.</p> \r\n<h2>Stack Parameters&nbsp;</h2> \r\n<p>Setting references via stack parameters is not very different from hard-coded values, but it\'s definitely a&nbsp;small progress, because <strong>we can change parameter values via our continuous delivery process</strong> (pipeline). But there is still no guarantee that the value is correct.</p> \r\n<pre>// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-A\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    \r\n// stackB.yml\r\n\r\nParameters:\r\n  ServiceA:\r\n    Type: String\r\n    Description: \"Reference to the Service A\"\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: !Ref ServiceA\r\n      ...</pre> \r\n<p>Because the stack&nbsp;A publishes the service A in its outputs, we can set the value even in an automation manner. But the problem with inconsistence, in case the resources has changed, remains.&nbsp;</p> \r\n<h2>Exports/Imports</h2> \r\n<p>The most secure way how to deal with stack dependencies in AWS CloudFormation is to use exports/imports. The exported (and somewhere imported) r<strong>esources are protected from changes</strong> and we get a handy overview of our dependencies out of the box.</p> \r\n<pre>// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-A\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    Export:\r\n      Name: \"ServiceA\"\r\n    \r\n// stackB.yml\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: !ImportValue \"ServiceA\"\r\n      ...</pre> \r\n<p>Now, any <strong>change of the exported value will cause an integrity error</strong> and so we can be sure that our dependencies are always correct.&nbsp;</p> \r\n<h2>Parameterized Exports/Imports</h2> \r\n<p>The approach above is fine for small systems with only few stacks. As our system grows there are more and more stacks and we can easily lose the overview which resource belongs to which stack. A good practice here is to use the stack names as &quot;namespaces&quot; to group all the stack resources under the same prefix:</p> \r\n<pre>// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: !Sub \"${AWS::StackName}-service-A-${AWS::Region}\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    Export:\r\n      Name: !Sub \"${AWS::StackName}-ServiceA\"</pre> \r\n<p>The question is, how to pass the name of the exported variable? We can hard-code it, but it will couple the template code with the stack name, which is&nbsp;undesirable, because the code shouldn\'t have any knowledge how stacks are deployed - named.</p> \r\n<p>Another option is to pass variable names as stack parameters, which could work fine, but it means hard and unnecessary effort, because, all in all, the <strong>names are part of the stack&nbsp;API and therefore mustn\'t change</strong> (only the stack name is variable).</p> \r\n<p>The&nbsp;compromise is to <strong>pass only the stack name as a parameter</strong>:&nbsp;</p> \r\n<pre>// stackB.yml\r\n\r\nParameters:\r\n  StackNameA:\r\n    Type: String\r\n    Description: \"Name of the Stack A\"\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: \r\n            Fn::ImportValue: {\"Fn::Sub\": \"${StackNameA}-ServiceA\"}\r\n      ...</pre> \r\n<p>With this approach we have all the benefits of <strong>exports/imports integrity</strong> while&nbsp;<strong>variability and deployment independence</strong> is preserved.</p> \r\n<p>Happy infrastructure coding!</p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(49, 'dealing-with-an-exception-is-one-thing', 1549708000, 'Dealing with an Exception is One Thing', '<p>Nothing new, but I keep seeing this bad practice around again and again... Let\'s explain why this is incorrect: <code>boolean&nbsp;save()</code>.</p>', '<p><em>\"Functions should do one thing. Error handling is one thing.\"</em> -- Robert C. Martin (Clean Code)</p>\r\n<p>This is a very simple principle, but I see it in action only rarely. What\'s actually wrong with that?:</p>\r\n<pre class=\"brush: java\">class TextFile {\r\n    private final Path path;\r\n    \r\n    TextFile(Path path) { this.path = path; }\r\n    \r\n    boolean save(String content) {\r\n        try {\r\n            byte[] bytes = content.getBytes(\"UTF-8\"); \r\n            Files.write(this.path, bytes);\r\n            \r\n            return true;\r\n            \r\n        } catch (IOException e) {\r\n            // log the exception\r\n        }\r\n        return false;\r\n    } \r\n}\r\n</pre>\r\n<p>The thing here is we are trying to inform the client about the fact the save operation was successful or not. But this is not what the method is supposed to do, <strong>the method should save the content, period</strong>.</p>\r\n<p>Exposing a boolean return value the method says to its client that the error is some kind of correct behavior. But the client doesn\'t expect such a behavior, the client simply expect the content to be saved. If not, it is an incorrect behaviour and an exception should occur.</p>\r\n<p>The <code>save</code> method is different from, for example, a <code>boolean&nbsp;readable()</code> method. We expect this method to answer the question if the file is readable or not. And <code>false</code> is a valid answer in this case.</p>\r\n\r\n<p>What\'s the right approach? If you prefer unchecked exceptions (I do), consider the following:</p>\r\n<pre class=\"brush: java\">class TextFile {\r\n    private final Path path;\r\n    \r\n    TextFile(Path path) { this.path = path; }\r\n    \r\n    void save(String content) {\r\n        try {\r\n            byte[] bytes = content.getBytes(\"UTF-8\"); \r\n            Files.write(this.path, bytes);\r\n            \r\n        } catch (IOException e) {\r\n            throw new FileNotSavedException(\"Cannot save a file: \" + path, e);\r\n        }\r\n    } \r\n}\r\n\r\nclass FileNotSavedException extends RuntimeException {\r\n    FileNotSavedException(String message, Throwable cause) {\r\n        super(message, cause);\r\n    }\r\n} \r\n</pre>\r\n<p>Now, the <code>save</code> method is doing exactly what we expect from it: saves a context in a file. When this isn\'t for whatever reason possible, an exception is thrown and it\'s up to <strong>the client to deal with the situation</strong>.</p>\r\n\r\n<p>Have an exceptional day!</p>', 'false', 'false', 1, 1),
(50, 'product-releasing-pipeline-in-aws', 1550770000, 'Product Releasing Pipeline in AWS', '<p>Continuous delivery (CD) brings a lot of ideas essential for a modern software product deployment. In this article we discuss how to follow CD principles by building CD pipelines with an example in AWS.</p>', '<p>Taking CD principles seriously we have to keep in mind following: <strong>CD pipeline is a stream of changes and stages (actions) act like a filter</strong>.</p> \r\n<p>An example of a filter action is testing or a manual approval. When testing fails, the pipeline run is rejected and the delivery ends. Similarly, an approval is a barrier, allowing or permitting the pipeline to go on, rather than a trigger.</p> \r\n<p>Consider a simple CD pipeline for a service delivery:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ServicePipeline.png\" alt=\"Service Pipeline\" /></p> \r\n<p>After a source change triggers a new pipeline instance, code is built and goes into a DEV stage. The stage consists of three actions: the stack is deployed, tested and approved eventually. The approval could be triggered automatically via an external process like some system testing or manually by a developer. <strong>No new changes go into the stage as long as the stage is not either approved or rejected</strong>. This prevents a newer commit from modifying the stage while testing it.</p> \r\n<p>When a commit is approved in the DEV stage, it can go on into the QA stage. In this stage QA tests and validations are proceeded. The approval actions prevents again new changes to be deployed while still testing. If no bugs are found, the stage is approved and the commit goes into the PROD stage (production), where it is deployed.</p> \r\n<h2>Continuous Delivery Example Scenario</h2> \r\n<p>Consider the following scenario of a continuous delivery process:</p> \r\n<ol> \r\n<li>The first commit <code>c1</code> was pushed. It triggers the pipeline and it\'s automatically built and deployed into the DEV stage.</li> \r\n<li>Integration tests run successfuly and the DEV stage was approved. The pipeline moves into another stage - QA - where the commit <code>c1</code> is deployed.</li> \r\n<li>Another commit <code>c2</code> was pushed. The pipeline is triggered, the commit is built and deployed in the DEV stage. The QA stage stays on the <code>c1</code>.</li> \r\n<li>The <code>c2</code> was approved in the DEV stage. QA testers are still busy with the <code>c1</code>, status quo.</li> \r\n<li>A new commit <code>c3</code> was pushed, built and deployed into the DEV stage. QA testers still have no results from the <code>c1</code>.</li> \r\n<li>The QA testers found a bug in the <code>c1</code> and the commit was rejected in the QA stage. The pipeline goes on and the next commit <code>c2</code> is deployed into the QA stage.</li> \r\n<li>A new commit <code>c4</code> was pushed, built and deployed into the DEV stage. QA testers are busy with the <code>c2</code> now.</li> \r\n<li>Meanwhile the <code>c4</code> was approved in the DEV stage and the next commit was deployed.</li> \r\n<li>A bug was found in the <code>c4</code> already in the DEV stage - propably via a system/end-to-end/GUI test - the commit was rejected.</li> \r\n<li>Hardworking QA testers found a bug in the <code>c2</code> as well and the commit was rejected. The pipeline goes on and the next commit <code>c3</code> is deployed into the QA stage.</li> \r\n<li>After some time of testing, QA testers validated and approved the <code>c3</code> and the commit was deployed into the PROD stage.</li> \r\n</ol> \r\n<p>The following table summarize the above scenario:</p> \r\n<table cellspacing=\"0\" cellpadding=\"3\" border=\"1\" width=\"100%\"> \r\n<tbody> \r\n<tr> \r\n<th>Action</th> \r\n<th>Source+Building</th> \r\n<th>StagingDEV</th> \r\n<th>StagingQA</th> \r\n<th>StagingPROD</th> \r\n<th>Change</th> \r\n</tr> \r\n<tr> \r\n<td>commit c1</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c1 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c1 in DEV</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c1 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>commit c2</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c2 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c2 in DEV</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>-</td> \r\n</tr> \r\n<tr> \r\n<td>commit c3</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c3 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>reject c1 in QA</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c2 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>commit c4</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>- </td> \r\n</tr> \r\n<tr> \r\n<td>approve c3 in DEV</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c4 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>reject c4 in DEV</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>- </td> \r\n</tr> \r\n<tr> \r\n<td>reject c2 in QA</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c3 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c3 in QA</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td>c3 in PROD deployed</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<ul> \r\n<li>&#9711; - in progress</li> \r\n<li>&#10003; - succeeded</li> \r\n<li>&#10007; - rejected</li> \r\n</ul> \r\n<h2>Product Releasing Pipeline</h2> \r\n<p><em>Continuous deployment</em> is a practice of deploying changes into the production automatically when developed, tested and validated via a DevOps team. This is not always the best idea, because a <strong>production release is actually a management decision</strong>.</p> \r\n<p>A <em>product</em> is a group of services forming an independent unit, typically delivered under a single name. The <strong>product services should be tested, validated and released at once</strong>.</p> \r\n<p>For such a scenario we would like to have a mechanism for approving all the related services together - a <em>product releasing pipeline</em>.</p> \r\n<p>A product releasing pipeline typically consists of system/end-to-end/GUI testing and several manual approval steps.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ReleasingPipeline.png\" alt=\"Product Releasing Pipeline\" /></p> \r\n<p>When triggered, tests are run for the whole product (sytem) in the DEV environment. If the tests succeed, the changes are ready to go to the next stage - QA. This command action &quot;stage into QA&quot; is implemented as an approval of the previous (DEV) stage. Similarly, when QA testing is successful, changes are ready to be deployed into the production - implemented as an approval of the QA stage in service pipelines.</p> \r\n<h2>Example in AWS</h2> \r\n<p>AWS provides a fully managed CD pipeline solition: <a href=\"https://aws.amazon.com/codepipeline\" target=\"_blank\">AWS CodePipeline</a>.</p> \r\n<p>The releasing pipeline is triggered via a CloudWatch event (for example every nicht at 1:00), end-to-end tests are executed and when succeed a Lambda is invoked. The Lambda puts an approval result programmatically into all the service pipelines.</p> \r\n<p>Service pipeline names are prefixed with a product name - a parameter in the releasing pipeline. Service pipelines must follow this convention. Service pipeline stages must follow naming conventions as well (<code>approve</code> actions in the <code>StagingDEV</code> and <code>StagingQA</code> stages).</p> \r\n<p>Approving the QA stage and releasing into the production is implemented via a manual approval action. Then the Lambda is invoked as well as in the previous step.</p> \r\n<p>When a bug is found, the problem must be worked out individually - a problematic commit should be rejected in a particular service pipeline (the releasing pipeline don\'t reject delivery globally for all the service pipelines).</p> \r\n<p>You can find a sample implementation of a releasing pipeline and two service pipelines in <a href=\"https://github.com/ttulka/aws-samples/tree/master/releasing-pipeline\" target=\"_blank\">my GitHub</a>.</p> \r\n<p>Happy releasing!</p>', 'false', 'false', 1, 2),
(51, 'glass-box-testing-does-not-need-mocking', 1553245000, 'Glass-Box Testing Doesn\'t Need Mocking', '<p>Black-box testing is testing of a component via its API without any knowledge of its implementation details. As the opposite there is the white-box testing. And it about testing implementation, right? Well, no...</p>', '<p>Indeed, while white-box testing we do see into the component, but that doesn\'t mean we access implementation details <em>directly</em>. What\'s more, <em>white</em> is the opposite of black, but white is not transparent after all...</p> \r\n<p>To avoid confusion, further we will strictly use its alternative name <em>glass-box testing</em>. Glass is transparent but impermeable - exactly like a real glass-box test! </p> \r\n<h2>Glass-Box Testing</h2> \r\n<p>What is glass-box testing actually? Even by glass-box testing we still <strong>access the component only via its API</strong>, but we do have some <strong>internal knowledge</strong> about what\'s going on inside (as glass is transparent), which makes use of it for better &quot;informed&quot; tests.</p> \r\n<p>For example to find interesting test cases and input combinations (as we usually can\'t test all possible variants of input exhaustively).</p> \r\n<p><strong>Without this internal knowledge we can\'t do code coverage</strong> - how can we know how many lines of code are test-covered, when we actually don\'t know the code?!</p> \r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/glassbox.png\" alt=\"Glass-box testing\" /> </p> \r\n<p>Consider an example code (in Java):</p> \r\n<pre class=\"brush: java\">interface Saver {\r\n\r\n    void save(UUID id, Object input);\r\n}\r\n</pre> \r\n<p>As the method <code>save</code> doesn\'t return anything (<code>void</code>), it is very difficult to test it only using its API contract (interface) - all we can do is to call the method with a combination of input parameters and check if no error occurs.</p> \r\n<p>Nevertheless, we can assume existence of a <code>load</code> method somewhere else:</p> \r\n<pre class=\"brush: java\">interface Loader {\r\n\r\n    Object load(UUID id);\r\n}\r\n</pre> \r\n<p>And, theoretically, we can use this interface to test if the record can be loaded after being saved.</p> \r\n<p>Such a design obviously sucks, operations for saving and loading objects should stick together in a single coherent contract:</p> \r\n<pre class=\"brush: java\">interface Repository {\r\n\r\n    void save(UUID id, Object input);\r\n    \r\n    Object load(UUID id);\r\n}\r\n</pre> \r\n<p>Now, we can easily test the whole scenario, because it really doesn\'t make sense anyway to test functions that are supposed to work with each other in isolation:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void inMemoryRepositoryTest() {\r\n    Repository repo = new InMemoryRepository();\r\n    UUID id = UUID.randomUUID();\r\n    \r\n    assertNull(repo.load(id));    \r\n    \r\n    repo.save(1, \"test\");\r\n        \r\n    assertEquals(\"test\", repo.load(id));\r\n}\r\n</pre> \r\n<h2>We Don\'t Need No Mocks!</h2> \r\n<p>Consider another example of a delivery service:</p> \r\n<pre class=\"brush: java\">interface DeliveryService {\r\n\r\n    void dispatch(String productId, String customerId);\r\n}\r\n</pre> \r\n<p>One domain rule says that a <em>Promo product</em> cannot be delivered:</p> \r\n<pre class=\"brush: java\">interface Product {\r\n    ...\r\n    boolean deliverable();\r\n}\r\n\r\nclass PromoProduct implements Product {\r\n\r\n    PromoProduct(String name, Double price) { ... }\r\n    ...\r\n    public boolean deliverable() { return false; }\r\n}\r\n</pre> \r\n<p>Our simple delivery service loads a product and a customer from their repositories and saves them into its repository to be proceeded:</p> \r\n<pre class=\"brush: java\">class SimpleDeliveryService implements DeliveryService {\r\n\r\n    private final Repository productRepo, customerRepo, deliveryRepo; \r\n\r\n    SimpleDeliveryService(\r\n            Repository productRepo, Repository customerRepo, Repository deliveryRepo) {\r\n        this.productRepo = productRepo;\r\n        this.customerRepo = customerRepo;\r\n        this.deliveryRepo = deliveryRepo;\r\n    }\r\n\r\n    public void dispatch(Long productId, Long customerId) {\r\n        Product product = this.productRepo.load(productId);\r\n        \r\n        if (!product.deliverable()) {\r\n            throw new UndeliverableProductException(product);\r\n        }\r\n                \r\n        Customer customer = this.customerRepo.load(customerId);\r\n        \r\n        Delivery delivery = new Delivery(product,  customer, LocalDateTime.now());\r\n        this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n    }\r\n}\r\n</pre> \r\n<p>For our domain rule we come up with a unit test:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {\r\n    Repository productRepo = mock(Repository.class);\r\n    UUID productId = UUID.randomUUID();\r\n    productRepo.save(productId, new PromoProduct(\"Test\", 12.3));\r\n    \r\n    Repository customerRepo = mock(Repository.class);\r\n    UUID customerId = UUID.randomUUID();\r\n    customerRepo.save(customerId, new Customer(\"John Smith\", \"Evergreen Terrace 123\"));   \r\n             \r\n    DeliveryService service = new SimpleDeliveryService(\r\n            productRepo, customerRepo, mock(Repository.class));    \r\n    try {\r\n        service.dispatch(productId, customerId);\r\n        \r\n        fail(\"Promo product should not be to deliver.\");\r\n\r\n    } catch (UndeliverableProductException ignore) {\r\n        // we expect this to happen    \r\n    }\r\n}\r\n</pre> \r\n<p>Uff, that was a lot of stuff we had to do to test such a simple rule. We had to mock three repository objects!</p> \r\n<p>Maybe we should re-think our contract once more:</p> \r\n<pre class=\"brush: java\">interface DeliveryService {\r\n\r\n    void dispatch(Product product, Customer customer);\r\n}\r\n\r\nclass SimpleDeliveryService implements DeliveryService {\r\n\r\n    private final Repository deliveryRepo; \r\n\r\n    SimpleDeliveryService(Repository deliveryRepo) {\r\n        this.deliveryRepo = deliveryRepo;\r\n    }\r\n\r\n    public void dispatch(Product product, Customer customer) {        \r\n        if (!product.deliverable()) {\r\n            throw new UndeliverableProductException(product);\r\n        }\r\n        \r\n        Delivery delivery = new Delivery(product, customer, LocalDateTime.now());\r\n        this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n    }\r\n}\r\n</pre> \r\n<p>Now, things got much easier, we saved several lines of code (to be test-covered) and two mocks:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {             \r\n    DeliveryService service = new SimpleDeliveryService(mock(Repository.class));    \r\n    try {\r\n        service.dispatch(\r\n            new PromoProduct(\"Test\", 12.3), \r\n            new Customer(\"John Smith\", \"Evergreen Terrace 123\")\r\n        );\r\n        \r\n        fail(\"Promo product should not be to deliver.\");\r\n\r\n    } catch (UndeliverableProductException ignore) {\r\n        // we expect this to happen    \r\n    }\r\n}\r\n</pre> \r\n<p>This is pretty cool, but can we go even further?</p> \r\n<pre class=\"brush: java\">class Delivery {\r\n\r\n    private final Product product;\r\n    private final Customer customer;\r\n    private final LocalDateTime createdAt;\r\n\r\n    Delivery(Product product, Customer customer) {\r\n        if (!product.deliverable()) {\r\n            throw new UndeliverableProductException(product);\r\n        }\r\n        this.product = product;\r\n        this.customer = customer;\r\n        this.createdAt = LocalDateTime.now();\r\n    }\r\n}\r\n\r\ninterface DeliveryService {\r\n\r\n    void dispatch(Delivery delivery);\r\n}\r\n\r\nclass SimpleDeliveryService implements DeliveryService {\r\n\r\n    private final Repository deliveryRepo; \r\n\r\n    SimpleDeliveryService(Repository deliveryRepo) {\r\n        this.deliveryRepo = deliveryRepo;\r\n    }\r\n\r\n    public void dispatch(Delivery delivery) {\r\n        this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n    }\r\n}\r\n</pre> \r\n<p>Our domain rule is now fully implemented in the <code>Delivery</code> domain object while <code>DeliveryService</code> was reduced just to integration with the external resource (repository).</p> \r\n<p>Consequently, we don\'t need mocks in the test anymore:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {                \r\n    try {\r\n        new Delivery(\r\n            new PromoProduct(\"Test\", 12.3), \r\n            new Customer(\"John Smith\", \"Evergreen Terrace 123\")\r\n        );\r\n                \r\n        fail(\"Promo product should not be to deliver.\");\r\n\r\n    } catch (UndeliverableProductException ignore) {\r\n        // we expect this to happen   \r\n    }\r\n}\r\n</pre> \r\n<p>The <strong>domain rule is now test covered with a simple unit test</strong> without need to use (or mock) the delivery service.</p> \r\n<p><strong>Services must be tested as a part of integration testing</strong> against real resources (e.g. databases) to check if they are correctly integrated, but domain rules stays in the deepest part of the domain model and could be fully covered with simple unit tests without any need of mocking.</p> \r\n<p>Happy testing!</p>', 'false', 'false', 1, 1),
(52, 'double-testing', 1561126000, 'Double Testing', '<p>Write your tests once and run them twice - as both unit and integration tests - sounds like a good deal, let\'s take a look at this practice.</p>', '<p>This article focuses on Java, Spring framework and Maven.</p>\r\n\r\n<p>We have a modular application with two independent String MVC web components. We created a Spring Boot starter for each component to bring them together in a monolithic Spring Boot application.</p>\r\n\r\n<p>Each web component has its own set of tests. We have some <a href=\"http://rest-assured.io/\" target=\"_blank\">REST-Assured</a> tests for REST-based endpoints and Selenium-based tests for testing requests in the end-to-end manner (we check whether expected elements are present on the page).</p>\r\n\r\n<p>To run the tests independently as a part of the component build process we have to simulate the web environment and to mock (or somehow else provide) all needed resources. We can achieve this with <code>@SpringBootTest</code>:</p>\r\n<pre class=\"brush: java\">\r\n@SpringBootTest(\r\n    classes = IndexController.class /* this is what we test */,	\r\n    webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT\r\n)\r\n@EnableAutoConfiguration\r\nclass IndexTest {\r\n\r\n    @Value(\"http://localhost:${local.server.port:8080}\")\r\n    private String url;\r\n\r\n    @Test\r\n    void indexResource_shouldContainWeb1() {\r\n        given()\r\n            .baseUri(url)\r\n            .basePath(\"/\").\r\n        when()\r\n            .get().\r\n        then()\r\n            .statusCode(200)\r\n            .contentType(\"text/plain\")\r\n            .body(containsString(\"Web1\"));\r\n    }\r\n}\r\n</pre>\r\n<p>If you don\'t (or can\'t) include <code>@EnableAutoConfiguration</code> you can create your own annotation with a minimal web configuration from <code>spring-boot-starter-web</code>:\r\n<pre class=\"brush: java\">\r\n    @Target(ElementType.TYPE)\r\n    @Retention(RetentionPolicy.RUNTIME)\r\n    @Documented\r\n    @Configuration\r\n    @Import({\r\n        ServletWebServerFactoryAutoConfiguration.class,\r\n        DispatcherServletAutoConfiguration.class,\r\n        WebMvcAutoConfiguration.class\r\n    })\r\n    @interface MinimalWebConfiguration {\r\n    }\r\n</pre>    \r\n<p>To be sure that a component works in the application as well as running standalone, we can use the same tests, this time executed against the application. Let\'s call them integration tests and run them in a further phase of the application delivery process.</p> \r\n<p>Let\'s create an abstract class containing the whole test code:</p>\r\n<pre class=\"brush: java\">\r\nabstract class IndexTestBase {\r\n    @Test\r\n    void indexResource_shouldContainWeb1() {\r\n        // the test code...\r\n    }\r\n}\r\n</pre>    \r\n<p>Standalone (unit) tests use this base:</p>\r\n<pre class=\"brush: java\">\r\n// Spring Boot settings as above...\r\nclass IndexTest extends IndexTestBase {\r\n	// noting else in this class\r\n}\r\n</pre>\r\n<p>As well as the integration tests:</p>\r\n<pre class=\"brush: java\">\r\n@SpringJUnitConfig\r\nclass IndexIT extends IndexTestBase {\r\n    @Configuration\r\n    static class ITConfig {\r\n        // intentionally empty\r\n    }\r\n}\r\n</pre>\r\n<p>Alternatively to <code>@SpringJUnitConfig</code> you can use <code>@ExtendWith(SpringExtension.class)</code> together with <code>@ContextConfiguration</code> to gain a better overlook.</p>\r\n\r\n<p><code>IndexTest</code> is executed by Maven Surefire Plugin in the <code>test</code> phase, but <code>IndexIT</code> integration test is ignored by Surefire because it doesn\'t match the default naming pattern (<code>*Test</code>).</p>\r\n\r\n<h2>Application Integration</h2>\r\n<p>Web components are integrated into the application by adding their Spring Boot Starters as dependencies:\r\n<pre class=\"brush: xml\">\r\n&lt;dependency&gt;\r\n	&lt;groupId&gt;com.ttulka.samples.doubletesting&lt;/groupId&gt;\r\n	&lt;artifactId&gt;sample-component-web1-spring-boot-starter&lt;/artifactId&gt;\r\n&lt;/dependency&gt;\r\n</pre>\r\n<p>And setting different URL paths:</p>\r\n<pre class=\"brush: java\">\r\n@RestController(\"indexControllerWeb1\")	/* unique name to avoid a bean collision */\r\n@RequestMapping(\"${doubletesting.path.web1:}\")	/* integrated under an unique url path */\r\npublic class IndexController {\r\n	// endpoints definition...\r\n}\r\n</pre>\r\n<pre class=\"brush: yaml\">\r\n# application.yml\r\ndoubletesting.path:\r\n  web1: web1\r\n  web2: web2\r\n</pre>\r\n\r\n<h2>Running Integration Tests</h2>\r\n<p>Integration tests should run in a later phase of the application delivery process than unit tests. The reason is, integration tests are generally slower than unit tests (mainly due to time needed for building the environment) and running them in an earlier phase would delay the feedback, which is highly undesirable.</p>\r\n<p>We create a separate Maven module not included in the build phase at all, skipped with <code>-DskipITs</code> or alternatively by settings of a build profile.</p>\r\n<p>To run the tests from a component, first we have to collect them and include as a dependency. We use Maven JAR Plugin and its goal <code>test-jar</code> for that:\r\n<pre class=\"brush: xml\">\r\n&lt;!-- pom.xml of a component --&gt;\r\n&lt;build&gt;\r\n	&lt;plugins&gt;\r\n		&lt;plugin&gt;\r\n			&lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;\r\n			&lt;executions&gt;\r\n				&lt;execution&gt;\r\n					&lt;id&gt;test-jar&lt;/id&gt;\r\n					&lt;goals&gt;\r\n						&lt;goal&gt;test-jar&lt;/goal&gt;\r\n					&lt;/goals&gt;\r\n				&lt;/execution&gt;\r\n			&lt;/executions&gt;\r\n		&lt;/plugin&gt;\r\n	&lt;/plugins&gt;\r\n&lt;/build&gt;\r\n</pre>\r\n<p>The plugin collects all the tests as a separate artifact, we then include in the integration test module:</p>\r\n<pre class=\"brush: xml\">\r\n&lt;!-- pom.xml of the integration tests module --&gt;\r\n&lt;dependencies&gt;\r\n	&lt;dependency&gt;\r\n		&lt;groupId&gt;com.ttulka.samples.doubletesting&lt;/groupId&gt;\r\n		&lt;artifactId&gt;sample-component-web1&lt;/artifactId&gt;\r\n		&lt;version&gt;0&lt;/version&gt;\r\n		&lt;type&gt;test-jar&lt;/type&gt;\r\n		&lt;scope&gt;test&lt;/scope&gt;\r\n	&lt;/dependency&gt;\r\n	&lt;!-- other dependencies... --&gt;\r\n&lt;/dependencies&gt;	\r\n</pre>\r\n<p>Then, all we need is to copy test classes with Maven Dependency Plugin and prepare the environment. We use Maven Failsafe Plugin and its phases <code>pre-integration-test</code> to start the application and <code>post-integration-test</code> to stop it and clean resources.</p>\r\n<p><code>IndexIT</code> will be executed automatically because it matches Failsafe naming pattern (<code>*IT</code>). Alternatively we can use tagging from JUnit 5 (<code>@Tag(\"integration\")</code>) together with Failsafe configuration (<code>groups</code>) for a finer control of test executions.</p>\r\n<p>We have to include all the test dependencies into the test pom.xml as dependencies with scope <code>test</code> don\'t come transitively.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/double-testing.png\" alt=\"Double Testing\"/></p>\r\n\r\n<p>A working sample project can be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/double-testing\" target=\"_blank\">my Github</a>.</p>\r\n\r\n<p>And that\'s it! One test code base, two different executions - unit and integration.</p>\r\n<p>Keep on testing!</p>', 'false', 'false', 1, 1),
(53, 'no-binaries-in-the-codebase', 1565110000, 'No Binaries in the Codebase', '<p>Binary data shouldn\'t be a part of the codebase. This is pretty well-known practice. But how to proceed when we do need binaries in our codebase, for instance as test data?</p>', '<p>The solution is straight-forward: Pack binary resources into an artifact separated from the codebase in a repository.</p>\r\n<p>A good practice is to use a <strong>special classifier</strong> for all the artifacts of this kind, for instance <code>testdata</code>.</p>\r\n\r\n<p>After uploading the resources artifact into a repository (manually or via an API), we can use it as a dependency in our codebase (Maven):</p>\r\n<pre>\r\n&lt;plugin&gt;\r\n  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n  &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\r\n  &lt;executions&gt;\r\n    &lt;execution&gt;\r\n      &lt;id&gt;unpack-test-resources&lt;/id&gt;\r\n      &lt;phase&gt;process-test-resources&lt;/phase&gt;\r\n      &lt;goals&gt;\r\n        &lt;goal&gt;unpack&lt;/goal&gt;\r\n      &lt;/goals&gt;\r\n      &lt;configuration&gt;\r\n        &lt;outputDirectory&gt;${project.build.directory}/test-classes&lt;/outputDirectory&gt;\r\n        &lt;artifactItems&gt;\r\n          &lt;artifactItem&gt;\r\n            &lt;groupId&gt;com.ttulka.samples.testdata&lt;/groupId&gt;\r\n            &lt;artifactId&gt;sample-test-resources&lt;/artifactId&gt;\r\n            &lt;version&gt;1.0.0&lt;/version&gt;\r\n            &lt;classifier&gt;testdata&lt;/classifier&gt;\r\n            &lt;type&gt;zip&lt;/type&gt;\r\n          &lt;/artifactItem&gt;\r\n        &lt;/artifactItems&gt;\r\n      &lt;/configuration&gt;\r\n    &lt;/execution&gt;\r\n  &lt;/executions&gt;\r\n&lt;/plugin&gt;\r\n</pre>\r\n<p>The Maven plugin will download and unpack the resources into the test classpath before the test are actually executed.</p>\r\n<p>Binaries are then available in the codebase (Java):</p>\r\n<pre>\r\nthis.getClass().getResource(\"/resource1.dat\");\r\n</pre>\r\n\r\n<p>Another good practice is to create a <strong>domain-based resources structure</strong> inside the artifacts. Just put the resources into a sub-folder:</p>\r\n<pre>\r\ndomainA.zip\r\n┕ domainA/\r\n  ┕ resource1.dat\r\n  ┕ resource2.dat\r\n</pre>\r\n<p>This organization enables composition of resources in the <a href=\"/double-testing\">integration testing</a>.</p>\r\n\r\n<p>The example code could be found in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/no-binaries-in-codebase/textfile\">my GitHub</a>.</p>\r\n\r\n<p>Happy resourcing!</p>', 'false', 'false', 1, 2),
(54, 'do-not-share-data-among-threads', 1567445000, 'Don\'t Share Data among Threads', '<p>Distribution of a task among several threads means horizontal scaling - the more computing resources (processors) the less time to work the task out. Sharing data among threads brings the need to <strong>synchronize which kills the scaling capability</strong> of the computing. How to proceed when shared data are needed?</p>', '<p>Consider a very simple ETL system where the Extractor produces a finite sequence of numbers, the Transformer converts numbers into strings, and the Loader finally saves the string into a database. Because the database access is expensive, the Loader works in batches: first, collect data up to a limit batch size, then write the whole batch into the database. To use resources efficiently the Loader runs in multiple threads.</p> \r\n<p><img alt=\"Simple multi-thread ETL\" src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ETL-with-shared-resources.png\" /></p> \r\n<p>And here comes trouble: shared data is introduced (batch collection) and methods must be synchronized. The result is almost the same or event worse than using a single thread without synchronization.</p> \r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, sync</th> \r\n<th>2 threads, sync</th> \r\n<th>4 threads, sync</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>50.768 ms</td> \r\n<td>51.083 ms</td> \r\n<td>51.674 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p> </p>\r\n<p>Some optimization to minimize the size of the critical section are practicable, but not always. How to avoid synchronization as much as possible?</p> \r\n<h2>Thread Own Data</h2> \r\n<p>One approach is to let each thread to own its data. Such data doesn\'t have to be synchronized. The drawback is a design-shift of the Loader from a simple implementation to a threading-aware one:</p> \r\n<pre class=\"brush: java\">class BatchLoaderThreadOwnData {\r\n\r\n    private final int batchSize;\r\n    private final JdbcTemplate jdbcTemplate;\r\n\r\n    private boolean finished = false;\r\n\r\n    // the object holds a map of data for each thread\r\n    // the only one concurrent access, therefore synchronized\r\n    private final Map&lt;Long, ThreadOwnData&gt; threadOwnDataMap = new ConcurrentHashMap&lt;&gt;();\r\n\r\n    public void load(String result) {\r\n        threadOwnData().resultsBatch.add(result);\r\n\r\n        if (finished || threadOwnData().counter++ &gt;= batchSize) {\r\n            batchLoad(threadOwnData());\r\n        }\r\n    }\r\n\r\n    public void finish() {\r\n        finished = true;\r\n        threadOwnDataMap.values().forEach(this::batchLoad);\r\n    }\r\n    \r\n    private ThreadOwnData threadOwnData() {\r\n        return threadOwnDataMap.computeIfAbsent(\r\n                  Thread.currentThread().getId(), \r\n                  id -&gt; new ThreadOwnData(batchSize));\r\n    }\r\n    \r\n    // ...\r\n}\r\n</pre> \r\n<p>The results are actually much better:</p> \r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, own data</th> \r\n<th>2 threads, own data</th> \r\n<th>4 threads, own data</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>52.129 ms</td> \r\n<td>29.668 ms</td> \r\n<td>20.050 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p> </p>\r\n<h2>Instance per Thread</h2> \r\n<p>We can go a step further and let a tread own the whole object. This allows us to reuse the first thread-unsafe version just as with a single thread. The drawback is a complicated execution logic and the need to create new instances, which is not always possible:</p> \r\n<pre class=\"brush: java\">// the only synchronization here \r\nMap&lt;Long, BatchLoaderUnsafe&gt; batchLoaders = new ConcurrentHashMap&lt;&gt;();\r\n// ...\r\n// run in an async executor:\r\nBatchLoaderUnsafe loader = batchLoaders.computeIfAbsent(\r\n    Thread.currentThread().getId(),\r\n    id -&gt; new BatchLoaderUnsafe(BATCH_SIZE, jdbcTemplate));\r\n// call it on an unsynchronized thread-owned loader\r\nloader.load(i); \r\n</pre> \r\n<p>The results are, as expected, comparable to the previous solution:</p> \r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, thread own</th> \r\n<th>2 threads, thread own</th> \r\n<th>4 threads, thread own</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>51.919 ms</td> \r\n<td>30.051 ms</td> \r\n<td>19.814 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p> </p>\r\n<h2>Conclusion</h2> \r\n<p>In the modern world of microservices and distributed computing is scalability one of the most important attributes. Techniques like immutability are not always applicable, but synchronization must still be reduced to minimum. When not done so, the performance could be even worse when running in a single thread as threading overheads must be paid.</p> \r\n<p>When it is not possible to create a new worker instance for a thread, the worker could be designed to create a new instance of data per thread. When every thread owns its data, there is no need for further synchronization.</p> \r\n<p>The whole code could be found in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/shared-resources-among-threads\">my GitHub</a>.</p> \r\n<p>Happy threading!</p>', 'false', 'false', 1, 1),
(55, 'domain-driven-serverless-design', 1568462000, 'Domain-Driven Serverless Design', '<p>One reason I really like the serverless architecture approach is being pretty selfish: one has to care only about what matters - the code. Well, I know code is not everything, but as a developer I\'m just having more fun coding than scripting infrastructure in YAML or similar. For people like me is the serverless model a dream come true. But how to do serverless without turning the dream into a nightmare?</p>', '<p>\r\nIt\'s <a href=\"https://martinfowler.com/bliki/MonolithFirst.html\" target=\"_blank\">well known</a> that the microservices-first approach leads often to a failure. The point here is to know the domain well before splitting the system up into autonomous services. Once split up refactoring across boundaries becomes difficult (or even impossible) due to the lack of collective code ownership. Werner Vogels\' famous statement says &quot;<a href=\"https://www.allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\" target=\"_blank\">APIs are forever</a>&quot;, once published the interface cannot be changed. Without knowing the domain well one usually ends up with a <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\" target=\"_blank\">CRUD</a>-like <a href=\"https://www.michaelnygard.com/blog/2017/12/the-entity-service-antipattern/\" target=\"_blank\">entity services</a>, which wakes him up&nbsp;every night&nbsp;in a lather of sweat.</p> \r\n<p>Let\'s illustrate this with an example.&nbsp;A CRUD-like entity service looks like this:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-1.png\" alt=\"Entity Service\" /> </p> \r\n<p>We have here one (micro)service built around the Car entity and five functions (some people call them <em>nanoservices</em>) implementing its CRUD operations. The potential database or a storage is an internal part of the service and it\'s not accessible or visible to the outer world.</p> \r\n<p>Consider a car rental company with a web page displaying a list of cars available to rent. With only entity services the page controller must retrieve a list of all cars from the Cart service, then a list of all rentals from the Rental service and finally match cars not included in any rentals:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-2.png\" alt=\"Multiple requests\" /> </p> \r\n<p>Even in this simple scenario there are several problems:</p> \r\n<p> </p> \r\n<ul> \r\n<li>Knowledge of Car and Rental entities on the client side leads to tight-coupling of the services.</li> \r\n<li>Availability of the feature relies on all involved services.</li> \r\n<li>Multiple synchronous requests result to a lot of overhead and increase costs rapidly.</li> \r\n<li>Transferring more data really needed means throughput waste.</li> \r\n<li>Complex communication makes the system difficult to reason about.</li> \r\n</ul> \r\n\r\n<p>One can easily image a more complex scenario where a function calls a function which call a function... This ends up not only in mess but in a very expensive mess, as synchronous calls in functions are charged for both the blocked caller and the blocking callee.</p> \r\n<p>Synchronization always means coupling. Serverless systems are great for an asynchronous communication, which is however not always possible. Fortunately, there are several options how to tame this beast. Using tools like AWS Step Functions or Azure Logic Apps can optimize the composition of function calls, but it\'s still not applicable everywhere. The solution is to design the services in a way they don\'t need to make any synchronous calls whatsoever - make them <a href=\"https://en.wikipedia.org/wiki/Domain-driven_design\" target=\"_blank\">domain-driven</a>!</p> \r\n<p>How would the scenario be implemented in the domain-driven style? Well, what is the feature here? The controller method already told us: <em>cars to rent</em>.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-3.png\" alt=\"Cars to rent\" /> </p> \r\n<p>Now, the controller makes only one single request to get a list of cars available to rent, exactly what asked for. Further there will be function like <code>rent-a-car</code>, <code>return-a-car</code> or <code>extend-a-rental</code>. All of those are autonomous, which means, they have all they need to work the feature out. Again, the service contains its own data, gathered for example by an event listening function like <code>a-new-car-stored</code>&nbsp;and similar.</p> \r\n<p>Important to notice is the names of new functions - all are domain-driven. Technical concepts like <em>create</em> or <em>delete</em> disappeared from the model completely and that\'s the whole point.</p> \r\n<p>To summarize it:</p> \r\n<ol> \r\n<li>Know your domain well,</li> \r\n<li>build services around the domain,</li> \r\n<li>sleep well in the night.&nbsp;</li> \r\n</ol> \r\n<p>Before we reach the first point, we should forget not only about serverless but microservices as well. First, a <a href=\"https://speakerdeck.com/axelfontaine/majestic-modular-monoliths\" target=\"_blank\">monolith</a> is the right way to go. Growing up enough to know the domain boundaries well, we can start with big services. Splitting them up into serverless functions is the last step.</p> \r\n', 'false', 'false', 1, 1),
(64, 'how-i-do-tdd', 1573988050, 'How I do TDD', '<p>I really like Test-Driven Development (TDD) and apply it almost always. The problem with TDD is that it focuses too much on <i>working software</i>.</p>', '<p align=\"center\"><i>\"Make it work, make it right, make it fast.\" ~&nbsp;Kent Beck</i></p>\r\n\r\n<p>Don\'t get me wrong, code must work, but that just shouldn\'t be the number one priority.</p>\r\n\r\n<p align=\"center\"><blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\">It is more important for code to be changeable than that it work. Code that does not work, but that is easy to change, can be made to work with minimum effort. Code that works but that is hard to change will soon not work and be hard to get working again.</p>&mdash; Uncle Bob Martin (@unclebobmartin) <a href=\"https://twitter.com/unclebobmartin/status/1192392951294500864?ref_src=twsrc%5Etfw\">November 7, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\r\n\r\n<p>In TDD, design and refactoring towards a better architecture come first when the feature is already implemented and tests are green. Refactoring then often needs to rewrite and delete a lot of code, activity which not everyone likes to do, especially when the code has just been written.</p>\r\n\r\n<p>Taking TDD dogmatically so often leads to a perfectly working code with a poor design.</p>\r\n\r\n<p>I find more eligible to apply TDD first in the implementation phase. The process of development so looks like follows:</p>\r\n\r\n<ol>\r\n	<li>Understand the problem.</li>\r\n	<li>Design the API clearly without any implementation concerns.</li>\r\n	<li>Implement requirements one by one applying TDD.</li>\r\n</ol>\r\n\r\n<p>This approach ensures a well designed architecture with all benefits of TDD.</p>\r\n\r\n<p>It\'s still necessary to keep <i>\"make it right\"</i> after <i>\"make it work\"</i> as it means optimization and polishing the code like removing duplicates, restructuring methods, renaming variables etc. The API should remain untouched and the tests ensure all is still working.</p>\r\n\r\n<p>To paraphrase Kent:</p>\r\n\r\n<p align=\"center\"><i>Understand it, design it, make it work, make it right.</i></p>\r\n\r\n<p>Happy TDDing!</p>\r\n', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(65, 'keep-test-code-inside-the-test', 1574553600, 'Keep Test Code inside the Test', '<p>Noone wants to write one thing twice. Reducing duplicates makes code shorter and clearer. How much this applies for test code?</p>', '<p>A few days ago I was pair-programming with another developer in the <a href=\"/how-i-do-tdd\">Test-Driven Development</a> manner. TDD and pair-programming is the best fit. I usually apply TDD after having understood the problem and designed the API, in the implementation phase. This approach helps me reduce the need for later refactoring and avoid testing of trivial code.</p>\r\n\r\n<p>Having the requirement reflected in the API, we wrote a test:</p>\r\n\r\n<pre class=\"brush: java\">\r\npublic interface Account {\r\n\r\n	boolean canLogin(String password);\r\n}\r\n\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n	Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n\r\n	assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>Then we implemented the feature and wrote the next test:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_cannot_login_with_an_invalid_password() {\r\n	Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n\r\n	assertThat(account.canLogin(\"xxx\")).isFalse();\r\n}\r\n</pre>\r\n\r\n<p>After implementing this we continued:</p>\r\n\r\n<pre class=\"brush: java\">\r\npublic interface Account {\r\n\r\n	boolean canLogin(String password);\r\n	\r\n	void changePassword(String newPassword);\r\n}\r\n\r\n@Test\r\nvoid password_is_changed() {\r\n	Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n	\r\n	account.changePassword(\"updated\");\r\n\r\n	assertThat(account.canLogin(\"updated\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>At that moment my partner proposed to move the first line of the tests into a separate method, like this:</p>\r\n\r\n<pre class=\"brush: java\">\r\nprivate Account account;\r\n    \r\n@BeforeEach\r\nvoid setupAccount() {\r\n	this.account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n}\r\n</pre>\r\n\r\n<p>It does follow the Don\'t Repeat Yourself principle and makes the test methods shorter, right? Look:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n	assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>Sounds well first, however, there are several issues.</p>\r\n\r\n<h2>Don\'t Share Code among Tests</h2>\r\n\r\n<p>Sharing code among different requirements is a bad idea, because business is volatile and the <a href=\"http://verraes.net/2014/08/dry-is-about-knowledge/\">rules might change independently</a>. Doing so comes with risks and drawbacks:</p>\r\n\r\n<h3>1. It\'s not obvious how the test is set up</h3>\r\n\r\n<p>Looking at the test only, its not clear what is the arrangement of the test, how is the unit initialized and set up. To understand this, we have to scroll up to the setup method.</p>\r\n\r\n<h3>2. Tests are coupled</h3> \r\n\r\n<p>It is impossible to have different setups for different tests, although this is often desired. Consider the following test:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_with_no_email_cannot_login() {\r\n	Account account = new UserAccount(\"test\", null, \"pwd1\");\r\n\r\n	assertThat(account.canLogin(\"pwd1\")).isFalse();\r\n}\r\n</pre>\r\n\r\n<p>Such a dilemma leads often to a complex setup method trying to do more than one thing, which makes the test suite difficult to understand and maintain.</p>\r\n\r\n<h3>3. Tests are not well isolated</h3>\r\n\r\n<p>Tests must run in isolation from each other. Tests must never be meant to run in a particular order. Consider the following tests:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n	assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n\r\n@Test\r\nvoid password_is_changed() {	\r\n	account.changePassword(\"updated\");\r\n	\r\n	assertThat(account.canLogin(\"updated\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>When the second test runs before the first one, it fails.</p>\r\n\r\n<p>Even not frequent in practice, it should always be possible to run the tests in parallel. Bearing this in mind helps you to write well isolated tests.</p>\r\n\r\n<h3>4. Unnecessary work</h3>\r\n\r\n<p>Consider another test in the test suite:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid empty_password_raises_an_error() {\r\n	assertThrows(InvalidPasswordException.class, () -&gt;\r\n			new UserAccount(\"test\", \"test@example.com\", \"\"));\r\n}\r\n</pre>\r\n\r\n<p>Even not needed at all, the setup method is executed and resources created, which is just a waste.</p>\r\n\r\n<h2>Don\'t Share Dependencies amoung Tests</h2>\r\n\r\n<p>The same is true for dependencies. Instead of doing this (with Spring):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Autowired\r\nprivate AccountRegistry registry;\r\n\r\n@Value(\"${test.admin.username}\")\r\nprivate String adminUsername;\r\n    \r\n@Test\r\nvoid registered_user_account_is_in_the_registry() {\r\n	Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\", registry);	\r\n	account.register();\r\n	\r\n	assertThat(registry.byUsername(\"test\").isPresent()).isTrue();\r\n}\r\n\r\n@Test\r\nvoid admin_account_has_admin_rights() {\r\n	Account account = new UserAccount(adminUsername, \"test@example.com\", \"pwd1\");\r\n	\r\n	assertThat(account.hasAdminRights()).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>...it\'s way better to this (with Spring and JUnit 5):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid registered_user_account_is_in_the_registry(@Autowired AccountRegistry registry) {\r\n	Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\", registry);	\r\n	account.register();\r\n	\r\n	assertThat(registry.byUsername(\"test\").isPresent()).isTrue();\r\n}\r\n\r\n@Test\r\nvoid admin_account_has_admin_rights(@Value(\"${test.admin.username}\") String username) {\r\n	Account account = new UserAccount(username, \"test@example.com\", \"pwd1\");\r\n	\r\n	assertThat(account.hasAdminRights()).isTrue();\r\n}\r\n</pre>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Write your tests so that everything the test needs is included inside the test. Letting the test code go out of its boundaries makes the test dependent on its context, which brings unnecessary complexity into the test suite.</p>\r\n\r\n<p>Benefits of reducing code duplicity don\'t really count here, because having hard-to-maintain tests pays a much higher price than a few extra lines of code.</p>\r\n\r\n<p>Happy testing!</p>', 'false', 'false', 1, 1),
(66, 'treat-data-as-data', 1575206000, 'Treat Data as Data', '<p>Object-oriented approach is a mighty concept making software more maintainable, which means cheaper and easier to understand. Problems come at boundaries, where objects have to be passed on into a different layer or another system. There, the objects become just data and should be treated as that.</p>', '<p>This problem is not new, was already noticed by <a href=\"https://blog.ploeh.dk/2011/05/31/AttheBoundaries,ApplicationsareNotObject-Oriented/\">Mark Seemann</a> and many others. Michael Nygard writes in <i>Release It!</i>:</p>\r\n<p align=\"center\" class=\"quote\"><em>\"What appears as a class in one layer should be mere data to every other layer.\"</em></p>\r\n\r\n<p>Objects in the domain layer are <strong>\"living\" active entities defined by their behavior</strong>. But what happens when an object needs to be persisted, or for example displayed on the screen? What is an object for a database? How is an object represented in a message or in a response of a REST call? It\'s mere data.</p>\r\n\r\n<p>As an example, consider a REST controller returning an object as JSON. With Spring framework we typically do it as follows:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@GetMapping\r\npublic Account login(String username, String password) {\r\n    return accounts.login(username, password);\r\n}\r\n</pre>\r\n\r\nBecause <code>Account</code> provides a <code>getUsername()</code> method, the response looks like:</p>\r\n\r\n<pre class=\"brush: json\">\r\n{\r\n    \"username\": \"test\" \r\n}\r\n</pre>\r\n\r\n<p>The problem with this approach is that the domain layer (<code>Account</code>) is highly coupled to the application layer (controller): a change in the domain object can lead to a broken contract. Another problem is leaking of implementation details: adding a new getter to the domain object will change the response as follows:</p>\r\n\r\n<pre class=\"brush: json\">\r\n{\r\n    \"username\": \"test\",\r\n    \"password\": \"pwd1\" \r\n}\r\n</pre>\r\n\r\n<p>Another obvious problem is, that Spring requires getters to convert the domain object to its serialized representation (e.g. JSON). This breaks encapsulation of the object and leads to a shift from the OOP understanding of objects as a unit of behavior to treating objects as poor data structures with a bunch of attached procedures.</p>\r\n\r\n<h2>Data is Data</h2>\r\n<p>The solution is to understand this gap and to <strong>treat objects as objects and data as data</strong>. A traditional way of doing this is known as Data Transfer Object (DTO):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@GetMapping\r\npublic LoginDTO login(String username, String password) {\r\n    Account account = accounts.login(username, password);\r\n    return new LoginDTO(account.getUsername());\r\n}\r\n\r\nclass LoginDTO {\r\n\r\n    private final String username;\r\n    \r\n    public LoginDTO(String username) {\r\n        this.username = username;\r\n    }\r\n    \r\n    public String getUsername() {\r\n        return this.username;\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>Creating a DTO for all use-cases is a lot of hard work. Is it really worth? What are the actual benefits of DTOs? I can think of two:</p>\r\n<ol>\r\n    <li>Static typing</li>\r\n    <li>Explicit structure</li>\r\n</ol>\r\n\r\n<h3>Static Typing</h3>\r\n<p>Static typing is a big benefit of strongly typed languages that brings a good level of confidence as a lot of bugs are discovered already during compilation. The problem here is, that no input/output is actually strongly typed. Consider the following endpoint:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(@RequestBody ToRegisterDTO toRegister) {\r\n    accounts.register(\r\n        toRegister.getUsername(), \r\n        toRegister.getPassword(), \r\n        toRegister.getEmail());\r\n}\r\n\r\nstatic class ToRegisterDTO {\r\n\r\n    private final String username;\r\n    private final String password;\r\n    private final String email;\r\n    \r\n    // constructor and getters...\r\n}\r\n</pre>\r\n\r\n<p>The method expects the input in the following format:</p>\r\n<pre class=\"brush: json\">\r\n{\r\n    \"username\": ...,\r\n    \"password\": ...,\r\n    \"email\": ...\r\n}\r\n</pre>\r\n\r\n<p>But there is nothing to prevent the client to send anything different:</p>\r\n<pre class=\"brush: json\">\r\n{\r\n    \"UserName\": ...,\r\n    \"pass\": ...,\r\n    \"e-mail\": ...\r\n}\r\n</pre>\r\n<p>The typical solution is to validate the input, but <strong>no typing will help us here</strong>. So why should we bother?</p>\r\n\r\n<h3>Explicit Structure</h3>\r\n<p>Explicit is always good, but DTOs create a lot boiler-plate code and are not much different to pure data structures. Compare the following variants:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(@RequestBody ToRegisterDTO toRegister) {\r\n    accounts.register(\r\n        toRegister.getUsername(), \r\n        toRegister.getPassword(), \r\n        toRegister.getEmail());\r\n}\r\n\r\n// vs.\r\n\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(@RequestBody Map&lt;String, String&gt; toRegister) {\r\n    accounts.register(\r\n        toRegister.get(\"username\"), \r\n        toRegister.get(\"password\"), \r\n        toRegister.get(\"email\"));\r\n}\r\n</pre>\r\n\r\n<p>In my opinion there is no big difference, in the second variant a lot of code for DTOs disappeared (the less to maintain the better) and the code tells the reader much more explicitly that data, no objects, are to be found here.</p>\r\n<p>A Data Transfer Objects are, despite the name, no object at all. There are nothing more than strongly typed data structures. As the strong typing doesn\'t bring a great value, explicit data structures like Map and List could be used instead.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>Whether using DTOs or standard data structures, <strong>dealing with data should be explicit and obvious from the code</strong>. Leaking domain beyond layer boundaries increase coupling and can undesirably effect the API. This risk should be avoided by treating data as data clearly and explicitly. Objects are units of behavior which can\'t cross layers. This fact should be reflected in the code.</p>\r\n\r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/objects-at-boundaries\" target=\"_blank\">GitHub</a>.\r\n\r\n<p>Happy coding!</p>', 'false', 'false', 1, 1),
(67, 'how-to-test-abstract-classes', 1575653000, 'How to Test Abstract Classes', '<p>Abstract classes typically offer one or more concrete methods. These must be tested as well. There are several ways how to do it, but which one to choose?</p>', '<p>Consider an abstract class with a concrete method (Java):</p>\r\n<pre class=\"brush: java\">\r\nabstract class Person {\r\n\r\n    protected final String firstName;\r\n    protected final String lastName;\r\n\r\n    public Person(String firstName, String lastName) {\r\n        this.firstName = firstName;\r\n        this.lastName = lastName;\r\n    }\r\n\r\n    public String fullName() {\r\n        return String.format(\"%s %s\", firstName, lastName);\r\n    }\r\n    \r\n    public abstract String greeting();\r\n}\r\n</pre>\r\n\r\n<p>What are the possibilities to test such a class?</p>\r\n\r\n<h2>Anonymous Classes</h2>\r\n\r\n<p>We can create an ad-hoc instance of the abstract class and stub the abstract method which are not under test:</p>\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid full_name_is_provided() {\r\n    Person person = new Person(\"John\", \"Smith\") {\r\n        @Override\r\n        public String greeting() {\r\n            return null;\r\n        }\r\n    };\r\n    assertEquals(\"John Smith\", person.fullName());\r\n}\r\n</pre>\r\n\r\n<p>But such a code is pretty cumbersome and hard to read. Just image a class with multiple abstract methods, the test code becomes longer and longer and breaks one of the main property of a good test - readability.</p>\r\n<p>Another problem is the <strong>coupling between the test and implementation</strong>. Even that the method is marked as <code>final</code>, the code should focus only on the contract and not on such implementation details as details can change in the future and a good test should be resistant to refactoring.</p>\r\n\r\n<p>Creating an anonymous class doesn\'t seem to be the best option. Can we do better?</p>\r\n\r\n<h2>Mocking</h2>\r\n\r\n<p>This is probably the first advice you get if you search on the Internet. With mocking everything is easy (Mockito):</p>\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid full_name_is_provided() {\r\n    Person person = mock(Person.class, withSettings()\r\n            .useConstructor(\"John\", \"Smith\")\r\n            .defaultAnswer(CALLS_REAL_METHODS));\r\n\r\n    assertEquals(\"John Smith\", person.fullName());\r\n}\r\n</pre>\r\n\r\n<p>Mocking an abstract class is practically just like creating an anonymous class but using convenient tools. It has the same drawbacks and, again, it\'s probably not the best option we have.</p>\r\n\r\n\r\n<h2>Concrete Class</h2>\r\n\r\n<p>An abstract class makes actually no sense without being extended with a concrete class. <strong>A concrete class is where the requirements must be met.</strong></p>\r\n<pre class=\"brush: java\">\r\nclass Sailor extends Person {\r\n\r\n    public Sailor(String firstName, String lastName) {\r\n        super(firstName, lastName);\r\n    }\r\n\r\n    @Override\r\n    public String greeting() {\r\n        return \"Ahoy!\";\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>This is the right place for testing:</p>\r\n<pre class=\"brush: java\">\r\nclass SailorTest {\r\n\r\n    @Test\r\n    void full_name_is_provided() {\r\n        Sailor sailor = new Sailor(\"James\", \"Cook\");\r\n\r\n        assertEquals(\"James Cook\", sailor.fullName());\r\n    }\r\n\r\n    @Test\r\n    void greeting_is_provided() {\r\n        Sailor sailor = new Sailor(\"James\", \"Cook\");\r\n\r\n        assertEquals(\"Ahoy!\", sailor.greeting());\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>Of course, there will be probably some duplicates in the test code when we have more concrete classes extending the abstract class. But the price is still smaller that losing the value of the test, when the acceptance depends on the inherited (potentially unknown) implementation. All in all, the inherited implementation can and often does (even it should be avoided as much as possible) change in the concrete classes and must be tested anyway.</p>\r\n\r\n<h2>True Object-Oriented</h2>\r\n\r\n<p>We can face a situation, for instance when working on a util library, where the provided default implementation is much more complicated that in our simple <code>fullName()</code> method. Such a case tell us that we are probably doing too much in the class. The solution is to introduce a new class and extract the functionality into it:</p>\r\n<pre class=\"brush: java\">\r\nclass Name {\r\n\r\n    private final String first;\r\n    private final String last;\r\n\r\n    public Name(String first, String last) {\r\n        this.first = first;\r\n        this.last = last;\r\n    }\r\n    \r\n    public String full() {\r\n        return String.format(\"%s %s\", firstName, lastName);\r\n    }\r\n}\r\n\r\nabstract class Person {\r\n\r\n    protected final Name name;\r\n\r\n    public Person(String firstName, String lastName) {\r\n        this.name = new Name(firstName, lastName);\r\n    }\r\n\r\n    public final String fullName() {\r\n        return name.full();\r\n    }\r\n    \r\n    public abstract String greeting();\r\n}\r\n</pre>\r\n\r\n<p>We moved the functionality into a specialized concrete class, easy to test. Now we can ensure the default implementation of the abstract class works as expected.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Implementation inheritance is in general not the best practice as it tightly couples children classes with the parent class. Composition should always be preferred over inheritance. However, there are cases where inheritance makes sense. In such cases testing must be very careful.</p>\r\n\r\n<p><strong>Business is always concrete.</strong> Relying blindly on testing of an abstract class could break concrete requirements. <strong>Test always a concrete class</strong> as the concrete class must satisfy the business requirements.</p>\r\n\r\n<p>Happy testing!</p>', 'false', 'false', 1, 1),
(68, 'solid-principles-in-java-by-example', 1575814500, 'SOLID Principles in Java by Example', '<p>There are a lot of articles about the SOLID principles. But usually a different example for a particular principle is to be found. Instead, would it be nice to demonstrate all of them on a single code snippet?</p>', '<p>We aren’t going to drive deep into the theory, as a lot was already written. We are interested mainly in code!</p>\r\n\r\n<p>Consider a simple payroll component, an example very favored by Uncle Bob himself:</p>\r\n\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n\r\n    private final static Map&lt;String, Employee&gt; registry = new HashMap&lt;&gt;();\r\n\r\n    protected final String personalId;\r\n    protected final String firstName;\r\n    protected final String lastName;\r\n\r\n    /** constructor */\r\n\r\n    public String fullName() {\r\n        return String.format(\"%s %s\", firstName, lastName);\r\n    }\r\n\r\n    public void register() {\r\n        registry.put(personalId, this);\r\n    }\r\n\r\n    public boolean isRegistered() {\r\n        return registry.containsKey(personalId);\r\n    }\r\n}\r\n\r\nclass Paycheck {\r\n\r\n    private final Employee employee;\r\n\r\n    /** constructor */\r\n\r\n    public double amount() {\r\n        if (employee instanceof Manager) {\r\n            return 2000.0;\r\n        }\r\n        if (employee instanceof Developer) {\r\n            return 1000.0;\r\n        }\r\n        return 0.0;\r\n    }\r\n}\r\n</pre>\r\n\r\n\r\n<h2>Single Responsibility Principle (SRP)</h2>\r\n<p>The SRP is about cohesion, it says that a component (function, method, class, module) should have only one reason to change.</p>\r\n\r\n<p>Our code breaks the SRP as any change in the persistence mechanism would require a change of the <code>Employee</code> code. For instance, a timestamp of the registration is persisted as well, etc.</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n    /** ... */\r\n    \r\n    private final static Map&lt;String, Employee&gt; registry = new HashMap&lt;&gt;();\r\n    \r\n    public void register() {\r\n        registry.put(personalId, this);\r\n    }\r\n    \r\n    public boolean isRegistered() {\r\n        return registry.containsKey(personalId);\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>To fix that we introduce a new specialized class <code>EmployeeRegistry</code> and put the persistence functionality into it.</p>\r\n<pre class=\"brush: java\">\r\nclass EmployeeRegistry {\r\n\r\n    private final static Map&lt;String, Employee&gt; map = new HashMap&lt;&gt;();\r\n\r\n    public void register(Employee employee) {\r\n        map.put(employee.personalId, employee);\r\n    }\r\n\r\n    public boolean isRegistered(Employee employee) {\r\n        return map.containsKey(employee.personalId);\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>Now, we just delegate the request:</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n    /** ... */\r\n    \r\n    private final static EmployeeRegistry registry = new EmployeeRegistry();\r\n    \r\n    public void register() {\r\n        registry.register(this);\r\n    }\r\n    \r\n    public boolean isRegistered() {\r\n        return registry.isRegistered(this);\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>We can do even better with the Dependency Inversion Principle, stay tuned.</p>\r\n\r\n<h2>Open-Closed Principle (OCP)</h2>\r\n<p>The OCP states that software components should be open for extension, but closed for modification. It means that we should extend the system functionality by adding new components rather than modifying the existing ones.</p>\r\n\r\n<p>With a new <code>Employee</code> subtype, calculating of the amout in the <code>Paycheck</code> must be modified:</p>\r\n<pre class=\"brush: java\">\r\npublic double amount() {\r\n    if (employee instanceof Manager) {\r\n        return 2000.0;\r\n    }\r\n    if (employee instanceof Developer) {\r\n        return 1000.0;\r\n    }\r\n    return 0.0;\r\n}\r\n</pre>\r\n\r\n<p>Better would be to create a method inside the <code>Employee</code> to return the salary for the paycheck:</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n    /** ... */\r\n    \r\n    public abstract double salary();\r\n}\r\n\r\nclass Manager extends Employee {\r\n    /** ... */\r\n    \r\n    @Override\r\n    public double salary() {\r\n        return 2000.0;\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>Now, the <code>Paycheck</code> could be simplified and will work with any additional <code>Employee</code> subclass.</p>\r\n<pre class=\"brush: java\">\r\nclass Paycheck {\r\n    /** ... */\r\n    \r\n    public double amount() {\r\n        return employee.salary();\r\n    }\r\n}\r\n</pre>\r\n<p>By the way, using <code>instanceof</code> violates the Liskov Substitution Principle, too. Avoid <code>instanceof</code> at any price!</p>\r\n\r\n<h2>Liskov Substitution Principle (LSP)</h2>\r\n<p>The LSP says that an object of type T should be replaceable with its subtypes S without affecting the correctness of the program P.</p>\r\n\r\n<p>Although the most of possible violations of LSP are in strongly typed languages caught by the compiler (return types, proper parameter subtypes, etc.), there are still invariants and contracts to take care of.</p>\r\n\r\n<p>From the definition, a volunteer has to salary; it actually makes no sense to talk about any salary:</p>\r\n<pre class=\"brush: java\">\r\nclass Volunteer extends Employee {\r\n    /** ... */\r\n    \r\n    @Override\r\n    public double salary() {\r\n        throw new RuntimeException(\"No salary for volunteers!\");\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>But what happends to the <code>Paycheck</code> (P) when we try to calculate the amount for a volunteer (S)?\r\n<pre class=\"brush: java\">\r\ndouble amount = new Paycheck(\r\n    new Volunteer(\"001\", \"John\", \"Smith\")\r\n).amount();\r\n</pre>\r\n\r\n<p>Unsurprisingly, an exception occurs. The exception breaks the contract we provided via the <code>Employee</code> to the <code>Paycheck</code> as not declared in the method signature. We have to fix it:</p>\r\n<pre class=\"brush: java\">\r\nclass Volunteer extends Employee {\r\n    /** ... */\r\n    \r\n    @Override\r\n    public double salary() {\r\n        return 0.0;\r\n    }\r\n}\r\n</pre>\r\n\r\n<h2>Interface Segregation Principle (ISP)</h2>\r\n<p>The ISP says that no client should be forced to depend on methods it does not use.</p>\r\n\r\n<p>A volunteer is actually not a payed employee and implementing it like that violates a business invariant. We can fix it with the ISP:</p>\r\n<pre class=\"brush: java\">\r\ninterface PayedEmployee {\r\n\r\n    double salary();\r\n}\r\n\r\nclass Manager extends Employee implements PayedEmployee {\r\n    /** ... */\r\n\r\n    @Override\r\n    public double salary() {\r\n        return 2000.0;\r\n    }\r\n}\r\n</pre>\r\n  \r\n\r\n<p>After the <code>PayedEmployee</code> was introduced, the method <code>salary()</code> has disappeared from the <code>Volunteer</code> and the <code>Employee</code> itself.</p>\r\n<p>The <code>Paycheck</code> depends only on what it really needs now:</p>\r\n<pre class=\"brush: java\">\r\nclass Paycheck {\r\n\r\n    private final PayedEmployee employee;\r\n\r\n    /** ... */\r\n}\r\n</pre>\r\n\r\n<h2>Dependency Inversion Principle (DIP)</h2>\r\n<p>The DIP states that high level modules should not depend on low level modules; both should depend on abstractions and abstractions should not depend on details.</p>\r\n\r\n<p>We have introduced the <code>EmployeeRegistry</code> to separate the persistence mechanism from the other code, but the class is still concrete and it\'s a hard dependency of the <code>Employee</code>.</p>\r\n<p>What happends if a different implementation is needed? For instance, consider using a database instead of a <code>Map</code>.</p>\r\n<p>Following the DIP we introduce an abstraction and let <code>Employee</code> depend on it:</p>\r\n<pre class=\"brush: java\">\r\ninterface EmployeeRegistry {\r\n\r\n    void register(Employee employee);\r\n    \r\n    boolean isRegistered(Employee employee);\r\n}\r\n\r\nabstract class Employee {\r\n\r\n    private final EmployeeRegistry registry;\r\n    \r\n    /** ... */\r\n}\r\n</pre>\r\n\r\n<p>Notice that abstractions invert dependencies.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>(Software) principles should never lead to dogma; they should provide a hint on unclear crossroads.</p>\r\n<p>SOLID principles can help recognize a problem in your code, but applying them blindly will likely do more harm than good.</p>\r\n\r\n<p>The example source code with SOLID commits (<a href=\"https://github.com/ttulka/blog-code-samples/tree/f0a5961d8e99772f39047f4a8322d12344acd2a7/solid-by-example\" target=\"_blank\">original</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/be83c69a5b9a75e6a99efdd969e87967c0e26b65/solid-by-example\" target=\"_blank\">SRP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/65d9799e20f2c695ca534eac9e6fd90f5fa823f6/solid-by-example\" target=\"_blank\">OCP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/4f7f59cff17f006b07a0a0c68703b7b918ec0941/solid-by-example\" target=\"_blank\">LSP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/66db7e330dde660b566e500628b2a069794c4934/solid-by-example\" target=\"_blank\">ISP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/a2397ff2ab7b8902456737eb93eb36cc13159136/solid-by-example\" target=\"_blank\">DIP</a>) is on my <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/solid-by-example\" target=\"_blank\">Github</a>.</p>\r\n\r\n<p>Happy coding!</p>', 'false', 'false', 1, 1),
(69, 'object-oriented-design-vs-persistence', 1579450000, 'Object-Oriented Design vs. Persistence', '<p>From time to time I attend discussions about OOP. Every time someone come up with the argument of dealing with persistence. The typical question can be reduced to <em>should an object persist itself or rather be persisted?</em> I believe the question is fundamentally wrong.</p>', '<h2>Traditional Approach</h2>\r\n\r\n<p>Traditionally (and very wrongly) an object is seen as data with a bunch of procedures dealing upon it. A typical school-book code looks like follows:</p>\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n\r\n  private String personalId;\r\n  private String firstName;\r\n  private String lastName;\r\n\r\n  String getPersonalId() { return personalId; }\r\n  String getFirstName() { return firstName; }\r\n  String getLastName() { return lastName; }\r\n}\r\n</pre>\r\n\r\n<p>How to persist the Person object? One approach is to create a repository:</p>\r\n<pre class=\"brush: java\">\r\nclass PersonRepository {\r\n\r\n  void save(Person person) {\r\n    // persist the object\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Second approach is to let the object persist itself:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n  // ...\r\n\r\n  void save() {\r\n    // persist the object\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Which one is better? None, or more precisely, depends.</p>\r\n\r\n<h2>Domain-Driven Design</h2>\r\n\r\n<p><a href=\"https://en.wikipedia.org/wiki/Domain-driven_design\" target=\"_blank\">Domain-Driven Design (DDD)</a> calls for designing objects driven by their business (domain) meaning rather than by technical aspects. According DDD, <strong>persistence is just an implementation detail</strong>. How this helps us with our dilemma? Actually a lot: the question, where to put the persistence, is driven by the domain as well as all other concerns.</p>\r\n\r\n<p>Just listen to the business, what does it say? Is it talking about \r\n <em>a register</em> for people registration?</p>\r\n<pre class=\"brush: java\">\r\nclass PersonRegister {\r\n\r\n  void register(Person person) {\r\n    // ...\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Or rather about person <em>reporting</em>, <em>checking in</em>, <em>applying</em> or <em>enrolling</em>?</p>\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n  // ...\r\n\r\n  void enroll() {\r\n    // ...\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Don’t think like a technician: \"this object must be persisted\", understand the domain and model around it. <strong>The domain must be found again in the model</strong>.</p>\r\n\r\n<p>Because the theory can never know your domain, it’s impossible to create a framework helping to cut the code vertically. All the tool can only help with the horizontal cutting and technical regards, which encourages the implementers to concentrate too much on techniques (like persistence) and forget about the domain. The biggest problem with <a href=\"https://www.oreilly.com/library/view/domain-driven-design-tackling/0321125215/\" target=\"_blank\">the blue book</a> I have is that it focuses a lot on tactical design and building blocks like Entities, Services and Repositories. At the end of the day that is the only thing some readers get from the text ignoring the real value: the importance of <strong>communication with domain experts and understanding the business</strong> being built.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Pure technical concerns like persistence don’t belong to object-oriented design. These are mere implementation details hidden behind APIs. APIs tell the story of the domain. <strong>Model the API around the domain, don’t let implementation leak to the API</strong>.</p>\r\n\r\n<p>Happy designing!</p>', 'false', 'false', 1, 1),
(70, 'multitenancy-and-the-cloud', 1584280000, 'Multitenancy and the Cloud', '<p>Multitenancy was and still is a very popular and successful architectural pattern of the last decades. But, that is likely to change with the advent of cloud computing.</p>', '<p>I remember well the time when hardware was expensive and uneasy to get. Waiting for weeks or even months to receive a new server I necessarily needed yesterday. Bothersome paperwork, awkward negotiations, hard-to-get approvals, and high costs. It was true pain to be avoided if possible. Multitenancy as an architectural pattern comes exactly from these circumstances. </p>\r\n<p>The idea is pretty simple: deploy and operate a single system supporting multiple customers. The resources are shared, adding a new customer is quick and easy. But, of course, as everything in software architecture, multitenancy has several trade-offs. The biggest one is a lot of additional complexity. <strong>Tenant isolation must be ensured on every system level</strong> from configurations to data. This is not a simple task. The code must be aware of it on all layers, which makes even simple things pretty complicated with negative impact on performance. With increasing numbers of tenants <strong>scalability can easily become an issue</strong> as well.</p>\r\n<p>Cloud computing as the enabler of DevOps, infrastructure automatization and containerization changed the traditional hardware provisioning. Nowadays, it is possible to spin up a new server, create a new database, or set up a new service bus within a second or even less. No more request tickets to an IT department! <strong>Hardware provisioning in the cloud became quick and easy, providing a solution for the problem multitenancy originally addressed</strong>.</p>\r\n<h2>Multi- vs Single-Tenancy</h2>\r\n<p>Compare a single multi-tenant system with multiple single-tenant systems:</p>\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/multi-vs-single-tenant-systems.png\" alt=\"Multi- vs Single-Tenancy\" /></p>\r\n<p>In his book <a href=\"https://leanpub.com/cloudstrategy\" target=\"_blank\">Cloud Strategy</a>, Gregor Hohpe compares these two approaches to <strong>an apartment building vs. single-family homes</strong>. Such a mental model helps us understand the differences in the approaches.</p>\r\n<h3>Development</h3>\r\n<p>It is definitely way simple to project a single-family house rather than a big apartment building. This is the main drawback of multi-tenant systems as multitenancy aspects leak deep into the structure and brings great complexity which slows development down and makes it more expensive and exhausting.</p>\r\n<h3>Building and Deployment</h3>\r\n<p>Having a construction plan and building material should mean no big difference in the building process. But, big apartment buildings tend to be unique and built ad hoc and unsystematic with project changes and adjustments during the construction work, while small family houses are uniform making them a perfect match for automatization of the building process. <strong>Automatization of everything</strong> is an important mindset-shift highly encouraged by the cloud services.</p>\r\n<h3>Maintenance</h3>\r\n<p>If you ever lived in an apartment house, you know how difficult and costly an accident can be. Old big buildings tend to resist change while small houses are easy to be rebuilt from the ground. Finding a bug, implementing a new feature, data migration, a design or functionality change, those activities are very hard in complex systems.</p>\r\n<h3>Efficiency and Scalability</h3>\r\n<p>While single-family houses can be built quickly and easily on demand, planning an apartment building must take into account the desired amount of tenants in advance: when the house is only half occupied it’s very inefficient to be run, the heating, elevator, lighting must still be taken care of and so on. Another trouble comes when the capacity is exceeded. <strong>Multi-tenant systems are based on vertical scaling</strong> with its well-known limitations. Even adopting horizontal scaling could be problematic with multi-tenant systems: Consider an extreme case where a feature is implemented as a serverless function (Function-as-a-Service). Because everything in a multi-tenant system is shared, <strong>constraints and limits are shared</strong> as well. A serverless function allows for example one thousand concurrent executions per second. With one hundred tenants accommodated in the system it makes theoretically only ten executions per second for a tenant, which could be just too little.</p>\r\n<h3>Operations and Management</h3>\r\n<p>It is probably easier to manage one building consisting one thousand apartments than to deal with one thousand small houses. To accommodate a tenant into an existing apartment will be always easier than to build a brand-new house for him. Depends on discipline and maturity of the automatization process. As the <strong>multiple single-tenant approach moves the multi-tenant complexity from development to operations</strong>, demands on the system administrators can grow. The key here is a good automatization of literally everything and adopting the DevOps mindset: divide the work among dev and ops people, bring them together and share the responsibility for the product within the team.</p>\r\n<h2>Conclusion</h2>\r\n<p>Cloud computing doesn’t mean the definite end of the multitenancy architectural pattern, but it does offer an alternative solution with a promise of reducing effort in development by shifting the complexity into operations. This is not thinkable without mature automatization and clean team responsibility.</p>\r\n<p>But, there are valid use-cases for multitenancy as well as there are valid use-cases for computing outside the cloud.</p>\r\n<p>Happy accommodating!</p>', 'false', 'false', 1, 2),
(71, 'events-vs-commands-in-ddd', 1585123000, 'Events vs. Commands in DDD', '<p>There are situations where events and commands seem to be a good solution for a problem. Where to use events and where are commands the best fit?</p>', '<h2>Differences between Events and Commands</h2>\r\n<p>Events represent a past, something that already happened and can’t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected. An event has typically multiple consumers, but a command is addressed to only one.</p>\r\n<p>Martin Fowler emphasizes in his <a href=\"https://youtu.be/STKCRSUsyP0\" target=\"_blank\">talk</a> the <strong>semantic difference between events and commands</strong> as a hint to understand the overall behavior by using the most natural terms for a particular business. In his point of view the question “events or commands” is a <em>naming</em> problem.</p>\r\n<p>While this is true, it’s not the only one difference. I believe the key is responsibility and level of abstraction.</p>\r\n\r\n<h2>Domain Meaning</h2>\r\n<p>Consider a problem with two possible solutions, first using an event, second using a command:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-1.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>With events Billing service subscribes to Sales service as a consumer, with commands Sales service <em>actively uses</em> Billing service. Events enable an independent choreography of services while commands create an orchestration where one or a few services control all others. This contradicts the <a href=\"http://udidahan.com/2010/11/15/the-known-unknowns-of-soa/\" target=\"_blank\">definition of a service</a> as a technical authority for a specific business capability. Sales and Billing are two separate business capabilities, which means Sales service should not include any payment considerations. On the other hand, it is fully Billing’s responsibility to deal with a newly placed order.</p>\r\n\r\n<h2>Commands as Infrastructure Messages</h2>\r\n<p>Commands are imperatives to a concrete action, typically the result of a user act. There’s usually no room for the called service to make any business decisions or reasoning about the action. Consider a common usage of a command:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-2.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>MailServer makes no business decisions, it just sends an e-mail. On the other hand, sending an e-mail, SMS, or displaying a message on the screen <em>is</em> a business decision made by Sales service. MailServer is probably a separate component maintained by a different team or an external vendor. We can apply the same for the payment:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-3.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>As already mentioned, we don’t want to make business decisions about payments in Sales service. We can stick with an event and wrap the payments into a separate service:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-4.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>Now, we have two separate business services (Sales, Billing) and one technical service (PayPal), which is <strong>encapsulated</strong> in Billing service from the domain point of view. We can see PayPal service as part of Billing implementation and CollectPayment as an <strong>infrastructure message</strong> living outside of the domain.</p>\r\n<p>We can reason about the MailServer similarly:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-5.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>Domain service Customer Notification reacts on OrderPlaced event and sends SendEmail command to MailServer (an infrastructure service) to carry out its business decision of sending an e-mail to the customer.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>Events and commands are not just two sides of the same coin. <strong>(Domain) events are part of the business domain and its ubiquitous language, while commands are a pure technical concern</strong>.</p>\r\n<ul>\r\n  <li>Ask business to define the service boundaries.</li>\r\n  <li>Use events for communication among domain services.</li>\r\n  <li>Use commands only for communication with technical services.</li>\r\n</ul>\r\n\r\n<p>Happy events!</p>', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(72, 'domain-collections', 1586325000, 'Domain Collections', '<p>Collection, List and Set are terms very familiar to developers but hardly used by business experts. Therefore, they should not be part of the domain (API).</p>', '<h2>Standard Collections Don’t Speak Language of the Domain</h2>\r\n\r\n<p><strong>Domain (API) must speak the domain language</strong>. Listen to the business experts. They’re probably talking just about <em>products</em> rather than <em>a collection</em> or <em>a list</em> of products. Even when a “list” is used, its meaning differs from the List class which appears as standard in programming languages like Java (<code>java.util.List</code>) or .NET (<code>System.Collections.Generic.List</code>).</p>\r\n\r\n<p>Consider a typical incorrect API (in Java):</p>\r\n<pre class=\"brush: java\">\r\ninterface FindProducts {\r\n\r\n    List&lt;Product&gt; cheaperThan(Money money);\r\n}\r\n</pre>\r\n<p>What is the corresponding requirement?</p>\r\n<ul><li><em>As a user I want to find products cheaper than X amount of money.</em></li></ul>\r\n\r\n<p>Well, the requirement talks about <em>products</em>, not <em>a list of products</em>. It means we have a mismatch between the domain and the code that models it. The flaw is not huge, but there are other problems connected.</p>\r\n\r\n<h2>Standard Collections Have Meaningless Operations</h2>\r\n\r\n<p>Let’s inspect the interface once again. What can a client do with it:</p>\r\n<pre class=\"brush: java\">\r\nfor (Product product : findProducts.cheaperThan(fiveDollars)) \r\n    System.out.println(product);\r\n\r\nfindProducts.cheaperThan(fiveDollars)\r\n    .stream()\r\n    .mapToDouble(Product::price)\r\n    .sum();\r\n</pre>\r\n<p>This is okay.</p>\r\n\r\n<pre class=\"brush: java\">\r\nfindProducts.cheaperThan(fiveDollars).size();\r\n\r\nfindProducts.cheaperThan(fiveDollars).get(1).price();\r\n</pre>\r\n<p>Does it make sense? Yeah, it could, depends on the use-case.</p>\r\n\r\n<p>What about these?:</p>\r\n<pre class=\"brush: java\">\r\nfindProducts.cheaperThan(fiveDollars).remove(1);\r\n\r\nfindProducts.cheaperThan(fiveDollars).clear();\r\n</pre>\r\n<p>Nah, those are very likely nonsense. The point is, even when those operations make no sense in the context of the use-case, they are still offered by the use-case API and nothing prevents the client from trying them out.</p>\r\n\r\n<p>Of course, the internal collections are probably not mutable and an exception will be thrown in runtime when the client does so, but this is just too late. Better would be not to provide such methods at all. It would make the client code safe right in compilation time.</p>\r\n\r\n<p>Not only are many methods meaningless, but even when they make sense, for example when we do want to provide the remove operation upon products, calling the method <code>List.remove()</code> will not bring the expected result - a product is maybe removed from the list, but it will still be found in the system.</p>\r\n\r\n<h2>Domain Collections to Rescue</h2>\r\n<p>We can fix this by introducing a new domain object Products:</p>\r\n<pre class=\"brush: java\">\r\ninterface FindProducts {\r\n\r\n    Products cheaperThan(Money money);\r\n}\r\n</pre>\r\n\r\n<p>What methods are in the Products signature? Everything which makes sense in the context of the domain. For example, we can sort the products:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n    \r\n    Products sorted(SortBy by);\r\n}\r\n</pre>\r\n\r\n<p>Sure, in the end we probably have to provide a way to receive the Product entities from the collection. We have several options:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products extends Iterable&lt;Product&gt; {\r\n}\r\n</pre>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n    List&lt;Product&gt; asList();\r\n}\r\n</pre>\r\n\r\n<p>The meaningless methods like <code>clear()</code> and <code>remove()</code> are still included on the standard List interface, but now the client knows he works with a list of products and not with the products themselves. The operations are called on the list and not on the original Domain Collection. It means, even when the client erases the list, the found products remain the same.</p>\r\n\r\n<p>We can go a bit further and use <code>java.util.stream.Stream&lt;T&gt;</code> which is immutable and more natural to what the result actually represents:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n    Stream&lt;Product&gt; asStream();\r\n}\r\n</pre>\r\n\r\n<p>Similarly, in reactive systems we can use reactive types:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n    Flux&lt;Product&gt; asPublisher();\r\n}\r\n</pre>\r\n\r\n<h3>Better Performance</h3>\r\n<p>Domain Collections help us not only to improve the API, but can have several technical benefits, too.</p> \r\n\r\n<p>For example, lazy loading can be applied as the data is not required until the collection “collapses”. This can save a lot of throughput in case the collection is loaded from a database or some external resource. Such optimization would be not possible with Standard Collections. Consider a usage:</p>\r\n<pre class=\"brush: java\">\r\nfindProducts\r\n    .cheaperThan(fiveDollars)       // not queried yet\r\n    .sorted(Products.SortBy.PRICE)  // not queried yet\r\n    .asStream()                     // function “collapse” -> data queried \r\n</pre>\r\n\r\n<p>A sample JDBC implementation follows:</p>\r\n<pre class=\"brush: java\">\r\nclass ProductsCheaperThan implements Products {\r\n\r\n    private final Money cheaperThan;\r\n    private final SortBy sortBy;\r\n\r\n    private final JdbcTemplate jdbcTemplate;\r\n\r\n    // constructor ...\r\n\r\n    @Override\r\n    public Products sorted(SortBy by) {\r\n        return new ProductsCheaperThan(cheaperThan, by, jdbcTemplate);\r\n    }\r\n\r\n    @Override\r\n    public Stream&lt;Product&gt; asStream() {\r\n        return jdbcTemplate.queryForList(String.format(\r\n                \"SELECT code, title, price FROM products \" +\r\n                \"WHERE price &lt; ? ORDER BY %s\", sortBy), cheaperThan.amount())\r\n                .stream()\r\n                .map(this::toProduct);\r\n    }\r\n\r\n    // private methods ...\r\n}\r\n</pre>\r\n<p>The full source code can be found in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/domain-collections\" target=\"_blank\">my Github</a>.</p>\r\n\r\n<h2>Summary</h2>\r\n<p><strong>Domain Collections</strong> encapsulate domain entities in the domain API and provide domain-meaningful operations upon them.</p>\r\n\r\n<p>Using Domain Collections over Standard Collections brings several benefits:</p>\r\n<ul>\r\n  <li>Model speaks domain language,</li>\r\n  <li>safety of operations can be ensured via static typing,</li>\r\n  <li>potential of performance optimization.</li>\r\n</ul>\r\n<p>Happy collecting!</p>\r\n', 'false', 'false', 1, 1),
(73, 'what-is-a-repository', 1586850000, 'What is a Repository', 'Which purpose has a Repository? To which layer does it belong to? And how to implement it correctly?', '<p>A <em>Repository</em> is a term from Domain-Driven Design (DDD), but I am actually not happy about that. Things like <em>Entity</em>, <em>Aggregate</em> and <em>Repository</em> are pure technical concepts and should never appear in the domain, which speaks language of a particular business only. Although DDD puts a Repository to the domain model (saying all Repository methods must have domain meanings), I find that too confusing to follow and very often misunderstood.</p>\r\n\r\n<p>In fact, <strong>persistence is just an implementation detail</strong> of business behavior (e.g. find a customer, register a new customer). There must be no persistence-related methods like <code>save()</code> or <code>load()</code> in the domain API.</p>\r\n\r\n<p>Therefore, I prefer to <strong>refactor Repository functionalities into concrete use-cases</strong>. This approach ends up with much simpler cohesive objects.</p>\r\n\r\n<h2>Do We Need Repositories at All?</h2>\r\n\r\n<p>Well, yes and no. We usually need a way to load and persist domain objects, but we don’t have to have a construct called “Repository” in our codebase.</p>\r\n\r\n<p>Consider the following use-case (in Java):</p>\r\n<pre class=\"brush: java\">\r\ninterface FindCustomer {\r\n\r\n    Customer byEmail(Email email);\r\n}\r\n</pre>\r\n\r\n<p>We can simply implement it with JDBC:</p>\r\n<pre class=\"brush: java\">\r\nclass FindCustomerJdbc implements FindCustomer {\r\n\r\n    private JdbcTemplate jdbcTemplate;\r\n\r\n    @Override\r\n    public Customer byEmail(Email email) {\r\n        return jdbcTemplate.queryForList(\r\n                \"SELECT firstName, lastName, email FROM customers \" +\r\n                \"WHERE email = ?\", email.value())\r\n                .stream()\r\n                .findAny()\r\n                .map(this::toCustomer)\r\n                .orElseGet(this::customerNotFound);\r\n    }\r\n    \r\n    // private methods...\r\n}\r\n</pre>\r\n\r\n<p>Because loading the entity from the database with an SQL query <em>is the actual implementation</em> of the use-case (as the class name says), there is no need to put any other layer in between. In this case <code>FindCustomerJdbc</code> <em>is</em> a Repository for the particular use-case.</p>\r\n\r\n<p>Yet typically we put another layer in between using an object called Repository:</p>\r\n<pre class=\"brush: java\">\r\nclass FindCustomerJdbc implements FindCustomer {\r\n\r\n    private CustomerRepository customerRepository;\r\n\r\n    @Override\r\n    public Customer byEmail(Email email) {\r\n        return customerRepository.findByEmail(email.value())\r\n                .map(this::toCustomer)\r\n                .orElseGet(this::customerNotFound);\r\n    }\r\n    \r\n    // private methods...\r\n}\r\n</pre>\r\n\r\n<p>We just moved the persistence-related code into a single place. What are the benefits here? One can say that 1) all persistence concerns can be changed together and 2) the code is more reusable.</p>\r\n\r\n<p>The first point could be valid when the storage type is volatile. But, how often happens that an application switches from one storage type to another?</p>\r\n\r\n<p>Changing the storage type (e.g. from XML files to a database) or even just the database type (e.g. relational to non-relational) needs usually a nontrivial shift in mindset and an additional persistence abstraction layer itself can hardly fulfil this goal. Such an attempt would likely lead to a too-general clumsy and inefficient solution, expensive to maintain (remember YAGNI), especially because it happens very rarely - I never faced such a situation personally!</p>\r\n\r\n<p>JDBC or JPA are themselves already solid abstractions making it possible to switch easily between database vendors.</p>\r\n\r\n<p>The latter point can be valid if we in fact face a lot of code duplications. For example, mapping from a persistent entry to the domain object, finding entities by ID etc.. In such cases, the abstraction will emerge clearly by itself. We can abstract the common functionality out to a separate object etc.. Otherwise, the Repository becomes quickly a bunch of persistence-related methods across different use-cases. That means the code is cut by technical instead of domain aspects. Independent use-cases become tightly coupled and difficult to maintain. <strong>Only good abstractions are reusable, but use-cases tend to be pretty concrete</strong>.</p>\r\n\r\n<p>Back to the code above. As we can see, the actual use-case implementation becomes a mere proxy to the Repository. One can think of skipping the proxy code altogether and implement the use-case just by the repository:</p>\r\n<pre class=\"brush: java\">\r\nclass CustomerRepositoryJdbc implements FindCustomer {\r\n\r\n    private JdbcTemplate jdbcTemplate;\r\n\r\n    @Override\r\n    public Customer byEmail(Email email) {\r\n        return jdbcTemplate.queryForList(\r\n                \"SELECT firstName, lastName, email FROM customers \" +\r\n                \"WHERE email = ?\", email.value())\r\n                .stream()\r\n                .map(this::toCustomer)\r\n                .findAny()\r\n                .orElseGet(this::customerNotFound);\r\n    }\r\n\r\n    // private methods...\r\n}\r\n</pre>\r\n\r\n<p>The problem is obvious: the object grows with every use-case added. Soon it ends up as a huge object responsible for everything involving persistence of the Customer. Such code is not clean, clumsy and hard to maintain, concrete methods tend to be heavily reused. Rather, the <strong>code should be structured by the domain</strong> not technical concerns!</p>\r\n\r\n<h2>Two Layer Repositories</h2>\r\n\r\n<p>There are situations where some kind of Repository makes sense, for example when using a framework like Spring.</p>\r\n<pre class=\"brush: java\">\r\ninterface CustomerRepository \r\n      extends CrudRepository&lt;CustomerEntry, UUID&gt; {\r\n\r\n    CustomerEntry findByEmail(String email);\r\n\r\n    // other methods...\r\n\r\n    @Entity\r\n    class CustomerEntry {\r\n        @Id\r\n        @GeneratedValue(strategy = GenerationType.AUTO)\r\n        public UUID id;\r\n        public String firstName;\r\n        public String lastName;\r\n        public String email;\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>Here the Spring CRUD Repository takes care of JPA implementation and hides its details from the actual use-case implementation. There are still too many responsibilities and potential high coupling among use-cases, but the repository serves at least a good purpose - hiding the complexity of the ORM framework out from the use-case code.</p>\r\n\r\n<p>The Spring Repository here is an infrastructure layer between the use-case implementation and a persistence store. It’s not anymore the Repository from DDD theory which appears in the domain. The Spring Repository is an explicit technical construct which is fine when used as such.</p>\r\n\r\n<p>For more details about the two layer repositories read this <a href=\"https://www.vzurauskas.com/2019/04/07/two-layer-repositories-in-spring/\" target=\"_blank\">great article</a>.</p>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>Repositories are highly misunderstood. The <strong>Repository pattern tends to be implemented as a pure technical construct</strong> and becomes easily a bunch of persistence methods with no common domain purpose. Rather they <strong>should be refactored to particular domain</strong> use-cases.</p>\r\n\r\n<p>However, <strong>Repositories can be used to hide complexity of particular persistence solutions</strong> like ORM. In that case, the Repository moves from the domain layer to the infrastructure layer. The cohesion and clarity of such Repositories should still be taken into account and the code should be cut by domain behavior. Good abstraction is king.</p>\r\n\r\n<p>You can find the source code is in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/repositories\" target=\"_blank\">my Github</a>.</p>\r\n\r\n<p>Happy coding!</p>', 'false', 'false', 1, 1);

-- --------------------------------------------------------

--
-- Table structure for table `Property`
--

CREATE TABLE IF NOT EXISTS `Property` (
  `name` varchar(50) NOT NULL,
  `value` varchar(100) DEFAULT NULL,
  PRIMARY KEY (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Property`
--

INSERT INTO `Property` (`name`, `value`) VALUES
('blogAuthor', 'Tomas Tulka - NET21 s.r.o.'),
('blogDescription', 'A small blog about programming and stuff.'),
('blogTitle', 'Tomas Tulka\'s Blog');

--
-- Constraints for dumped tables
--

--
-- Constraints for table `Comment`
--
ALTER TABLE `Comment`
  ADD CONSTRAINT `Comment_ibfk_1` FOREIGN KEY (`parentId`) REFERENCES `Comment` (`id`),
  ADD CONSTRAINT `Comment_ibfk_2` FOREIGN KEY (`postId`) REFERENCES `Post` (`id`);

--
-- Constraints for table `Post`
--
ALTER TABLE `Post`
  ADD CONSTRAINT `Post_ibfk_1` FOREIGN KEY (`authorId`) REFERENCES `Author` (`id`),
  ADD CONSTRAINT `Post_ibfk_2` FOREIGN KEY (`categoryId`) REFERENCES `Category` (`id`);
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
