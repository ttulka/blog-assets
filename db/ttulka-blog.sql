-- phpMyAdmin SQL Dump
-- version 4.7.7
-- https://www.phpmyadmin.net/
--
-- Host: localhost
-- Generation Time: Feb 19, 2021 at 04:35 PM
-- Server version: 10.3.18-MariaDB-0+deb10u1
-- PHP Version: 5.6.33-0+deb8u1

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET AUTOCOMMIT = 0;
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `ttulka-blog`
--

-- --------------------------------------------------------

--
-- Table structure for table `Author`
--

DROP TABLE IF EXISTS `Author`;
CREATE TABLE `Author` (
  `id` int(11) NOT NULL,
  `name` varchar(50) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Author`
--

INSERT INTO `Author` (`id`, `name`) VALUES
(1, 'Tomas Tulka');

-- --------------------------------------------------------

--
-- Table structure for table `Category`
--

DROP TABLE IF EXISTS `Category`;
CREATE TABLE `Category` (
  `id` int(11) NOT NULL,
  `name` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Category`
--

INSERT INTO `Category` (`id`, `name`) VALUES
(1, 'Programming'),
(2, 'Miscellaneous');

-- --------------------------------------------------------

--
-- Table structure for table `Comment`
--

DROP TABLE IF EXISTS `Comment`;
CREATE TABLE `Comment` (
  `id` int(11) NOT NULL,
  `createdAt` int(10) UNSIGNED NOT NULL,
  `author` varchar(50) DEFAULT NULL,
  `body` text DEFAULT NULL,
  `parentId` int(11) DEFAULT NULL,
  `postId` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT;

--
-- Dumping data for table `Comment`
--

INSERT INTO `Comment` (`id`, `createdAt`, `author`, `body`, `parentId`, `postId`) VALUES
(1, 1563007000, 'Siddharth Zunjarrao', 'This page is indeed quite helpful for our case. We are looking at a scenario where we have two different Microsoft Active Directories and our application can talk to only one AD. I can see a scenario where one LDAP and one AD is tested. Has MyVD been tested for connecting two AD\'s and act as one single AD to provide authentication?', NULL, 35),
(2, 1564316000, 'ttulka', 'The best would be to ask the authors of MyVD directly: https://github.com/TremoloSecurity/MyVirtualDirectory/issues', 1, 35),
(3, 1574188024, 'daggerson', 'An Api will still be changed as the implementation provides a better insight into the problem. Up-front design doesn\'t work. ', NULL, 64),
(4, 1574228083, 'ttulka', 'I agree. API could be improved as more details are explored during implementation, that\'s why I still see *make it right* very important. The difference is how much. A well-defined API tends to be pretty stable while implementing right ahead requires a lot of refactoring afterwards. This could be problematic as not everybody enjoys deleting code.', 3, 64),
(5, 1575456507, 'Pablo', 'Using Spring MVC itself makes the application non-OO.  You\'re trying to solve unnecessary problems you actually caused.', NULL, 66),
(6, 1575474306, 'ttulka', 'I agree with the first part: Spring framework is not very object-oriented indeed. As it can\'t know your domain, it focuses on solving technical issues. MVC pattern heavily used by Spring is not an OO practice as it conceptually belongs to a higher level. MVC is an architectural pattern. Object-oriented is (should be!) in MVC only its \"M\" layer. In general, MVC works with anemic data rather than objects. On the other hand, MVC is well known and understood, and, when used with caution, brings more benefits than drawbacks.\n\nI do disagree with your conclusion. Even in a perfectly OO application, when crossing the boundaries you don\'t deal with objects but mere data. Consider an application which communicates with another one via JSON or XML. The same problem emerges. You don\'t save objects but data into the database, you don\'t print object but data on the screen, etc.', 5, 66),
(7, 1575917643, 'sleeping_dragon', 'You\'re mentioning messages. I see using DTOs much more practical here as the client can derive the structure inside the message from the DTO type declaration without any need to know the data structure in advance from the protocol specification or some kind of documentation. ', NULL, 66),
(8, 1575962652, 'ttulka', 'I would be careful with saying that the client doesn\'t need to know the data structure it consumes. The structure is a part of the contract which should be well defined, published and understood by each participant. Even with a strongly typed structure the client *must know* which data it consumes, otherwise it can\'t do anything useful with it. \n\nAs there are several kinds of messages, there is no general answer. Talking about domain events, it could be okay to have a reference to a domain object (better immutable) inside the message. DTOs are fine in a monolithic application where you can have a module with DTO definitions shared across the application (known as \"Shared Kernel\" in Domain-Driven Design). As far as the message is sent via network, it contains nothing but data. Using shared libraries in the microservice ecosystem is an antipattern called \"Distributed Monolith\" (and it\'s really an utter nightmare to deal with).', 7, 66),
(9, 1576326865, 'Gerald', 'Isn\'t putting the `save(...)` method into `Employee` actually against the SRP? I would expect it only in the repository: `Registry#save(Employee)`, and being called from the application code.', NULL, 68),
(10, 1576355891, 'ttulka', 'Partly, you might be right. Many authors really say that. But I always prefer not to follow principles blindly. Doing what you propose follows the SRP strictly, but leads to exposing object\'s internals (typically via getters) and breaks the principle of encapsulation. The Employee is no employee anymore, just employee\'s data. Encapsulation is for me a higher principle that the SRP; software design is about tradeoffs.\nAnother point is, that the implementation of `save` is actually not in the Employee class. The class provides only a facade for the actual implementation. This means, a change in the persistence mechanism, like using a different database, will not require a change in the Employee class. And that\'s what the SRP is all about. ', 9, 68),
(11, 1577819388, 'Bert', 'Hello! I am following this, but when trying to run I get the following error: \n\"User <user> is not authorized to create: topic://ActiveMQ.Advisory.Connection\"\nThis only happens when I add the authorizationPlugin.\nAny pointers? Thanks.', NULL, 2),
(12, 1579470944, 'skapral', '> The biggest problem with the blue book I have is that it focuses a lot on tactical design and building blocks like Entities, Services and Repositories. At the end of the day that is the only thing some readers get from the text ignoring the real value: the importance of communication with domain experts and understanding the business being built.\n\nYes, I had exactly the same feeling about \"the blue book\" back in the days. Loved the idea behind ubiquitous language and \"talking with business\" a lot. But later... repositories, services, entities... made me sad quickly. These procedural dinosaurs just ruin the idea of orientation on domain. People get concerned more on patterns then the business purpose.\n\nI like the post.', NULL, 69),
(13, 1579472842, 'skapral', 'I.... wouldn\'t be so straight on that. IMO the subject is more complicated.\n\n> Sharing code among different requirements is a bad idea, because business is volatile and the rules might change independently.\n\nIt\'s true that business is volatile. And it\'s deadly right to avoid reusing volatile things. But there is one part of a typical object (or unit) under test that is (supposed to be) stable --- it\'s API. And API is stable when it is bound not to software requirements but to the purpose, the mission of software. And the purpose, if correctly defined, is stable.\n\nWhy it matters? Stable API can be a basis of reusable assertions. For example: if we consider that Account interface from your post is stable (as its purpose is to check whether account could be logged in), one can define a reusable assertion, like \"assertAccountCanBeLoggedIn(account, password)\". And this assertion would be pretty reusable for tests related to each and every current and future implementations of Account.', NULL, 65),
(14, 1579472882, 'skapral', 'More on this idea: https://www.pragmaticobjects.com/chapters/003_reusable_assertions.html', 13, 65),
(15, 1579547023, 'ttulka', '> But there is one part of a typical object (or unit) under test that is (supposed to be) stable --- it\'s API.\n\nI fully agree on that. The test `registered_user_account_is_in_the_registry` is working with the fact, that `Account::register` is stable.\n\nWhat I mean with \"sharing code\" is sharing implementation code, concretely the implementation of the test. Creating a shared object in the test suite and using it among tests, for example.', 13, 65),
(16, 1579547097, 'ttulka', '> Stable API can be a basis of reusable assertions.\n\nUsing reusable assertions, as you proposed in your block post, is an interesting idea, but being really practical it requires some kind of framework (like https://github.com/pragmatic-objects/oo-tests) or a lot of boilerplate code. As the one important purpose of a test is to provide a documentation, any unnecessary code serves against this purpose. See the 1. point (It\'s not obvious how the test is set up) in my post: implementing the test as a composition of assertions requires knowledge of those assertions. And only from the API providers, but especially from the users (clients). \n\nUnfortunately, I see a long way to go. My attempt was about to make the first move in that direction.', 13, 65),
(17, 1586597478, 'Andy', 'Why not just to have `Products.cheaperThan(..)`? It\'d make the interface simpler and more straightforward...', NULL, 72),
(18, 1586614018, 'ttulka', 'Having the find method on the `Products` object is okay for small projects where is no need for Domain Collections. You can have methods like `cheaperThan`, `byType`, `byCategory` etc. returning list `List<Product>` or similar. \r\nIn bigger codebases I see at least two problems with this approach:\r\n- One can\'t combine the find methods with Domain Collections methods. The find methods belong to a stateless service and a *Domain Collection* represents a stateful collection of objects. So this call would be valid but completely nonsense without calling any find method before: `products.sortByPrice()`. Such an object has two meanings, which is never a good idea. \r\n- Adding a new use-case for removing products, the same interface would have to be used (to be consistent). The object would then have too many responsibilities, which is bad for the maintainability and usability of such an API.', 17, 72),
(19, 1586937844, 'Turanga', 'Finding a customer by id sounds like a good abstraction to me. What\'s the issue with this kind of reusable code?', NULL, 73),
(20, 1586940437, 'ttulka', 'It could be okay if such an abstraction really emerges (you find the exactly same code over and over in code). But you should be aware that it always introduces coupling. \nFor example, consider the use cases NotifyCustomer and InvalidateCustomer. For performance reasons you may decide to load only customer name and email in first case, but you still need more Customer data for the latter. In such a scenario the findById method is reusable no more. \nThis shows how concrete use cases actually are and how brittle reusability is.\n\nI usually try to think about how easy/hard it would be to deploy the use case as a standalone running component (microservice). Even if you probably don\'t do it for FindCustomer, this kind of thinking will help you to structure the code better in sense of high cohesion and low coupling.', 19, 73),
(21, 1587310088, 'Sridhar B.', 'Those use cases are coupled anyway, at least through the database.', 19, 73),
(22, 1587310134, 'Sridhar B.', 'But the CustomerRepository is already partitioned by a domain concern - Customer...', NULL, 73),
(23, 1587311708, 'ttulka', 'Well, yes, it\'s better than having a single gigantic repository for all entities... \r\n\r\nBut the domain is spread throughout layers. To implement a new use case one has to always touch the repository. Same for changes - a change in any use case requests a change in the repository.', 22, 73),
(24, 1587311762, 'ttulka', 'It doesn\'t have to be so. Think for example about a CQRS solution with a separate database just for queries (`FindCustomer`).', 19, 73),
(25, 1589155219, 'AzmirH', 'Wouldn\'t it be better to put the Controllers into a domain package as well?', NULL, 45),
(26, 1589955951, 'ttulka', 'That\'s a very good point! This post doesn\'t take much care of proper Domain-driven design (it was not the purpose) - I\'m sorry for that. \r\n\r\nAnyway, first we have to understand what is the \"web\" module. It could be a composite that is using multiple services, so we can name it \"portal\", \"catalog\" or similar. If the controller *is* part of the service, we should put it into a service package indeed, like \"..order.OrderController\".\r\n\r\nOne thing to mention is that the controller is kinda client of the service domain and should probably not be using protected APIs (like Repository) directly. A way to prevent developers from this temptation could be to put the Controller into a sub-package, like \"..order.web.OrderController\".\r\n\r\nYou can see an example of the idea here: https://github.com/ttulka/ddd-example-ecommerce', 25, 45),
(27, 1594260643, 'Perry', '> good abstraction doesn\'t need interfaces\nInterfaces are the purest form of abstraction, those two things go naturally together.', NULL, 76),
(28, 1594372210, 'ttulka', 'Sure, but Interfaces are just a technical construct, they don\'t guarantee a good abstraction whatsoever. Further, there are languages with no Interfaces such as JavaScript, this, however, doesn\'t mean that one can\'t build any abstractions in JavaScript.\r\n\r\nSo, yes, Interfaces are a good tool to express abstractions on the language level. However, they are not necessary for building abstractions, and they are not necessarily abstractions. That\'s what I tried to say in the post.', 27, 76),
(29, 1606539741, 'Chandler', '> cohesion is defined by the client...  this is exactly what the Single Responsibility Principle teaches us.\n\nI don\'t think so. The SRP means \"single\" responsibility, that is, a single thing. Bringing responsibilities together will violate the SRP!', NULL, 80),
(30, 1606661135, 'ttulka', 'To quote the author, taken directly from Robert Martin\'s blog (https://bit.ly/3mnmXvW):\r\n\r\n\"The SRP states that each software module should have one and only one reason to change. [...] Those changes can only originate from a single person, or rather, a single tightly coupled group of people representing a single narrowly defined business function.\"\r\n\r\nAnd later on:\r\n\r\n\"This principle is about **people**.\"\r\n\r\nThis means that if a single person has a set of responsibilities those *should* be brought together.', 29, 80),
(31, 1606661835, 'ttulka', 'To me, it makes a lot of sense to put people into the center, as the client defines the desired behavior of the system. \nIdeally every business use-case should be carried out by a single request. \nDesign of services and objects should be behavior-driven.', 29, 80),
(32, 1607305402, 'Cody', 'The traditional definitions are IMO much simpler: cohesion - number of connections inside a code component, coupling - number of connections between code components. \r\nWhy don\'t you just use those definitions?', NULL, 80),
(33, 1607326371, 'ttulka', 'Yes, your definitions are simple, the question is, how beneficial they actually are...?\r\n\r\nIn your case, cohesion works against coupling and vice versa. It\'s similar to the myth I described, just the opposite extreme. For example, a God object would have the highest cohesion. Okay. Does this help us further? Not much.\r\n\r\nI don\'t think that it is enough to just count connections and tune their degrees. It\'s more complicated than that.\r\nMy view on cohesion might be a bit unconventional, but it helps build well modular systems. Main takeaways are:\r\n- Business and client drive cohesion.\r\n- Cohesion drives coupling.\r\n\r\nI believe that good software is more than a sum of its parts. It\'s as good as well it reflects the business it serves.', 32, 80),
(34, 1607310775, 'jeremy', 'Colors in the last picture, what are they supposed to mean?', NULL, 82),
(35, 1607411046, 'ttulka', 'It\'s kinda feelings map. \r\nHow risky systems as such are. How expensive, error-prone, hard to reason about, etc.', 34, 82),
(40, 1610476246, 'sam', 'I’m not sure, what the value of `iovs_len` means. There is already a length of the data (string) in the vector. Is it always `1`?', NULL, 97),
(41, 1610477437, 'ttulka', '`iovs_len` is the length of the list of IO vectors, not the length of the data. We have only one string, which means only one IO vector, therefore the value is `1`.\r\n\r\nConsider an example, where you have two strings to print to stdout. There must be also two IO vectors to describe those strings. The value of `iovs_len` must be then `2`:\r\n```\r\n(data (i32.const 16) \"hello\")\r\n(data (i32.const 22) \"world\")\r\n...\r\n(i32.store (i32.const 0) (i32.const 16)) ;; iov_base \"hello\"\r\n(i32.store (i32.const 4) (i32.const 5))  ;; iov_len \"hello\"\r\n(i32.store (i32.const 8) (i32.const 22)) ;; iov_base \"world\"\r\n(i32.store (i32.const 12) (i32.const 5)) ;; iov_len \"world\"\r\n\r\n(call $fd_write\r\n    (i32.const 1)\r\n    (i32.const 0)  ;; list of IO vectors starts on index 0\r\n    (i32.const 2)  ;; we have two IO vectors (two strings)\r\n    (i32.const 27) ;; 22 + 5 = 27\r\n)\r\n```\r\nResult:\r\n```\r\n$ wasmtime run hello2.wasm \r\nhelloworld\r\n```', 40, 97),
(42, 1610959268, 'Boyan Mihaylov', 'Great article! I played with these compilations some time ago and created the Wheel of WebAssembly (https://boyan.io/wasm-wheel/), which demonstrates how to compile from plenty of languages into WebAssembly', NULL, 98),
(43, 1610976557, 'ttulka', 'Cool stuff :-) Thanks for sharing!', 42, 98),
(44, 1613494274, 'Randy Y', 'Where did the cohesion equation sited in this post come from?', NULL, 80),
(45, 1613542578, 'ttulka', 'It\'s a format summary of what I describe in the text as my practical understanding of what cohesion is.', 44, 80);

-- --------------------------------------------------------

--
-- Table structure for table `Post`
--

DROP TABLE IF EXISTS `Post`;
CREATE TABLE `Post` (
  `id` int(11) NOT NULL,
  `url` varchar(100) NOT NULL,
  `createdAt` int(10) UNSIGNED NOT NULL,
  `title` varchar(100) NOT NULL,
  `summary` text DEFAULT NULL,
  `body` text DEFAULT NULL,
  `tags` varchar(255) DEFAULT NULL,
  `isMenu` enum('true','false') NOT NULL DEFAULT 'false',
  `isDraft` enum('true','false') NOT NULL DEFAULT 'true',
  `authorId` int(11) NOT NULL,
  `categoryId` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Post`
--

INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(1, 'about', 1392920000, 'About me', '<p>Ahoy, I&nbsp;am <a href=\"http://ttulka.com\" target=\"_blank\">Tomas Tulka</a>, a software developer based in a&nbsp;small village in Germany.</p>\r\n<p>I&nbsp;write mostly Java, and I&nbsp;love JavaScript. Currently,  I\'m working for a big corporation on a&nbsp;medical software. I&nbsp;wrote my first production code when I&nbsp;was fourteen; unfortunately, I&nbsp;can\'t find it anymore.</p>', '<p>Welcome to my blog of wonderfully random thoughts, which came to&nbsp;my mind over the years. Feel free to&nbsp;look around, stay awhile, and make yourself at&nbsp;home.</p>\r\n\r\n<p align=\"center\" style=\"float:right\"><img src=\"/assets/img/ttulka.png\" alt=\"Tomas Tulka | ttulka\"/></p>\r\n\r\n<p>You can catch me on&nbsp;<a href=\"https://www.linkedin.com/in/tomas-tulka\" target=\"_blank\">LinkedIn</a>.</p>\r\n\r\n<p>Please feel free to comment and discuss!</p>', NULL, 'true', 'false', 1, 2),
(2, 'activemq-jaas-custom-login-module', 1393000000, 'ActiveMQ + JAAS Custom Login Module', '<p>It is pretty easy to find how to run built-in JAAS plugin, but what shall you do when you want to your own JAAS LoginModule implementation for ActiveMQ broker authentication (for instance when you have the module already written)?</p> \r\n', '<h2>ActiveMQ + JAAS <br /></h2> \r\n<p>ActiveMQ messaging broker has several options how to deal with authentication and authorization. </p> \r\n<p>The easiest one is to use the <code>simpleAuthenticationPlugin</code>, but it is not very flexible.</p> \r\n<p>Another option is the <code>jaasAuthenticationPlugin </code>using\r\n two property files (users and groups), quite effective solution but \r\nstill very clumsy when you consider that all the information about users\r\n and groups are stored into files.</p> \r\n<p>Following text will describe how to use customer-defined JAAS LoginModule as an ActiveMQ plugin.</p> \r\n<h2>JAAS Custom Login Module for ActiveMQ</h2> \r\n<h3>Authentication</h3> \r\n<p>Once we have developed a JAAS login module we can build it and pack as a JAR library. To use it in the broker we need to put the JAR on the classpath of a running ActiveMQ instance. There are two ways how to do it:</p> \r\n<ol> \r\n<li> Copy the JAR into lib folder of the broker -<code>ACTIVEMQ_HOME/lib/extra</code> or <code>ACTIVEMQ_HOME/lib/optional</code></li> \r\n<li>Put a path to the JAR to the java classpath of the broker - edit <code>ACTIVEMQ_HOME/bin/activemq</code> (Unix) or <code>ACTIVEMQ_HOME/bin/activemq.bat</code> (Win) and extend the <code>ACTIVEMQ_CLASSPATH</code> parameter:<br /> \r\n<ul> \r\n<li><code>ACTIVEMQ_CLASSPATH=&quot;${ACTIVEMQ_CLASSPATH};/var/lib/auth-test.jar&quot;</code> (Unix)</li> \r\n<li><code>set ACTIVEMQ_CLASSPATH=%ACTIVEMQ_CONF%;%ACTIVEMQ_BASE%/conf;%ACTIVEMQ_HOME%/conf;%ACTIVEMQ_CLASSPATH%;c:/Develop/Java/lib/auth-test.jar</code> (Win)</li> \r\n</ul> \r\n<ul> </ul> \r\n</li> \r\n</ol> \r\n<h4>\r\n\r\nBroker Setting to use the Plugin\r\n</h4> \r\n<p>Edit the <code>ACTIVEMQ_HOME/conf/login.config</code> property file to use the plugin as following (consider the plugin class is <code>cz.net21.ActiveMqLoginModule</code>):</p> \r\n<p> </p> \r\n<pre class=\"brush: plain\">MyLoginModule { \r\n  cz.net21.ActiveMqLoginModule&nbsp; required debug=true;\r\n};\r\n</pre> \r\n<p> </p> \r\n<p>Edit the <code>ACTIVEMQ_HOME/conf/activemq.xml</code> configuration file as following:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;broker … &gt; \r\n  …\r\n  &lt;plugins&gt;\r\n    &lt;jaasAuthenticationPlugin configuration=\"MyLoginModule\" /&gt;\r\n  …\r\n  &lt;/plugins&gt;\r\n  …\r\n&lt;/broker&gt;\r\n</pre> \r\n<p> </p> \r\n<h4>Running the java code</h4> \r\n<p>Now we can run the java to see that the JAAS plugin is working:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"tcp://localhost:61616\");\r\nConnection connection = connectionFactory.createConnection(username, password);\r\nconnection.start();\r\n</pre> \r\n<p> </p> \r\n<p>If the method <code>login()</code> from the module class <code>cz.net21.ActiveMqLoginModule</code> returns <em>false </em>for added credentials, the code will return by en exception:</p> \r\n<pre class=\"brush: plain\">java.lang.SecurityException: User name [testuser] or password is invalid.\r\nCaused by: javax.security.auth.login.LoginException: Login Failure: all modules ignored</pre> \r\n<h3>Authorization</h3> \r\n<p>In this part we will look at a simple way how to authorize an user for broker\'s resources (queues/topics).</p> \r\n<p>Via <code>authorizationPlugin </code>we can setup all the rights for queues/topics and users groups. <br /></p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;authorizationPlugin&gt;\r\n  &lt;map&gt;\r\n    &lt;authorizationMap&gt;\r\n      &lt;authorizationEntries&gt;\r\n        &lt;authorizationEntry queue=\"&gt;\"\r\n          read=\"admins\"&nbsp; write=\"admins\"&nbsp; admin=\"admins\" /&gt;\r\n  …\r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>We can leave this settings and control the access via our custom JAAS module. All we need is to add an <code>UserPrincipal </code>object for the broker user and\r\na <code>GroupPrincipals </code>object  for the broker groups.</p> \r\n<p>Let\'s extend our module to consume a file with groups used in the authorizationPlugin:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">MyLoginModule { \r\n  cz.net21.ActiveMqLoginModule required \r\n    debug=true\r\n    org.apache.activemq.jaas.properties.group=\"groups.properties\";\r\n};</pre> \r\n<p> </p> \r\n<p>Here we use <code>org.apache.activemq.jaas.properties.group</code>  as a name of an option and <code>groups.properties</code> as a name of the property file with a list of groups. </p> \r\n<p> </p> \r\n<pre class=\"brush: java\">public void initialize(Subject subject, CallbackHandler handler, Map&lt;String, ?&gt; state, Map&lt;String, ?&gt; options) {\r\n  this.subject = subject;\r\n  groupsFile = options.get(\"org.apache.activemq.jaas.properties.group\") + \"\";\r\n  …\r\n</pre> \r\n<p> </p> \r\n<p>But we can use for instance <code>groupListFile </code>as the name and instead of the property file use a comma-separated file with just a list of group names.</p> \r\n<p>We will process the file and bind the logged user with one of the groups. </p> \r\n<pre class=\"brush: java\">public boolean login() throws LoginException {\r\n  try {\r\n    File f = new File(baseDir, groupsFile);\r\n    groups.load(new java.io.FileInputStream(f));\r\n  } catch (IOException e) {\r\n    throw new LoginException(\"Unable to load group properties file \" + groupsFile);\r\n  }\r\n  …\r\n</pre> \r\n<p>Then we need to put the <code>UserPrincipal </code>and <code>GroupPrincipals </code>objects into the subject\'s principals:</p> \r\n<pre class=\"brush: java\">principals.add(new UserPrincipal(user));\r\nprincipals.add(new GroupPrincipal(groupName));\r\nsubject.getPrincipals().addAll(principals);\r\n</pre> \r\n<p>If the <code>groupName</code> match the destination we will get the access, otherwise the code will return by en exception:</p> \r\n<p> </p> \r\n<pre class=\"brush: plain\">java.lang.SecurityException: User guest is not authorized to read from: queue://TestQ\r\n</pre> \r\n<p> </p> \r\n<p> <br /></p> \r\n<p>Congratulation, we have a JAAS plugin for authentication and authorization of ActiveMQ broker!</p> \r\n<h2>Appendix <br /></h2> \r\n<p>I am working with Java 7, ActiveMQ 5.9.0</p> \r\n<p>Please see the discussed code <a href=\"/storage/ActiveMQ_JAAS_Custom_Login_Module.zip\" title=\"The example code\">in the attachment</a>. <br /></p> \r\n<p><br /></p>', 'Programming,Java,JAAS,ActiveMQ', 'false', 'false', 1, 1),
(3, 'activemq-hornetq-and-rabbitmq-performance-comparison', 1393268000, 'ActiveMQ, HornetQ and RabbitMQ Performance Comparison', '<p>Messaging could be a great solution for a lot of projects regarding an inter-systems (and components) communication. But which vendor to choose? Which one is the best? </p> ', '\r\n<p>There is no proper answer for this question, because each and every provider has some pros and cons. Please read the first description of all of them and figure out only those matching your requirements. </p> \r\n<p>If your results include <strong>ActiveMQ</strong>, <strong>HornetQ </strong>and <strong>RabbitMQ</strong>, you are propably interested in performace and some practical observations now. And that is what is this article all about.</p>\r\n<h2>What is tested</h2> \r\n<p>We leave all the brokers in the default setting, no additional tuning was done yet. If you have some special requirement, you should probably look deeply at the features of the brokers and count in the final results.</p> \r\n<p>We run the same performance test for several scenarios:<br /></p> \r\n<ol> \r\n<li>One broker, one producer, one consumer on the only one server.</li> \r\n<li>One broker running on the server one, one producer, one consumer running on the server 2.</li> \r\n<li>Two brokers running on <strong>different </strong>servers (forwarding messages / bridging), one producer&nbsp; running on the server 1, one consumer running on the server 2.</li> \r\n</ol> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \r\n<tbody> \r\n<tr> \r\n<td><img alt=\"scenarios\" src=\"/storage/ActiveMQ_HornetQ_and_RabbitMQ_Performance_Comparison_2.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td style=\"text-align: center; padding-top: 3px;\"><em>Image 1:</em> Scenario 1, 2 and 3 architecture</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\nWe also run the test with different setting of clients:<br /> \r\n<ol> \r\n<li>queues, topics</li> \r\n<li>non-persistent, persistent massages</li> \r\n<li>one or ten threads for a queue/topic (one or ten message producers and consumers in parallel) </li> \r\n</ol>Because <strong>proportion of the results</strong> for the different scenarios is almost <strong>the same</strong>, we can focus only on the first scenario to invest the results and do the comparison. <br /> \r\n<h2>Results</h2>Parameters for the run:<br /> \r\n<ul> \r\n<li>non-persistent messages</li> \r\n<li>non-transacted</li> \r\n<li>sending messages in asynchronous mode</li> \r\n<li>receiving messages in asynchronous mode (AUTO_ACKNOWLEDGE)</li> \r\n<li>not encrypted</li> \r\n</ul> \r\n<p>Running on Linux system in virtual machine with 2 cores and 8 GB RAM.</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"3\" border=\"1\" class=\"resultsTable\" style=\"width: 701px;\"> <caption>Results table</caption> \r\n<tbody> \r\n<tr> \r\n<td> queues<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n</tr> \r\n<tr> \r\n<td> consumers (per queue)<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n</tr> \r\n<tr> \r\n<td>  producers (per queue)</td> \r\n<td> 1<br /></td> \r\n<td> 1<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n<td> 10<br /></td> \r\n</tr> \r\n<tr> \r\n<td>message body size (bytes)<br /></td> \r\n<td> 128<br /></td> \r\n<td> 2048<br /></td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n<td>  128</td> \r\n<td>  2048</td> \r\n</tr> \r\n<tr> \r\n<td><strong> ActiveMQ</strong> (msg/sec)</td> \r\n<td><em> 14590</em></td> \r\n<td><em> 14565</em></td> \r\n<td><em> 15700</em></td> \r\n<td><em> 15640</em></td> \r\n<td><em> 17920</em></td> \r\n<td><em> 17995</em></td> \r\n<td><em>  18180</em></td> \r\n<td><em>19220</em></td> \r\n</tr> \r\n<tr> \r\n<td><strong>HornetQ</strong> (msg/sec)</td> \r\n<td><em> 46605</em></td> \r\n<td><em> 47830</em></td> \r\n<td><em> 10140</em></td> \r\n<td><em> 10802</em></td> \r\n<td><em> 16165</em></td> \r\n<td><em> 18885</em></td> \r\n<td><em> 19852</em></td> \r\n<td><em> 19905</em></td> \r\n</tr> \r\n<tr> \r\n<td><strong>RabbitMQ</strong> (msg/sec)</td> \r\n<td><em> 20550</em></td> \r\n<td><em> 14680</em></td> \r\n<td><em> 14422</em></td> \r\n<td><em> 13065</em></td> \r\n<td><em> 13770</em></td> \r\n<td><em> 12150</em></td> \r\n<td><em>  13800</em></td> \r\n<td><em> 11930</em></td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\"> \r\n<tbody> \r\n<tr> \r\n<td><img alt=\"results\" src=\"/storage/ActiveMQ_HornetQ_and_RabbitMQ_Performance_Comparison_1.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td style=\"text-align: center; padding-top: 3px;\"><em>Image 2:</em> Results chart</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<h2>My point of view</h2> \r\n<p><strong>ActiveMQ </strong>is very good tool with lot of documentation, examples, support and a huge community. Is it not the best regarding the performance, but it is announced to be rapidly improved by the version 6 using Appolo module. We will see...<br /> <br /></p> \r\n<p><strong>HornetQ </strong>really shines in the performance and it is pretty useful with a good support as well, but a lot of features (for instance the management console) is tightly&nbsp; bound with running as a JBoss module. Running in a stand-alone mode (as I did) has several disadvantages and I would not recomment it. But if you are using (or going to use) the JBoss Application Server for running you web application, HornetQ could the right option for you. </p> \r\n<p>There is a bit mess in versioning of libraries, especially if you want to work with different vendors of components involved in the integration (Netty, Spring JMS, ...).</p> \r\n<p>Queues and topics must be declared in hornetq-jms.xml before and cannot be created and deleted dynamically using JMS API. <br /> <br /></p> \r\n<p><strong>RabbitMQ</strong> does not have a good performance and it is not very human-friendly (all the setting are written in Erlang, which makes longer configurations very unclear for a programmer). </p> \r\n<p>There is only a commercial implementation of JMS. You can use pure RabbitMQ libraries or the Spring AMQP, but it is a different approach from JMS and has several limitations, for instance AMQP 0-9-1 protocol used as native doesn\'t support durable subscribers.</p> \r\n<p> <br /></p>', 'Event-Driven,ActiveMQ,HornetQ,RabbitMQ', 'false', 'false', 1, 2),
(4, 'principals-from-jaas-through-cas-to-spring-security', 1394041000, 'Principals from JAAS through CAS to Spring Security', '<p>This article is about a custom integration of Java Authentication and Authorization Service (JAAS), Central Authentication Service (CAS), and Spring Security.</p> \r\n', '<p>Let\'s imagine a situation we have  a developed username-password based JAAS Login Module implementing an authentication process.<br />The JAAS Login Module is used by CAS server as a authentication handler.<br />The CAS server is used by Spring Security via its CAS authentication provider plugin.</p> \r\n<p>So far it is easy to implement by a lot of existing manuals and HOWTOs using just provided libraries with no need to extend or modify a code.</p> \r\n<p>But what if the <strong>JAAS module contains also an authorization</strong>, implemented for instance by putting principals (roles) to a subject. To <strong>transmit the roles</strong> from JAAS module through CAS server to the Spring context we need to do some additional work as following.<br /></p>\r\n<p><em>Principal</em> is a term used by JAAS and CAS in a different meaning. Spring Security module uses rather a term <em>role</em>. We will use the term <em>a principal </em>or <em>a role </em>in the meaning of <strong>user\'s group</strong>.</p> \r\n<h2>Collaboration</h2> \r\n<p> The collaboration among nodes is shown in the picture:</p> \r\n<p><img src=\"/storage/Principals_from_JAAS_through_CAS_to_Spring_Security_1.png\" /><br /></p> \r\n<h2>JAAS Login Module</h2> \r\n<p> There is no big deal with JAAS, we need just to implement the <code>javax.security.auth.spi.LoginModule</code> interface and put our logic to authenticate a user.</p> \r\n<p>For authorization we will use our custom class <strong><code>CommonGroupPrincipal </code></strong>implementing the <code>java.security.Principal</code> interface.</p> \r\n<p>By some logic we assign roles to the user (as in the following very simple example) optimally in the commit method:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">principals.add(new CommonGroupPrincipal(\"ROLE_USER\"));\r\nif (\"admin\".equals(user)) {\r\n    principals.add(new CommonGroupPrincipal(\"ROLE_ADMIN\"));\r\n}\r\nsubject.getPrincipals().addAll(principals);</pre> \r\n<p> Now we can use the modul in the CAS server. All we need to do is run an application server (Tomcat for instance) containing the CAS WAR file with a Java option defining the JAAS configuration file location.<br />Modify the run script of the application server as following (for Tomcat, Windows):</p> \r\n<pre class=\"brush: plain\">set JAVA_OPTS=%JAVA_OPTS% -Djava.security.auth.login.config=%CATALINA_HOME%/conf/jaas.config</pre> \r\n<p>and create the configuration file: <br /></p> \r\n<pre class=\"brush: plain\">CAS {\r\n    ttulka.test.auth.jaas.module.CasLoginModule required;\r\n};</pre> \r\n<p>Alternatively the Java default java.security.auth.login.config could be used, or the Java option could be set directly from the code of <code>CustomJaasAuthenticationManager </code>(see below):</p> \r\n<pre class=\"brush: java\">System.setProperty(\"java.security.auth.login.config\", \"/path/to/jaas/config/file\");</pre> \r\n<h2>Spring Security CAS plugin</h2> \r\n<p>We can setup the Spring Security CAS plugin in a usual way, but we will implement our custom <code>authenticationUserDetailsService </code>property from the CAS authentication provider bean:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"casAuthenticationProvider\" class=\"org.springframework.security.cas.authentication.CasAuthenticationProvider\"&gt;\r\n	&lt;property name=\"authenticationUserDetailsService\"&gt;\r\n		&lt;bean class=\"ttulka.test.auth.spring.CustomAuthenticationUserDetailsService\" /&gt;\r\n	&lt;/property&gt;\r\n	...</pre> The class <code>CustomAuthenticationUserDetailsService </code>implements the <code>org.springframework.security.core.userdetails.AuthenticationUserDetailsService</code> interface and its only one method deals with an object representing a CAS response token:&nbsp;\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n<pre class=\"brush: java\">public class CustomAuthenticationUserDetailsService implements AuthenticationUserDetailsService&lt;CasAssertionAuthenticationToken&gt; {\r\n&nbsp;&nbsp; &nbsp;@Override\r\n&nbsp;&nbsp; &nbsp;public UserDetails loadUserDetails(CasAssertionAuthenticationToken token) throws UsernameNotFoundException {\r\n\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;final String groupList = (String)token.getAssertion().getPrincipal().getAttributes().get(\"CommonGroupPrincipal\");\r\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;...\r\n</pre> \r\n<p>The variable <code>groupList</code> is holding a <strong>roles list transmitted from the CAS server</strong> got from the JAAS module. We will use to <strong>build a list of the granted authorities</strong> returned as a part of the <code>UserDetails</code> object.</p> \r\n<h2>CAS server extension</h2> \r\n<p> To push a CAS to consume and transmit additional attributes as roles we need to implement our custom alternative of an <strong>authentication handler</strong> and its standard implementation by the class <strong><code>JaasAuthenticationHandler</code></strong> class. Unfortunately the desired method <code>authenticateUsernamePasswordInternal</code> is in the <code>JaasAuthenticationHandler</code> class defined as final, so we cannot just simply extend the class.</p> \r\n<p>To work with our custom handler we need to extend the CAS <strong>authentication manager</strong>, too. As in the previous class, the standard implementation <strong><code>AuthenticationManagerImpl</code></strong> is set as a final class, so we have to create a new one.</p> \r\n<h3>JAAS Authentication Handler</h3> \r\n<p>Alike the <code>JaasAuthenticationHandler </code>we will extend the class <code>org.jasig.cas.authentication.handler.support.AbstractUsernamePasswordAuthenticationHandler</code> and implement just the method <code>authenticateUsernamePasswordInternal</code>.</p> \r\n<p>Using the standard JAAS process we log in via credentials and get principals from the JAAS subject represented by <code>CommonGroupPrincipal</code> objects (see above).<br />From the list we create a comma-separated string and put it into a principals map by a key as the class name (a public accessible property of the handler class):</p> \r\n<pre class=\"brush: java\">public class CustomJaasAuthenticationHandler extends AbstractUsernamePasswordAuthenticationHandler {\r\n    ...\r\n    private Map&lt;String, Object&gt; principals = new HashMap&lt;&gt;();\r\n    ...\r\n    protected boolean authenticateUsernamePasswordInternal(final UsernamePasswordCredentials credentials)...\r\n        ...\r\n        loginContext = new LoginContext(realm, handler);\r\n        loginContext.login();\r\n	   \r\n        processPrincipals(loginContext.getSubject().getPrincipals());\r\n        ...\r\n    ...\r\n    private void processPrincipals(Set&lt;Principal&gt; principalsSet) {\r\n        ...\r\n        for (Principal p : principalsSet) {\r\n            if (p instanceof CommonGroupPrincipal) {\r\n                sb.append(p.getName());\r\n                ...\r\n            }\r\n        }\r\n        principals.put(CommonGroupPrincipal.class.getSimpleName(), sb.toString());\r\n        ...\r\n</pre> \r\n<p>The handler will be used directly by our new manager (see below) so needs no additional setting. <br />Alternatively we can define it in the CAS webapp configuration file (<code>WEB-INF/deployerConfigContext.xml</code>) as a new bean and then inject into the manager.</p> \r\n<p> </p> \r\n<h3>JAAS Authentication Manager</h3> \r\n<p>Alike the <code>AuthenticationManagerImpl</code> we will extend the class <code>org.jasig.cas.authentication.AbstractAuthenticationManager</code> and implement just the method <code>authenticateAndObtainPrincipal</code>.</p> \r\n<p>After the authentication succeeds we build the principal\'s attributes from the handler\'s principals and return:</p> \r\n<pre class=\"brush: java\">handler = new CustomJaasAuthenticationHandler();\r\n...\r\nPrincipal principal = new SimplePrincipal(((UsernamePasswordCredentials)credentials).getUsername(), handler.getPrincipals());\r\n\r\nreturn new Pair&lt;AuthenticationHandler,Principal&gt;(handler, principal);</pre> \r\n<p>In the CAS webapp configuration file (<code>WEB-INF/deployerConfigContext.xml</code>) we need to set the manager:</p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"authenticationManager\" class=\"ttulka.test.auth.cas.CustomJaasAuthenticationManager\" /&gt;</pre> \r\n<p>To include the attributes with roles in the response to the Spring Security plugin we need to <strong>allow the attribute name <code>CommonGroupPrincipal</code> </strong>for a registered service in <code>WEB-INF/deployerConfigContext.xml</code>:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;bean id=\"serviceRegistryDao\" class=\"org.jasig.cas.services.InMemoryServiceRegistryDaoImpl\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;property name=\"registeredServices\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;list&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;bean class=\"org.jasig.cas.services.RegexRegisteredService\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ... \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;property name=\"allowedAttributes\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;list&gt;\r\n &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;value&gt;CommonGroupPrincipal&lt;/value&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/list&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/property&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/bean&gt; \r\n            ...\r\n</pre> \r\n<p>The last step is to modify the response renderer JSP page <code>WEB-INF/view/jsp/protocol/2.0/casServiceValidatorSuccess.jsp</code> to contain the attributes: <br /></p> \r\n<pre class=\"brush: xml\">&lt;cas:serviceResponse xmlns:cas=\'http://www.yale.edu/tp/cas\'&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;cas:authenticationSuccess&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;cas:user&gt;${fn:escapeXml(assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.id)}&lt;/cas:user&gt;&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;     &lt;c:forEach var=\"attr\" items=\"${assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes}\"&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &lt;cas:${fn:escapeXml(attr.key)}&gt;${fn:escapeXml(attr.value)}&lt;/cas:${fn:escapeXml(attr.key)}&gt;\r\n    &nbsp;&nbsp;&nbsp; &lt;/c:forEach&gt;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &lt;/cas:attributes&gt; \r\n        ... \r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p> </p> \r\n<p>The response will then looks like: <br /></p> \r\n<pre class=\"brush: xml\">&lt;cas:serviceResponse xmlns:cas=\'http://www.yale.edu/tp/cas\'&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:authenticationSuccess&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:user&gt;admin&lt;/cas:user&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;cas:CommonGroupPrincipal&gt;ROLE_USER,ROLE_ADMIN&lt;/cas:CommonGroupPrincipal&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/cas:attributes&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/cas:authenticationSuccess&gt;\r\n&lt;/cas:serviceResponse&gt;\r\n</pre> \r\n<p>The responsed XML is then processed by the Spring\'s <code>CustomAuthenticationUserDetailsService</code> (see above).</p> \r\n<p>All the CAS-related classes must be packed as a JAR library and copied to <code>/WEB-INF/lib</code> folder of the CAS WAR application.<br /></p> \r\n<p> </p> \r\n<p>And the integration is ready to use!</p> \r\n<h2>Appendix</h2> \r\n<p>I am working with Java 7, CAS 3.5.2, Spring 3.2.4. and Spring Security module 3.2.0.</p> \r\n<p>Please see the discussed code <a href=\"/storage/Principals_from_JAAS_through_CAS_to_Spring_Security.zip\" title=\"The example code\">in the attachment</a>. </p>', 'Programming,Java,JAAS,CAS', 'false', 'false', 1, 1),
(5, 'synchronization-with-modification-of-the-lock-reference', 1395765000, 'Synchronization with Modification of the Lock Reference', '<p>...is very bad practice. Nevertheless, it is not so rare to meet it.</p> \r\n', '<p>In <strong>legacy sources</strong> I have found a really tricky code causing an occasional error.</p> \r\n<p>Well, you can say all around synchronization is tricky, but good understanding is a clue to eliminate the magic - this article might help a bit.</p>\r\n<p>Let\'s consider a very basic logic of the code: </p> \r\n<p><em>Thread-agents are dealing with a shared data of the singleton system.</em></p> \r\n<p>Alright, easy, the <strong>code working with the shared data must be synchronized</strong>.\r\n Yes, but don\'t forget that the threads (the agents) are working with \r\ndata of another class (the system) - we can not use synchronized methods\r\n as they are using the instance of an agent as a mutex instead of the \r\nclass the data belongs to. Well, let\'s use the shared system\'s object as\r\n a lock and... we are ready... wait a minute: This is exactly the \r\nsolution implemented in the legacy code!</p> \r\n<p>Where\'s the problem - this must work! Sure, but not in any case... <br /></p> \r\n<p>In a very simplified way the original code looks like this:</p> \r\n<pre class=\"brush: java\">public class SystemApp {\r\n \r\n&nbsp;&nbsp;&nbsp; static Integer count = 0;\r\n\r\n&nbsp;&nbsp;&nbsp; static class AgentThread extends Thread {\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; public void run() {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; try {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Thread.sleep(ThreadLocalRandom.current().nextInt(100));\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } catch (InterruptedException e) { }&nbsp;&nbsp;&nbsp; \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // do something\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // ...\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; synchronized (count) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; count ++;\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // do something else\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // ...\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n \r\n&nbsp;&nbsp;&nbsp; public static void main(String[] argv) {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // run agents\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; for (int i = 0; i &lt; 1000; i ++) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; new AgentThread().start();\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // sleep for a while to let agents finish their work\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; try {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Thread.sleep(5000);\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; } catch (InterruptedException e) { }&nbsp;&nbsp;&nbsp; \r\n \r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // print the result\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; System.out.println(count);\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<p>What is the expected result printed on the screen? 1000? Right!</p> \r\n<p>But guess what we get by running it five times:</p> \r\n<p>955<br />979<br />973<br />959<br />963</p> \r\n<p>Of course, the pain is in here:</p> \r\n<pre class=\"brush: java\">synchronized (count) {\r\n&nbsp;&nbsp;&nbsp; count ++;\r\n}</pre> \r\n<p>We are using as a lock an object referenced by the variable <em>count</em>. But this reference is changed inside the critical section. This means there is no guarantee that two or more parallel threads get the same lock object. And so sometimes happens this scenario: </p> \r\n<ol> \r\n<li>The thread A and the thread B are trying to enter the critical section.</li> \r\n<li>The thread A gets the access and the lock referenced by the variable <em>count </em>(currently for instance with a value of integer=2) gets locked up.</li> \r\n<li>The thread B is waiting for the release of the lock (integer=2).</li> \r\n<li>The thread A changes the reference of the variable <em>count </em>to the new value (integer=3) and releases the lock (integer=2).</li> \r\n<li>The thread B waiting for the lock (integer=2) gets that lock, enters the critical section and locks up the lock object (integer=2).</li> \r\n<li>The parallel thread C tries to enter the critical section with the lock object referenced by the variable <em>count </em>(integer=3 - changed by the thread&nbsp;A).</li> \r\n<li>The tread C gets the access to the critical section because its lock is now the object integer=3 while the thread B is being in the critical section as well (with the lock object integer=2).</li> \r\n</ol> \r\n<p>As you can see, by the seventh point we have <strong>two threads in the critical section</strong>.\r\n</p> \r\n<p>The lesson is: <u>never ever use a variable object as a lock!</u><br /></p> \r\n<p> <br /></p> \r\n<p> </p> \r\n<p>Synchronization is tricky and it is always good to think twice. </p> \r\n<p>By the way, using the API from the <em>java.util.concurrent</em> package is a good idea, too.\r\n</p> \r\n<p> <br /></p>', 'Programming,Java', 'false', 'false', 1, 1),
(6, 'rmi-meets-jms', 1403112000, 'RMI meets JMS', '<p>A simple lightweight library for <strong>calling remote services via JMS</strong>.</p> \r\n', '<p>Library is using only JMS 1.1 API and it\'s Requestor and bytes messages.</p> \r\n<p>As simple as it can be, ready to use!</p>\r\n<h2>How to use</h2> \r\n<p>Put the maven dependency into your POM file:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n   &lt;groupId&gt;cz.net21.ttulka&lt;/groupId&gt;\r\n&nbsp;&nbsp; &lt;artifactId&gt;rmi-meets-jms&lt;/artifactId&gt;\r\n&nbsp;&nbsp; &lt;version&gt;1.0.1&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Alternatively you can <a href=\"storage/rmi-meets-jms-1.0.1.jar\">download the library in the JAR file</a> and put it directly into your project classpath.</p> \r\n<h3>Remote service</h3> \r\n<p>The remote service is just a simple interface implemented by a class:</p> \r\n<pre class=\"brush: java\">public interface Service {\r\n \r\n&nbsp;&nbsp;&nbsp; Integer myMethod1(Integer i);\r\n&nbsp;&nbsp;&nbsp; void myMethod2(String str);\r\n}\r\n\r\npublic class ServiceImpl implements Service {\r\n\r\n&nbsp;&nbsp;&nbsp; public Integer myMethod1(Integer i) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; return i * 2;\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; public void myMethod2(String str) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; System.out.println(\"myMethod2 says: \" + str);\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<p>Methods of the service can be overloaded. <br /></p> \r\n<h3>Java code</h3> \r\n<p>To use the library directly from a java code you need to create a JMS factory and a destination queue:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">final QueueConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"tcp://localhost:61616\");\r\nfinal Queue queue = new ActiveMQQueue(\"MyQueue1\");</pre> \r\n<p>Then, on the <strong>server side</strong>, the remote service provider must be created with the service implementation as a parameter:</p> \r\n<pre class=\"brush: java\">final Service serviceImpl = new ServiceImpl();\r\n\r\nfinal RemoteServiceProvider provider = new RemoteServiceProvider(connectionFactory, queue, serviceImpl);</pre> \r\n<p>On the <strong>client side</strong> the remote service consumer must be created and then the remote service can be obtained:</p> \r\n<pre class=\"brush: java\">final RemoteServiceConsumer consumer = new RemoteServiceConsumer(connectionFactory, queue, Service.class);\r\n\r\nfinal Service service = (Service)consumer.getService();\r\n</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>and the service can be called:</p> \r\n<pre class=\"brush: java\">Integer res1 = service.myMethod1(3);\r\nSystem.out.println(res1);               // will print \"6\"\r\n\r\nservice.myMethod2(\"Hello, server!\");    // will print \"Hello, server!\" on the server\'s console output</pre> \r\n<h3>Spring approach</h3> \r\n<p>The same can be achieved with the Spring framework very easily (as everything is easy with the Spring):</p> \r\n<pre class=\"brush: xml\">&lt;!-- Server side --&gt;\r\n&lt;bean id=\"server\" class=\"cz.net21.ttulka.rmimeetsjms.RemoteServiceProvider\"&gt;\r\n	&lt;constructor-arg ref=\"jmsFactory\" /&gt;\r\n	&lt;constructor-arg ref=\"queue\" /&gt;\r\n	&lt;constructor-arg ref=\"serviceImpl\" /&gt;\r\n&lt;/bean&gt;\r\n\r\n&lt;!-- Client side --&gt;\r\n&lt;bean id=\"client\" class=\"cz.net21.ttulka.rmimeetsjms.RemoteServiceConsumer\"&gt;\r\n	&lt;constructor-arg ref=\"jmsFactory\" /&gt;\r\n	&lt;constructor-arg ref=\"queue\" /&gt;\r\n	&lt;constructor-arg value=\"mypack.Service\" /&gt;\r\n&lt;/bean&gt;\r\n&lt;bean id=\"serviceProxy\" factory-bean=\"client\" factory-method=\"getService\" /&gt;\r\n\r\n&lt;!-- Service implementation --&gt;\r\n&lt;bean id=\"serviceImpl\" class=\"mypack.ServiceImpl\" /&gt;\r\n\r\n&lt;!-- JMS connection factory --&gt;\r\n&lt;bean id=\"jmsFactory\" class=\"org.apache.activemq.ActiveMQConnectionFactory\"&gt;\r\n	&lt;property name=\"brokerURL\" value=\"tcp://localhost:61616\" /&gt;\r\n&lt;/bean&gt;\r\n\r\n&lt;!-- JMS destination --&gt;\r\n&lt;bean id=\"queue\" class=\"org.apache.activemq.command.ActiveMQQueue\"&gt;\r\n	&lt;constructor-arg index=\"0\" value=\"MyQueue1\" /&gt;\r\n&lt;/bean&gt;</pre> \r\n<p>All you need to do is to initialize the Spring context:</p> \r\n<pre class=\"brush: java\">final ApplicationContext context = new ClassPathXmlApplicationContext(\"spring-context.xml\");</pre> \r\n<p>and get the service proxy on the client side:</p> \r\n<pre class=\"brush: java\">final Service service = (Service)context.getBean(\"serviceProxy\");</pre> \r\n<h2>Limitation <br /></h2> \r\n<p>There is only one limitation regarding the parameters of the remote service: all the <strong>parameters must be serializable</strong> (must implement <code>java.io.Serializable</code>).</p> \r\n<p><br /></p> \r\n<p>Have fun!</p> ', 'RMI,JMS,Java,Releases', 'false', 'true', 1, 1),
(7, 'rmi-meets-jms-1-0-1-released', 1408898000, 'RMI meets JMS 1.0.1 Released!', '<p>New version of RMI meets JMS released! <br /></p>', '<p>The last release has a version number 1.0.1.</p> \r\n<p>It improves the library in meaning of <strong>performance </strong>(optimized process of serialization) and adds additional <strong>logging </strong>of error statuses.</p> \r\n<p> </p> \r\n<p> <br /></p>\r\n<p>Use and gain!</p> \r\n<p> <br /></p>', 'RMI,JMS,Java,Releases', 'false', 'true', 1, 1),
(8, 'gradle-build-from-an-ant-script', 1415637000, 'Gradle Build from an Ant Script', '<p>Currently I am working on a redesign of a pretty complex project based on Ant builds. The task is to move the whole concept into Gradle.</p> ', '<p>I will continuously update this article how my work will be moving forward and put interesting issues and my solutions for them.</p> \r\n<p>If you have a better solution, please comment and discuss!</p>\r\n\r\n<h2>Does a file exist?</h2> \r\n<p>In the Ant script I have found such a construct to check if a file exists:</p> \r\n<p> </p> \r\n<pre class=\"brush: xml\">&lt;pathconvert property=\"MyFile.isPresent\" setonempty=\"false\"&gt;\r\n    &lt;path&gt;\r\n&nbsp;&nbsp;&nbsp;     &lt;fileset dir=\"src\" includes=\"MyFile-*.dat\" /&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/path&gt;\r\n&lt;/pathconvert&gt;\r\n</pre> \r\n<p>Obviously it checks the folder <em>src</em> for files with names of the filter <em>MyFile-*.dat</em>.</p> \r\n<p>We can do the same with Gradle\'s <code>FileCollection</code>:</p> \r\n<p> </p> \r\n<pre class=\"brush: groovy\">MyFile.isPresent = !fileTree(\"src\").include(\'MyFile-*.dat\').isEmpty();</pre> \r\n<p>\r\nThis will create a file filter on the folder and check if the match is empty, then set to a variable.</p> \r\n<h2>Substring </h2> \r\n<p>In the Ant script the macro was defined as a javascript:</p> \r\n<pre class=\"brush: xml\">&lt;scriptdef name=\"substring\" language=\"javascript\"&gt;</pre> \r\n<p>With Gradle you can easily implement it in java:</p> \r\n<pre class=\"brush: groovy\">def substring(String text, String regexp, int result) {\r\n&nbsp;&nbsp;&nbsp; java.util.regex.Pattern p = java.util.regex.Pattern.compile(regexp);\r\n&nbsp;&nbsp;&nbsp; java.util.regex.Matcher m = p.matcher(text);\r\n&nbsp;&nbsp;&nbsp; if (m.find( )) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; return m.group(result + 1);\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; return null;\r\n}</pre> \r\n<p>and use it:</p> \r\n<pre class=\"brush: groovy\">def version = substring(files.getAsPath(), \"executor-(.*)\\\\.msi\", 0)</pre> \r\n<p> </p> \r\n<h2> Custom builds</h2> \r\n<p>Because the result from the new Gradle build must be the same as from the old Ant build <strong>without changes of the project structure</strong>, we need to customize standard <code>java </code>plugin builds a bit.</p> \r\n<h3>Source sets</h3> \r\n<p>Java plugin provides a pretty high-level configuration element called <code>sourceSets</code>. You can change the sources location by setting the <code>sourceSets</code> up:</p> \r\n<pre class=\"brush: groovy\">sourceSets {\r\n&nbsp;&nbsp;&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; java {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; srcDirs = [\"mysource/mypkg\"]\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; exclude \"test/**\"\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n}</pre> \r\n<h3>Destination of the result classes</h3> \r\n<p>Compilation of the java classes in the <a target=\"_blank\" href=\"http://www.gradle.org/docs/current/userguide/java_plugin.html\"><em>java plugin</em></a> is done by the target called <code>compileJava</code>. If you want to change the destination of the compilation, can do it easily by rewriting the variable <code>destinationDir</code>:</p> \r\n<pre class=\"brush: groovy\">compileJava {\r\n&nbsp;&nbsp;&nbsp; destinationDir = file(\"myBuildDir\")&nbsp;&nbsp;&nbsp; // change the default dir for classes\r\n}\r\n</pre> \r\n<h3>Additional clean</h3> \r\n<p> If you want to put some additional actions into the standard clean task, do it easily:</p> \r\n<pre class=\"brush: groovy\">clean &lt;&lt; {\r\n&nbsp;&nbsp;&nbsp; // ... do some clean up\r\n}\r\n</pre> \r\n<p> </p> \r\n<h3>Compile task</h3> \r\n<p> In the old Ant script I have a compile task I need to keep. To make it work correctly together with the standard <code>java</code> plugin, make it dependent on the <code>classes</code> task, which is equivalent in the context:</p> \r\n<pre class=\"brush: groovy\">task compile(dependsOn: classes) {&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // depends on the standard java task \"classes\"\r\n}\r\n</pre> \r\n<h3>Another build</h3> \r\n<p> If you want to have a build of different sources in the same Gradle file (of course in a different task), you can do it like this:</p> \r\n<pre class=\"brush: groovy\">sourceSets {\r\n&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; ...\r\n&nbsp; }\r\n&nbsp; compileApp2 {\r\n&nbsp;&nbsp;&nbsp; java {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDirs = [\"anothersource/mypkg2\"]\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"app/**\"\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}\r\n...\r\ntask compileApp2(type: JavaCompile, dependsOn: prepareApp2) {\r\n&nbsp;&nbsp;&nbsp; source = sourceSets.compileApp2.allSource.srcDirs\r\n&nbsp;&nbsp;&nbsp; destinationDir = file(\'app2BuildDir\')&nbsp;&nbsp;&nbsp; \r\n&nbsp; &nbsp; classpath = configurations.compile&nbsp; \r\n}\r\n</pre> \r\n<br>', 'Gradle,Ant', 'false', 'false', 1, 1),
(9, 'memory-examiner', 1420478000, 'Memory Examiner', '<p>When you need to learn something by hear, you must repeat it again and again... and it is hard.</p> \r\n<p>This little program could help you!</p> \r\n<p> It is a simple test application for pairs of term - explanation, it could be used for technical definitions as well as words of a foreign language.<br /></p>', '<h2>Fill up the lexicon</h2> \r\n<p>First we need to define the lexicon of pairs term-explanation from which the application shall test us.</p> \r\n<p> We can do it in two easy ways:</p> \r\n<ol> \r\n<li>by the form in the bottom part of the application frame,</li> \r\n<li>putting the pairs as <strong>two lines of a text file</strong> (first line the term, second the explanation and so on) and import them via the application menu.</li> \r\n</ol> \r\n<p>The lexicon can be as well exported into a text file via the application menu, this will create two lines of text for each term-explanation pair. The file can be then edited and imported into the application and vise versa.</p> \r\n<h2>Test your memory!</h2> \r\n<p>The application will show you first the term and let you think. You can let show the explanation. Then you can move forward by clicking the green or red button if you had known the term or not.</p> \r\n<p>The application will server the most difficult words with priority. </p> \r\n<h3>Structure your tests</h3> \r\n<p>The application uses the file named <strong><code>lexicon.dat</code></strong> in the working directory as the storage. You can backup your lexicons in files and just rename the file with the demanding lexicon to <code>lexicon.dat</code> to push it into the application (the application has to be restarted to load the new lexicon file).</p> \r\n<p>Or you can use export - import functions to achieve this. But be aware that doing this you will loose you success-fail results!</p> \r\n<h2>Get it</h2> \r\n<p>You can download the <a href=\"/storage/memory-examiner-1.0.jar\">executable JAR archive</a> and run it by the command (if your OS is not configured to run JAR files on the JRE implicitly):</p> \r\n<p><code>java -jar memory-examiner-1.0.jar</code></p> \r\n<p> <br /></p> \r\n<p>And because this is a blog about programming, you can see the <a href=\"/storage/memory-examiner-1.0-sources.jar\">source codes</a> as well.</p>', 'Learning', 'false', 'true', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(10, 'managing-non-java-resources-from-gradle', 1424198000, 'Managing Non-Java Resources from Gradle', '<p>There are really complex projects which consist of a lot of heterogeneous parts, modules, batch scripts, binaries...</p> ', '<p>Let\'s consider a project containing a module written in C... How to proceed in this case when we want to manage everything by the same way, compile from Gradle and version in a Maven repository?</p> \r\n<p>This article describes an easy way how to achieve this tough DevOps goal.</p><h2>Building non-java resources with Gradle <br /></h2> \r\n<p>For building non-java sources there is a package of plugins for dealing with native binaries, this package is currently under incubating but seems to be pretty usable so far.</p> \r\n<p>Details can be found in the <a target=\"_blank\" href=\"https://gradle.org/docs/current/userguide/nativeBinaries.html\" title=\"Building native binaries\">Building native binaries</a> chapter of the official documentation. <a href=\"https://gradle.org/docs/current/userguide/nativeBinaries.html\" title=\"Building native binaries\"><br /></a></p> \r\n<p>There is a support for several languages like C, C++, Assembly, Windows resources, ...</p> \r\n<p>We will focus on C here, as the language differences are not the point of our issue.</p> \r\n<h3>C builds with Gradle</h3> \r\n<p>Applying the &quot;c&quot; Gradle plugin with a simple setting of source codes located in <code>src/main/c</code> and <code>src/main/c/headers</code> in the project folder:</p> \r\n<pre class=\"brush: groovy\">apply plugin: \'c\'\r\n\r\nexecutables {\r\n&nbsp; main { }\r\n}\r\n\r\nsources {\r\n&nbsp; main {\r\n&nbsp;&nbsp;&nbsp; c {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; source { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDir \"src/main/c\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"**/*.c\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exportedHeaders { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srcDir \"src/main/c/headers\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; include \"**/*.h\" \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}</pre> \r\n<p> This script will simply compile the sources in the source path and put the result (.exe file) into <code>build/binaries/mainExecutable</code> folder.</p> \r\n<p>More details can be found in the documentation (mentioned above).</p> \r\n<h2>Publish non-java resources into a Maven2 repository</h2> \r\n<p>For publishing we need to have an access to a Maven2 repository, for instance <a href=\"http://www.sonatype.org/nexus\" target=\"_blank\" title=\"Nexus server\">the Nexus server from Sonatype</a>.</p> \r\n<p>The goal here is to <strong>version the binaries in the repository</strong>, and what\'s more - we would like to make <strong>different variants of binaries</strong> (for instance each variant for a different platform).</p> \r\n<p>Using the Gradle <a title=\"Maven Publishing plugin\" target=\"_blank\" href=\"https://gradle.org/docs/current/userguide/publishing_maven.html\">Maven Publishing plugin</a> is a pretty straightforward way to do this:</p> \r\n<pre class=\"brush: groovy\">apply plugin: \'maven-publish\'\r\n\r\ngroup = \'cz.net21.ttulka\'\r\nversion = \'0.1\'\r\n\r\ntask myZip(type: Zip) { \r\n&nbsp; destinationDir = file(\'dist\') \r\n&nbsp; archiveName \'myC.zip\' \r\n&nbsp; from \'build/binaries/mainExecutable\' \r\n}\r\n\r\npublishing {\r\n&nbsp; publications {\r\n&nbsp;&nbsp;&nbsp; myPublicationName(MavenPublication) {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; artifact (myZip) {\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classifier = \'win32\'\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp; }\r\n}\r\n\r\nrepositories {\r\n&nbsp; maven {\r\n&nbsp;&nbsp;&nbsp; credentials { \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; username \'xxx\' \r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; password \'xyz\' \r\n&nbsp;&nbsp;&nbsp; }\r\n&nbsp;&nbsp;&nbsp; url \'http://localhost:8081/nexus/content/repositories/MyMavenRep1/\'\r\n&nbsp; }\r\n}\r\n</pre> \r\n<p>Don\'t forget to set the project name in the <code>settings.gradle</code> configuration file, for instance:</p> \r\n<pre class=\"brush: groovy\">rootProject.name = \'my-project-with-native-binaries\'</pre> \r\n<p>In the example snippet we have used the <code>Zip </code>Gradle task to archive the binaries into a zip file, then we have used the archive as the input for the Maven publication and finally added the classifier (<code>classifier = \'win32\'</code>) to annotate this build variant as a Win32 platform-oriented. </p> \r\n<h2>Load the binaries as a dependency in an independent project</h2> \r\n<p>As far as we have compiled the sources and put the binaries into the repository, we want to deal with them from another project (or a different module).</p> \r\n<p> To distinguish the binary dependencies from the others, we can define a new Gradle configuration:</p> \r\n<pre class=\"brush: groovy\">configurations {\r\n&nbsp; binaries {}\r\n}</pre> \r\n<p>Dependencies are then marked by this configuration flag, don\'t forget that we used the zip archive and the classifier:</p> \r\n<pre class=\"brush: groovy\">dependencies { \r\n&nbsp; binaries \'cz.net21.ttulka:my-project-with-native-binaries:0.1:win32@zip\' \r\n}</pre> \r\n<p>That\'s it, now we can do whatever we want:</p> \r\n<pre class=\"brush: groovy\">// copy the dependencies into a \'deps\' folder\r\ntask getDeps(type: Copy) { \r\n&nbsp; from configurations.binaries \r\n&nbsp; into \'deps/\' \r\n}</pre> \r\n<p> <br /></p> \r\n<p>Hopefully this article gave you a little insight how to deal with different types of resources in one unified way with Gradle, and help you optimize your DevOps processes!<br /></p> \r\n<p> <br /></p>', 'Gradle', 'false', 'false', 1, 1),
(11, 'xslt-multiple-xml-inputs', 1434551000, 'XSLT: Multiple XML Inputs', '<p>How to create one result document from more sources? It\'s easy with XSLT!</p>', '<p>Let\'s imagine, that we have two (or more) XML documents as the data sources. In our example we will use a list of products as the first document and a list of attributes (colors, in this case) as the second document. Each product as a ID of a color as its attribute, the name of the color is in the list of colors. We would like to show all the products with their color names in a nice HTML table.</p> \r\n<h2>Data sources </h2> \r\n<p><em><strong>products.xml</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;products&gt;\r\n&nbsp; &lt;product id=\"1001\" color=\"A1\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Strawberry&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1002\" color=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Blueberry&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1003\" color=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Plum&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&nbsp; &lt;product id=\"1004\" color=\"A3\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Green Apple&lt;/name&gt;\r\n&nbsp; &lt;/product&gt;\r\n&lt;/products&gt;</pre> \r\n<p><em><strong>colors.xml</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;colors&gt;\r\n&nbsp; &lt;color id=\"A1\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Red&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&nbsp; &lt;color id=\"A2\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Blue&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&nbsp; &lt;color id=\"A3\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;name&gt;Green&lt;/name&gt;\r\n&nbsp; &lt;/color&gt;\r\n&lt;/colors&gt;</pre> \r\n<h2>XSLT Transformation</h2> \r\n<p><em><strong>transformation.xsl</strong></em></p> \r\n<pre class=\"brush: xml\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" exclude-result-prefixes=\"xsl\"&gt;&nbsp; \r\n \r\n&nbsp; &lt;xsl:param name=\"colorsFile\"/&gt;\r\n&nbsp; &lt;xsl:variable name=\"colorsDoc\" select=\"document($colorsFile)\"/&gt;\r\n \r\n&nbsp; &lt;xsl:template match=\"/\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;html&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;body&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;table border=\"1\" cellpadding=\"10\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;tr bgcolor=\"#accfff\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;th&gt;Name&lt;/th&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;th&gt;Color&lt;/th&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/tr&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:for-each select=\"products\"&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:apply-templates select=\"product\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/xsl:for-each&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \r\n&nbsp;&nbsp;&nbsp; &lt;/table&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/body&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/html&gt;\r\n&nbsp; &lt;/xsl:template&gt;\r\n \r\n&nbsp; &lt;xsl:template match=\"product\"&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;tr&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;td&gt;&lt;xsl:value-of select=\"name\"/&gt;&lt;/td&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;td&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:variable name=\"colorId\" select=\"@color\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;xsl:value-of select=\"$colorsDoc/colors/color[@id=$colorId]/name\"/&gt;\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/td&gt;\r\n&nbsp;&nbsp;&nbsp; &lt;/tr&gt;\r\n&nbsp; &lt;/xsl:template&gt;\r\n \r\n&lt;/xsl:stylesheet&gt;</pre> \r\n<p>The parameter <code>colorsFile </code>takes its value from the processor (below), could be set to a file path like:</p> \r\n<pre class=\"brush: xml\">&lt;xsl:param name=\"colorsFile\" select=\"\'../input/colors.xml\'\" /&gt;</pre> \r\n<h2>Java processor</h2> \r\n<p>In this article we\'re using a Java processor, the solution can be but very easily converted to the pure XSLT solution and another processor (for instance an internet browser) can be used.</p> \r\n<p>All the classes are from the sub-packages of the package <code>javax.xml</code>. </p> \r\n<pre class=\"brush: java\">final DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\r\n\r\nfinal DocumentBuilder builder = factory.newDocumentBuilder();\r\nfinal Document document = builder.parse(\"products.xml\");\r\n\r\nfinal TransformerFactory tFactory = TransformerFactory.newInstance();\r\nfinal Transformer transformer = tFactory.newTransformer(new StreamSource(\"transformation.xsl\"));\r\n \r\ntransformer.setParameter(\"colorsFile\", \"colors.xml\");\r\n\r\ntransformer.transform(new DOMSource(document), new StreamResult(\"output.html\"));</pre> \r\n<p>And that\'s exactly what we wanted!</p> \r\n<p><strong><em>output.html</em></strong></p> \r\n<p> \r\n<table cellpadding=\"3\" border=\"1\"> \r\n<tbody> \r\n<tr bgcolor=\"#accfff\"> \r\n<th>Name</th> \r\n<th>Color</th> \r\n</tr> \r\n<tr> \r\n<td>Strawberry</td> \r\n<td>Red</td> \r\n</tr> \r\n<tr> \r\n<td>Blueberry</td> \r\n<td>Blue</td> \r\n</tr> \r\n<tr> \r\n<td>Plum</td> \r\n<td>Blue</td> \r\n</tr> \r\n<tr> \r\n<td>Green Apple</td> \r\n<td>Green</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> ', 'XML,XSLT', 'false', 'false', 1, 1),
(12, 'sql-null-tricky-equality', 1447269000, 'SQL NULL: tricky equality', '<p>Don\'t get confused with NULL in SQL!</p>', '<p>Especially when creating a SQL quary from the code, to make the life easier we are using constructions like this:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE 1=1</pre> \r\n<p>Then we can easily add a new condition just joined with AND (or OR):</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE 1=1 AND ...</pre> \r\n<p>and it will be working perfectly fine.</p> \r\n<p>Working with constans can\'t bring any problem, but it\'s getting tricky when we\'re working with variables (columns) like this:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num=num</pre> \r\n<p>The problem is the special <em>value</em> NULL. <br /></p>\r\n<p>Why do we actually need <code>num = num</code> instead of <code>1 = 1</code>? Well it could be handy when we have a condition inside the query:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE val =&nbsp; IF(num &gt; 2, 3, val)</pre> \r\n<p>By this query we say: give me all the data in the <code>test</code> table where the value of the <code>val</code> column equals 3, in case that value of the <code>num</code> column is greater then 2, otherwise we don\'t care of the value of the <code>val</code> at all.</p> \r\n<p><em>Comment: the control flow function <code>IF</code> is RDBMS-related (<code>IF</code> is available in MySQL).</em></p> \r\n<p>In case when the val column contains NULL values, we can get unexpected behaviour (unexpected for us, but logic as we will see next).</p> \r\n<h2>Working with the NULL values</h2> \r\n<p>Let\'s consider a simple table:</p> \r\n<pre class=\"brush: sql\">CREATE TABLE test (num INT, str VARCHAR(100), str2 VARCHAR(100));</pre> \r\n<p>with different values containg NULL values:</p> \r\n<pre class=\"brush: sql\">INSERT INTO test VALUES (1, \'abc\', \'abc\'), (2, NULL, NULL), (NULL, \'3\', NULL), (NULL, NULL, NULL);</pre> \r\n<p>The table looks like following:</p> \r\n<p> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n<td style=\"width: 33%;\">&nbsp;3</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL <br /></td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n</tbody> \r\n</table><br /> \r\n</p> \r\n<p>Executing this query:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num = num;</pre> \r\n<p>will return</p> \r\n<p> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">abc <br /></td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n<td style=\"width: 33%;\">&nbsp;NULL</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p> \r\n<p>But it\'s very suspicious, because we have added four not two rows.</p> \r\n<p>NULL values can\'t be compared by the <code>=</code> equality operator. The same behaviour we can see in this example:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE str = str2;</pre> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">abc <br /></td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p>Instead of <code>=</code> operator we need to use <code>IS</code> operator:</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE num IS num;</pre> \r\n<p>will return the whole table.</p> \r\n<pre class=\"brush: sql\">SELECT &#42; FROM test WHERE str IS str2;</pre> \r\n<table cellspacing=\"1\" cellpadding=\"1\" border=\"1\" style=\"width: 300px;\"> \r\n<tbody> \r\n<tr> \r\n<td valign=\"top\"><strong>num<br /></strong></td> \r\n<td valign=\"top\"><strong>str<br /></strong></td> \r\n<td valign=\"top\"><strong>str2<br /></strong></td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;1</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n<td style=\"width: 33%;\">&nbsp;abc</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;2</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n<tr> \r\n<td style=\"width: 33%;\">&nbsp;NULL <br /></td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n<td style=\"width: 33%;\">  NULL</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p>&nbsp;</p> \r\n<p>NULL is an unknown value. As <code>NULL != NULL</code>, we can\'t do <code>num = NULL</code> nor <code>num != NULL</code> to get the expected results.</p> \r\n<p><strong>Always use <code>IS</code> (<code>IS NOT</code>) operators while working with NULL values.</strong><br /></p> ', 'Database,SQL', 'false', 'false', 1, 1),
(13, 'javafx-2-simple-graphs', 1452103000, 'JavaFX 2: Simple Graphs', '<p>It\'s pretty easy to create a graphs in the JavaFX application, because there are a lot of neat libraries on the Internet. Trouble comes when you need to stick with JavaFX 2.</p> \r\n', '<p> This very very simple library will let you to create simple graphs like this one:</p> \r\n<p><img alt=\"Simple Graphs - preview\" src=\"/storage/simple-graphs-preview.png\" /><br /></p>\r\n<p>Let\'s consider a simple JavaFX 2 application:</p> \r\n<pre class=\"brush: java\">public class HelloWorld extends Application {\r\n\r\n&nbsp;&nbsp;&nbsp; private void init(Stage primaryStage) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; // TODO\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; @Override\r\n&nbsp;&nbsp;&nbsp; public void start(Stage primaryStage) throws Exception {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; init(primaryStage);\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; primaryStage.show();\r\n&nbsp;&nbsp;&nbsp; }\r\n\r\n&nbsp;&nbsp;&nbsp; public static void main(String[] args) {\r\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; launch(args);\r\n&nbsp;&nbsp;&nbsp; }\r\n} </pre> \r\n<p>We will fill the init method to set up the scene showing the graph nodes from the picture above.</p> \r\n<p>First, we need the parent node:</p> \r\n<p> </p> \r\n<pre class=\"brush: java\">final BoxNode parent = new BoxNode(\"Parent\", 250, 0);</pre> \r\n<p> </p> \r\n<p>Then the children nodes:</p> \r\n<pre class=\"brush: java\">final BoxNode myNode1 = new BoxNode(\"MyNode 1\", 0, 200);\r\nfinal BoxNode myNode2 = new BoxNode(\"MyNode 2\", 300, 400);</pre> \r\n<p>Second, we connect the nodes with each other (the parent to children, the first child to the second one and also with itself):</p> \r\n<pre class=\"brush: java\">parent.addArrowTo(myNode1, \"arrow 1\");\r\nparent.addArrowTo(myNode2, \"arrow 2\");\r\nmyNode1.addArrowTo(myNode2, \"arrow 3\");\r\nmyNode1.addArrowTo(myNode1, \"arrow round\");<p></p></pre> \r\n<p>Finally, we all the nodes to a group (<code>javafx.scene.Group</code>) and the group to the scene:</p> \r\n<pre class=\"brush: java\">final Group root = new Group();\r\nprimaryStage.setScene(new Scene(root));\r\nroot.getChildren().addAll(parent, myNode1, myNode2);</pre> \r\n<p>That\'s all!</p> \r\n<p>The library is available as a <a href=\"/storage/simple-graphs-1.0.0.jar\">JAR</a> or <a href=\"https://github.com/ttulka/javafx2-simple-graphs\">source archive</a>.</p> ', 'Java,Java FX', 'false', 'true', 1, 1),
(14, 'javascript-menu-plugin-for-mobile-web-applications', 1454521000, 'JavaScript Menu Plugin for Mobile Web Applications', '<p>It\'s not easy to put all the navigation information in the small space of a website for mobile apps, especially when you have a lot of categories, departments etc.</p> \r\n', '<p>This JavaScript (<a href=\"https://jquery.com/download\" target=\"_blank\">jQuery</a>) based dynamic menu could help you with the challenge.</p> \r\n<p> It\'s small, compact, customizable, easy to deploy and free to use! <br /></p>\r\n<h2>Live Example</h2> \r\n<p>Example page optimized for the mobile devices with the menu is <a target=\"_blank\" href=\"http://www.net21.cz/mobile-menu\">here</a>.</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" style=\"width: 100%;\"> \r\n<tbody> \r\n<tr> \r\n<td style=\"width: 33%;\"> <img border=\"1\" alt=\"Menu for Mobile Web Apps #1\" src=\"/storage/mobilMenu_screenshot_1.png\" /></td> \r\n<td style=\"width: 33%;\"> <img border=\"1\" alt=\"Menu for Mobile Web Apps #2\" src=\"/storage/mobilMenu_screenshot_2.png\" /></td> \r\n</tr> \r\n<tr> \r\n<td valign=\"top\"><br /></td> \r\n<td valign=\"top\"><br /></td> \r\n</tr> \r\n</tbody> \r\n</table><img border=\"1\" src=\"/storage/mobilMenu_screenshot_3.png\" alt=\"Menu for Mobile Web Apps #3\" /><br /> \r\n</p> \r\n<h2>Get started</h2> \r\n<p>Dowload the <a href=\"http://www.net21.cz/mobile-menu/mobileMenu.js\" target=\"_blank\">Menu jQuery Plugin</a> (<a href=\"http://www.net21.cz/mobile-menu/mobileMenu.min.js\" target=\"_blank\">compressed</a>) or the <a href=\"http://www.net21.cz/mobile-menu/mobileMenu.zip\">this whole example</a>.</p> \r\n<p>To deploy the menu on your website you need to include jQuery and the plugin scripts:</p> \r\n<pre class=\"brush: javascript\">&lt;script type=\"text/javascript\" src=\"http://code.jquery.com/jquery-1.12.0.min.js\"&gt;&lt;/script&gt;\r\n&lt;script type=\"text/javascript\" src=\"mobileMenu.min.js\"&gt;&lt;/script&gt;</pre> \r\n<p>Then to connect a start point on the page (typically a button or a text block) with the plugin:</p> \r\n<pre class=\"brush: javascript\">&lt;script type=\"text/javascript\"&gt;\r\n$(document).ready(function() {\r\n&nbsp; $(\"#mobileMenuStartButton\").mobileMenu(\"url/to/data.json\");\r\n});\r\n&lt;/script&gt;</pre> \r\n<h3>Data structure <br /></h3> \r\n<p>As the first parameter of the <code>mobileMenu</code> plugin you have to set an URL to a JSON file containing the menu data in the structure like following:</p> \r\n<pre class=\"brush: json\">{\r\n&nbsp; \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.1\", \"link\" : \"link1.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2.1\", \"link\" : \"link1.2.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.2.2\", \"link\" : \"link1.2.2\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 1.3\", \"link\" : \"link1.3\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 2\", \"link\" : \"link2\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3\", \"menu\" : [\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3.1\", \"link\" : \"link3.1\"},\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {\"name\" : \"Text 3.2\", \"link\" : \"link3.2\"}\r\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]} \r\n&nbsp; ]\r\n}</pre> \r\n<p>As showed in the example data above, the structure starts with the <code>menu</code> record with a value of a list of menu items. Every menu item has a name and a link or a sub-menu of the same structure recursively.</p> \r\n<h3>Custom styling</h3> \r\n<p>The menu component contains several CSS classes which can be styled by the user.</p> \r\n<p> The easiest way is to download the <a href=\"view-source:http://www.net21.cz/mobile-menu/mobileMenu.css\" target=\"_blank\">example CSS style sheet</a> and adjust it.</p> \r\n<p>The important CSS are following:</p> \r\n<ul> \r\n<li><strong><code>mobileMenuButton </code></strong><br />The starter element will get this class additionaly. Together with the class <code>expanded</code> indicates the opened menu.<br /> <br /></li> \r\n<li><strong><code>mobileMenuMain</code></strong><br />The wrapper for the whole menu context. It\'s important to set atributte <code>position:absolute</code> for this to make the menu overflowing the context of the page below.<br /> <br /></li> \r\n<li><strong><code>mobileMenuBlock</code></strong><br />The container for the menu items context.<br /> <br /></li> \r\n<li><strong><code>mobileMenuItem</code></strong><br />The class for the menu items, has following additional classes:<br /> <br /></li> \r\n<ul> \r\n<li><code>title</code><br />The menu item showing the name of the currently expanded class.<br /> <br /></li> \r\n<li><code>first</code><br />The first item in the first menu list (without title item).<br /> <br /></li> \r\n<li><code>link</code><br />Indicates that the menu item has no sub-menu but it\'s only a simple link.<br /></li> \r\n</ul> \r\n</ul> \r\n<p> <br /></p>', 'JavaScript,Releases', 'false', 'true', 1, 2),
(15, 'upgrade-java-se-7-to-java-se-8-ocp-programmer-summary', 1468344000, 'Upgrade Java SE 7 to Java SE 8 OCP Programmer - Summary', '<p>Maybe you are updating your Oracle Professional Certification from Java 7 to Java 8 as I just did a few days ago. \r\n This summary contains everything important to learn to achieve this goal.</p> \r\n', '<p>You can <a href=\"/storage/Upgrade_Java_SE_7_to_Java_SE_8_OCP_Programmer_-_Summary.pdf\">download this summary</a> as a PDF document.</p> \r\n<p>I wish you good luck!</p>\r\n<p>Of course that this summary it\'s not enought, if you are beginning to learn all the new stuff in the Java 8.</p> \r\n<p>For deeper study I can recoment you a few things:</p> \r\n<ul> \r\n<li> OCP: Oracle Certified Professional Java SE 8 Programmer II Study Guide: Exam 1Z0-809<br /><a target=\"_blank\" href=\"https://www.amazon.de/OCP-Certified-Professional-Programmer-1Z0-809/dp/1119067901\">https://www.amazon.de/OCP-Certified-Professional-Programmer-1Z0-809/dp/1119067901<br /></a> <br /></li> \r\n<li>Mock Exams from Enthuware<br /><a target=\"_blank\" href=\"http://enthuware.com/index.php/mock-exams/oracle-certified-professional/ocpjp-8-1z0-810-questions\">http://enthuware.com/index.php/mock-exams/oracle-certified-professional/ocpjp-8-1z0-810-questions<br /></a><br /></li> \r\n<li>Pu-erh Dark Tea - good for the memory and whole body fitness<br /><a href=\"https://en.wikipedia.org/wiki/Pu-erh_tea\">https://en.wikipedia.org/wiki/Pu-erh_tea</a> <br /></li> \r\n</ul> \r\n<p> </p> \r\n<p><br />If you find some mistakes or have some comments to the summary, please feel free to discuss!</p> \r\n', 'Learning,Certification,Java', 'false', 'false', 1, 2),
(16, 'pre-processing-spring-beans-of-prototype-scope', 1470586000, 'Pre-processing Spring Beans of Prototype Scope', '<p>Sometimes you have a type of bean in the Spring framework which you want to create by every asking for it - in the scope prototype.</p> \r\n', '<p>Pre-processing of those <strong>prototypes</strong> only one and in the beginning &nbsp;could be tricky.</p> \r\n<p>Let\'s consider an annotation which tell you someting about the setting of the beans. For example which alternative names is your GUI box connected to. You want to create a proper bean based on its alternative name of course without featching all the beans every time and looking for the right one (of course you can do it by lazy caching, but that\'s still not the best solution). Or what if you want to get a list of all the alternative names of course <u>withou the need to create the beans</u> first?</p> \r\n<p>This short article will tell you how to do it.</p>\r\n<p>Consider the following annotations:</p> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Documented\r\n@Component\r\n@Scope(value = SCOPE_PROTOTYPE)\r\npublic @interface Box {\r\n}</pre> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Documented\r\n@Component\r\n@Scope(value = SCOPE_PROTOTYPE)\r\npublic @interface AlternativeNames {\r\n&nbsp; &nbsp; AlternativeName[] value();\r\n}</pre> \r\n<pre class=\"brush: java\">@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Repeatable(AlternativeNames.class)\r\n@Documented\r\npublic @interface AlternativeName {\r\n&nbsp; &nbsp; String value();\r\n}</pre> \r\n<p>Then you have defined the base annotation <code>@Box </code>which tell the Spring framework that this class is a bean, <code>@AlternativeNames</code> which allows you to put the <code>@AlternativeName</code> annotation more times (<em>Repeating Annotations</em>).</p> \r\n<p>Based on the definitions you can create a class like this:</p> \r\n<pre class=\"brush: java\">@Box\r\n@AlternativeName(\"my-box\")\r\n@AlternativeName(\"box-of-mine\")\r\npublic class MyBox {\r\n&nbsp; &nbsp; // ... some code goes here\r\n}</pre> \r\n<p>Now it\'s time for the pre-processor:</p> \r\n<pre class=\"brush: java\">@Component\r\npublic class BoxContextListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; {\r\n\r\n&nbsp; &nbsp; private final ConfigurableListableBeanFactory factory;\r\n\r\n&nbsp; &nbsp; @Autowired\r\n&nbsp; &nbsp; public BoxContextListener(ConfigurableListableBeanFactory factory) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; this.factory = factory;\r\n&nbsp; &nbsp; }\r\n\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void onApplicationEvent(ContextRefreshedEvent event) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; String[] beanDefinitionNames = event.getApplicationContext().getBeanDefinitionNames();\r\n&nbsp; &nbsp; &nbsp; &nbsp; for (String beanDefinitionName : beanDefinitionNames) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; BeanDefinition beanDefinition = factory.getBeanDefinition(beanDefinitionName);\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String originalClassName = beanDefinition.getBeanClassName();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClassName != null &amp;&amp; !originalClassName.isEmpty()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; final Class&lt;?&gt; originalClass = Class.forName(originalClassName);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClass.isAnnotationPresent(Box.class)) {\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (originalClass.isAnnotationPresent(AlternativeNames.class) \r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; || originalClass.isAnnotationPresent(AlternativeName.class)) { \r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; AlternativeName[] mappings = originalClass.getAnnotationsByType(AlternativeName.class);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (AlternativeName alternativeMapping : mappings) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String alternativeName = alternativeMapping.value();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // do someting with the alternative name ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (ClassNotFoundException e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (BeansException e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // ...\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>That\'s the way how to get values from the annotation of the Spring prototype beans in the pre-processing.</p> \r\n<p>I hope it helped!</p>', 'Programming,Java,Spring', 'false', 'false', 1, 1),
(17, 'resolving-a-generic-type-with-the-spring-framework', 1472312000, 'Resolving a Generic Type with Spring Framework', '<p>In the case of generic beans sometimes you need to get the generic type value for some specific reasons. A typical example could be parsing some data into the type.</p>', '<p>Following code shows you how to parse a JSON data into the specific structure defined by a generic class.</p>\r\n\r\n<pre class=\"brush: java\">\r\nimport com.fasterxml.jackson.databind.ObjectMapper;\r\nimport org.springframework.core.GenericTypeResolver;\r\n// ...\r\n\r\n@Component \r\npublic class MyBean&lt;T extends MySuperBean&gt; {\r\n\r\n  private final Class&lt;T&gt; genericType;\r\n\r\n  private final ObjectMapper mapper = new ObjectMapper();\r\n\r\n  public MyBean() {\r\n    this.genericType = (Class&lt;T&gt;) GenericTypeResolver\r\n        .resolveTypeArgument(getClass(), MyBean.class);\r\n  }\r\n\r\n  public T parseData(String json) {\r\n    return mapper.readValue(json, this.genericType);\r\n  }\r\n}\r\n</pre> \r\n\r\n<p>Spring tool <code>GenericTypeResolver</code> will set the <code>genericType</code> class value which is later used as the parameter for the JSON parsing.</span></p> \r\n<p>Happy resolving!</p> \r\n', 'Programming,Java,Spring', 'false', 'false', 1, 1),
(18, 'ant-script-to-copy-a-snippet-of-a-xml-to-another-xml', 1475855000, 'Ant Script to Copy a Snippet of a XML to Another XML', '<p>I know, Ant is not the most modern technology, but there are still Ant-based systems we have to maintenance.</p> \r\n', '<p>Sometimes we need to cut or copy a piece of a XML document and save it to another.</p> \r\n<p>We will use <a href=\"http://www.oopsconsultancy.com/software/xmltask/\" target=\"_blank\">XmlTask</a> library by OOPS Consultancy to achive this goal.&nbsp;</p>\r\n<p>First of all, we have to define an Ant task:</p> \r\n<p>&lt;taskdef name=&quot;xmltask&quot; classname=&quot;com.oopsconsultancy.xmltask.ant.XmlTask&quot; /&gt;</p> \r\n<p>Don\'t forget to put the downloaded library into the Ant libs folder or onto the classpath:</p> \r\n<p><code>ant ... -lib xmltask.jar</code></p> \r\n<p>Let\'s consider two XML configuration files:</p> \r\n<p><code><strong>conf.xml</strong></code></p> \r\n<pre class=\"brush: xml\">&lt;configuration&gt;\r\n&nbsp; &lt;applications&gt;\r\n&nbsp; &nbsp; &lt;app id=\"MyCoolApp\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"AnotherApp\" /&gt;\r\n&nbsp; &lt;/applications&gt;\r\n&lt;/configuration&gt;&nbsp;</pre> \r\n<p><code><strong>settings.xml</strong></code></p> \r\n<pre class=\"brush: xml\">&lt;settings&gt;\r\n...\r\n&nbsp; &lt;apps&gt;\r\n&nbsp; &nbsp; &lt;app id=\"thisAppWasAlreadyHere\" /&gt;\r\n&nbsp; &lt;/apps&gt;\r\n&lt;/settings&gt;</pre> \r\n<p>And we want to merge those documents in the way we get this content of the <code>settings.xml</code>:</p> \r\n<pre class=\"brush: xml\">&lt;settings&gt;\r\n&nbsp; ...\r\n&nbsp; &lt;apps&gt;\r\n&nbsp; &nbsp; &lt;app id=\"thisAppWasAlreadyHere\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"MyCoolApp\" /&gt;\r\n&nbsp; &nbsp; &lt;app id=\"AnotherApp\" /&gt;\r\n&nbsp; &lt;/apps&gt;\r\n&lt;/settings&gt;</pre> \r\n<p>We can use the xmltask\'s command call to get a path-defined snippet into the buffer and append it with the command insert.</p> \r\n<p>Here is the whole code:&nbsp;</p> \r\n<pre class=\"brush: xml\">&lt;xmltask source=\"conf.xml\"&gt;\r\n&nbsp; &lt;call path=\"configuration/applications/app\" buffer=\"apps_storedXml\"&gt;\r\n&nbsp; &nbsp; &lt;param name=\"id\" path=\"@id\" /&gt;\r\n&nbsp; &nbsp; &lt;actions&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;echo&gt;Merge the app id=\"@{id}\"&lt;/echo&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;xmltask source=\"settings.xml\" dest=\"settings.xml\"&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;insert path=\"settings/apps\" buffer=\"apps_storedXml\" /&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/xmltask&gt;\r\n&nbsp; &nbsp; &lt;/actions&gt;&nbsp;\r\n&nbsp; &lt;/call&gt;\r\n&lt;/xmltask&gt;</pre> \r\n<p>That\'s it!</p> \r\n<p> </p>', 'Programming,Ant', 'false', 'false', 1, 1),
(19, 'ant-sequential-tasks', 1479229000, 'Ant Sequential Tasks', '<p>This article will show you how to implement an Ant task which consume a sequence of sub-task and call them with a parameter of the result from the execution.</p> \r\n', '<p>As an example we can consider a task <strong>taking a path to a directory</strong> as a parameter, <strong>fetching files</strong> in the directory and <strong>running a sequence of sub-task for each file</strong>.&nbsp;</p> \r\n<p>The task will be implemented as a <strong>Java library</strong>.</p>\r\n\r\n<p>Considering the example, we will have in our Ant script something like:</p> \r\n<pre>&lt;fetch-files path=\"/Windows\" suffix=\".exe\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;echo&gt;My file print: @{file}&lt;/echo&gt;\r\n\r\n        &lt;antcall target=\"process-file\"&gt;\r\n            &lt;param name=\"filename\" value=\"@{file}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/fetch-files&gt;\r\n</pre> \r\n<p>This should process all the <code>.exe</code> files in the <code>/Windows</code> directory and print the name of the file before processing.&nbsp;</p> \r\n<h2>Implementation in Java&nbsp;</h2> \r\n<p>Let\'s create a new Maven project, all we need is a dependency to the Ant API:</p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;org.apache.ant&lt;/groupId&gt;\r\n    &lt;artifactId&gt;ant&lt;/artifactId&gt;\r\n    &lt;version&gt;1.9.7&lt;/version&gt;\r\n&lt;/dependency&gt;\r\n</pre> \r\n<h3>Sequential Task&nbsp;</h3> \r\n<p>First of all we will create an abstract class which allows us to put the <code>&lt;sequential&gt;</code> part into the task:</p> \r\n<pre class=\"brush: java\">abstract class SequentialTask extends Task {\r\n\r\n    private MacroDef macroDef;\r\n    private Target owningTarget;\r\n\r\n    abstract String getAttributeName();\r\n\r\n    @Override\r\n    public void setOwningTarget(Target owningTarget) {\r\n        this.owningTarget = owningTarget;\r\n    }\r\n\r\n    public Object createSequential() {\r\n        macroDef = new MacroDef();\r\n        macroDef.setProject(getProject());\r\n\r\n        MacroDef.Attribute attribute = new MacroDef.Attribute();\r\n        attribute.setName(getAttributeName());\r\n        macroDef.addConfiguredAttribute(attribute);\r\n\r\n        return macroDef.createSequential();\r\n    }\r\n\r\n    void executeSequential(String attrValue) {\r\n        MacroInstance instance = new MacroInstance();\r\n        instance.setProject(getProject());\r\n        instance.setOwningTarget(owningTarget);\r\n        instance.setMacroDef(macroDef);\r\n        instance.setDynamicAttribute(getAttributeName(), attrValue);\r\n        instance.execute();\r\n    }\r\n}</pre> \r\n<p>All the task exending the class must define the declared <code>getAttributeName()</code> method which returns the name of the attribute (<code>&quot;file&quot;</code> in our example).</p> \r\n<p>Then we cann call the method <code>executeSequential(String attrValue)</code> for each item in the task result.</p> \r\n<h3>Fetching Files Task&nbsp;</h3> \r\n<p>Our concrete task implementation could look like:</p> \r\n<pre class=\"brush: java\">public class FetchFilesTask extends SequentialTask {\r\n\r\n    private static final String ATTR_NAME = \"file\";\r\n\r\n    private String path;\r\n    private String suffix;\r\n\r\n    public void setPath(String path) {\r\n        this.path = path;\r\n    }\r\n\r\n    public void setSuffix(String suffix) {\r\n        this.suffix = suffix;\r\n    }\r\n\r\n    @Override\r\n    String getAttributeName() {\r\n        return ATTR_NAME;\r\n    }\r\n\r\n    @Override\r\n    public void execute() { \r\n        if (path == null || path.trim().isEmpty()) {\r\n            throw new BuildException(\"Parameter \'path\' must be specified.\");\r\n        }\r\n\r\n        List&lt;String&gt; filesList = getFilesList(Paths.get(path), suffix);\r\n\r\n        for (String file : filesList) {\r\n            executeSequential(file.toString());\r\n        }\r\n    }\r\n\r\n    List&lt;String&gt; getFilesList(Path sourcePath, String suffix) {\r\n        final List&lt;String&gt; toReturn = new ArrayList&lt;&gt;();\r\n\r\n        try (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(sourcePath)) {\r\n            for (Path file: stream) {\r\n                if (Files.isRegularFile(file)) {\r\n                    if (suffix == null || file.toString().endsWith(suffix)) {\r\n                        toReturn.add(file.toString());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        catch (IOException | DirectoryIteratorException e) {\r\n            throw new BuildException(\"Error by reading \'\" + sourcePath + \"\'.\", e);\r\n        }\r\n        return toReturn;\r\n    }\r\n}</pre> \r\n<p>The class extends the <code>SequentialTask</code>, defines its abstract method and use the <code>executeSequential</code> method. </p> \r\n<p>Additionaly defines the class two task parameter&nbsp;<code>path</code> and&nbsp;<code>suffix</code> by the defining the <em>getters </em>for them.&nbsp;</p> \r\n<h2>Using the task in an Ant script</h2> \r\n<p>To call the task in an Ant script as showned in the example we have to provide the JAR library and define the task by its class:</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"fetch-files\" classname=\"cz.net21.ttulka.ant.FetchFilesTask\" classpath=\"AntTasks-1.0.jar\" /&gt;</pre> \r\n<p> </p> \r\n<p>You can download the <a href=\"/storage/AntTasks-1.0.jar\">compiled JAR library</a> or whole <a href=\"https://github.com/ttulka/ant-sequential-tasks\">project sources</a>.</p> \r\n<p>Have fun!</p>', 'Programming,Java,Ant', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(20, 'application-package-manager-with-ant-and-java', 1484074000, 'Application Package Manager with Ant and Java', '<p>Let\'s build a simple but powerful application package manager with Ant.</p>', '<p>\r\nEveryone knows the operation systems package manager like&nbsp;<em>dpkg </em>(from Debian) or&nbsp;<em>RPM Package Manager</em> (from RedHat).</p> \r\n<p>Sometimes there is a need of such a management system in our own use.</p> \r\n<p>The environment doesn\'t have to be an operation system, but for instance a<strong>&nbsp;web server</strong> or just a <strong>container application</strong>.</p> \r\n<p>The manager must be able to manage package <strong>dependencies</strong>, <strong>versions </strong>and <strong>installation </strong>to the environment system.</p>\r\n\r\n<p>All the information must be contained only inside the application itself, the system nor the manage <strong>must not</strong> have any additional data about a concrete package.</p> \r\n<p>To distinguish between a package and a package manager we will use in our context terms <em>Application Package</em> (<strong>AP</strong>) and <em>Application Package Manager</em> (<strong>APM</strong>).</p> \r\n<h2><em>Scenario One</em>: Container Application</h2> \r\n<p>We want to develop a container application which on the starup checks the application folder and install included application if not installed yet.</p> \r\n<p>Consider an application which simulates an operation system. It has a core, folder for configuration, run and so one. In the simplest case:</p> \r\n<pre>/applications/\r\n/system/\r\n       /conf/\r\n            /applications.conf\r\n       /core/\r\n            /lib/\r\n                /apm.jar\r\n                /system.jar\r\n            /apm.xml\r\n       /data/\r\n       /start.bat</pre> \r\n<p>The file <code>conf/applications.conf</code> contains pairs of <code>application-name=files-in-bin-directory-comma-separated</code>. It\'s empty with a brand new system installation or contains some initial system-provided application and files.</p> \r\n<p>The system calls the APM (<code>core/apm.xml</code>) after the <code>start.bat</code> script is executed. The APM takes care of the applications installation. Then a system program (from the library <code>core/lib/system.jar</code>) goes thru the <code>conf/application.conf</code> and run each application logic (in our case only prints the content of a file defined by the application).</p> \r\n<p>The <code>applications</code> directory stands outside the <code>system</code> directory, it means even when the system is completely updated remains the <code>applications</code> directory untouched and the included application are always ready for a new installation by the next startup.</p> \r\n<h3>AP Implementation</h3> \r\n<p>An AP is nothing more than a folder with a strict defined structure containing all the needed system-related information about itselft. We will put all the AP-related files into the <code>META-INF</code> sub-folder.</p> \r\n<p>Every AP must have a meta information file. We will create a ordinary property file and call it <code>meta.info</code>. The meta information file contains the <em>name</em>, <em>version </em>and <em>dependencies </em>(optionally) of the AP.</p> \r\n<p>For our example we need data to be executed (their content will be printed after the system start). We will put them into <code>data</code> folder.&nbsp;</p> \r\n<pre>META-INF/\r\n        /data/\r\n        /meta.info\r\n</pre> \r\n<p>For an application to be deployed must be the application folder named in the pattern name-version (e.g. code>MySuperAppA-1.0</code>) and placed in the <code>applications</code> folder. This is the default place for the APM to look for the new APs.</p> \r\n<h4>Demo APs&nbsp;</h4> \r\n<p>For the test purposes let\'s create three APs: <code>A</code>, <code>B</code>, <code>C</code> with the following dependencies:</p> \r\n<pre>A --&gt; B, C\r\nB --&gt; C\r\nC --&gt; (no dependency)</pre> \r\n<p>Each AP has two text files called be the pattern ap-name-1.txt and ap-name-2.txt with the content of the name of the AP and number of the data file.</p> \r\n<p>For example the AP named <code>B</code> contains in the folder <code>META-INF/data</code> a file <code>B-1.txt</code> with the content &quot;<code>B1</code>&quot; and a file <code>B-2.txt</code> with the content &quot;<code>B2</code>&quot;.</span></p> \r\n<p>After start of the demo system, we should see the following output:</p> \r\n<pre>C1\r\nC2\r\nB1\r\nB2\r\nA1\r\nA2&nbsp;</pre> \r\n<p>The order of the startups follows the dependencies definitions of the APs.</p> \r\n<h3>APM Implementation</h3> \r\n<p>First of all we need two Ant task for our APM script. The implementation of both is in the <code>core/lib/apm.jar</code> Java library and could be found in the source codes under the <code>ant-lib</code> project.</p> \r\n<h4>Dependencies order</h4> \r\n<p>This task takes a path as a parameter, goes thru this directory and gets all the folders inside it as an input.</p> \r\n<p>Then goes thru all of them and checks the dependencies from the <code>META-INF/meta.info</code>. So it creates a list of dependencies in the order of the need. For each application in the list a sequence inside the task will be executed. The sequence defines the installation procedure.</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"dependencies-order\" classname=\"cz.net21.ttulka.apm.ant.demo.DependenciesOrderTask\"\r\n         classpath=\"core/lib/apm.jar\" /&gt;\r\n...\r\n&lt;dependencies-order path=\"../applications\" relpathmetainfofile=\"META-INF/meta.info\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;antcall target=\"install-app\"&gt;\r\n            &lt;param name=\"appPath\" value=\"@{application}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/dependencies-order&gt;\r\n</pre> \r\n<h4>Configuration record&nbsp;</h4> \r\n<p>For every application we need to scan the <code>data</code> folder and create a list of the files inside. This list in comma-separated form will be appended into the applications configuration file.</p> \r\n<pre class=\"brush: xml\">&lt;taskdef name=\"create-conf-record\" classname=\"cz.net21.ttulka.apm.ant.demo.CreateConfRecordTask\"\r\n         classpath=\"core/lib/apm.jar\" /&gt;\r\n...\r\n&lt;create-conf-record path=\"${appPath}/META-INF/data\"&gt;\r\n    &lt;sequential&gt;\r\n        &lt;antcall target=\"write-conf-record\"&gt;\r\n            &lt;param name=\"record\" value=\"@{record}\" /&gt;\r\n        &lt;/antcall&gt;\r\n    &lt;/sequential&gt;\r\n&lt;/create-conf-record&gt;\r\n...\r\n&lt;target name=\"write-conf-record\"&gt;\r\n    &lt;echo file=\"conf/applications.conf\" append=\"true\"&gt;${line.separator}${appName}=${record}&lt;/echo&gt;\r\n&lt;/target&gt;&nbsp;</pre> \r\n<p> </p> \r\n<h2><em>Scenario Two</em>: Application (Web) Server&nbsp;</h2> \r\n<p>Such a APM could be used in the same way in the case of installing <strong>web applications into a server</strong>.&nbsp;</p> \r\n<p>Consider that the applications provide some functional services and their installation is <strong>dependent on one or more other</strong>.</p> \r\n<p>We can also install <strong>more versions of an application</strong> and enable only the latest version.</p> \r\n<p>For a proper run of an application the <strong>server needs to be configurated</strong> in a way known only to the application self.</p> \r\n<p>Application must be corretly <strong>re-installed after an update of the server</strong>, even when the old server data was completely removed.</p> \r\n<p>Even here helps the APM in all the above mentioned cases.&nbsp;</p> \r\n<h2>Downloads</h2> \r\n<p>You can download <a href=\"/storage/APM-demo.jar\">whole demo project</a>. After unpacking you can start the demo with <code>system/start.bat</code>.</p> \r\n<p>Alternatively you can download the <a href=\"https://github.com/ttulka/application-package-manager-demo\">source project</a>&nbsp;and addapt it to your needs and desires.</p> \r\n<p><br /></p>', 'Programming,Java,Ant', 'false', 'false', 1, 1),
(21, 'json-mock-data-generator', 1493141000, 'JSON Mock Data Generator', '<p>A small program to generate JSON data.</p>', '<p>It\'s easy to generate test mock JSON data for a small HTTP request or similar use because there is plenty of online tools providing this functionality for you.</p> \r\n<p>But what if you want to generate data for a database performance test for systems like MongoDB or Elasticsearch?</p> \r\n\r\n<p>Online tools can generate entities in tens, maybe in hunders, but in thousands or millions?&nbsp;</p> \r\n<p>In this case comes the&nbsp;<a href=\"https://github.com/ttulka/json-mock-data-generator\" target=\"_blank\" title=\"JSON Mock Data Generator\">JSON Mock Data Generator</a> on the scene.</p>', 'Java,Releases', 'false', 'false', 1, 2),
(22, 'how-to-effectively-protect-critical-section', 1496339000, 'How to Effectively Protect Critical Section', '<p>Not everywhere could be immutable objects used to ensure thread-safe code.</p> ', '<p>Sometimes we need to wait for some events (especially coming as user input) to initialize a variable. In the case this input can come from a concurrent environment we need to protect the initialization as critical section.</p> \r\n<p>Native Java approach is to use the keyword <code>synchronized</code> of the critical code and this will work... but is it <strong>effective </strong>as well?</p> \r\n<p>Let\'s discuss some more methods how to protect a critical section in the Java code.</p>\r\n\r\n<p>As usual we start as simple as possible, this is our code:</p> \r\n<pre class=\"brush: java\">class MyCriticalSection {\r\n\r\n    static Object toBeInitialized = null;   // mutable static object\r\n\r\n    public void callFromClient(Object... globalParams) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n\r\n        // so something with the toBeInitialized object\r\n        System.out.println(toBeInitialized.toString());\r\n    }\r\n\r\n    private Object initialize(Object... params) {\r\n        // do some better stuff here...\r\n        return new Object();\r\n    }\r\n}\r\n</pre> \r\n<p>Completely unsafe. So let\'s synchronize it:</p> \r\n<pre class=\"brush: java\">public synchronized void callFromClient(Object... globalParams) {\r\n    // ...\r\n}\r\n</pre> \r\n<p>Very ineffective. Actually we want to synchronize only the initialization part:</p> \r\n<pre class=\"brush: java\">public void callFromClient(Object... globalParams) {\r\n    synchronized (MyCriticalSection.class) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<p>Synchronizing on the class means a caller trying to access the section will be block even when another caller is inside another section synchronized on the class as well:</p> \r\n<pre class=\"brush: java\">void anotherCall() {\r\n    synchronized (MyCriticalSection.class) {\r\n        // ...\r\n    }\r\n}\r\n</pre> \r\n<p>The same for static methods:</p> \r\n<pre class=\"brush: java\">void static doSomething() {\r\n    synchronized (MyCriticalSection.class) {\r\n        // ...\r\n    }\r\n}\r\n</pre> \r\n<p>This could be okay when the initialization is distributed or static, but even for these cases we have a better option:</p> \r\n<pre class=\"brush: java\">private final Object initializationLock = new Object();     // could be static for static methods\r\n\r\npublic void callFromClient(Object... globalParams) {\r\n    synchronized (initializationLock) {\r\n        if (toBeInitialized == null) {\r\n            toBeInitialized = initialize(globalParams);\r\n        }\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<p>Looks nice, we\'re done...? Actually not, using the synchronized keyword is far not the most effectively way how to protect a critical section.</p> \r\n<p>Java 5 came with a package of atomic classes <code>java.util.concurrent.atomic</code> among then <code>AtomicBoolen</code>.</p> \r\n<p><code>AtomicBoolen</code> could be used to check atomically if a condition was already fulfilled or not:</p> \r\n<pre class=\"brush: java\">private final AtomicBoolean alreadyInitialized = new AtomicBoolean();\r\n\r\npublic void callFromClient(Object... globalParams) {\r\n    if (alreadyInitialized.compareAndSet(false, true)) {\r\n        toBeInitialized = initialize(globalParams);\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<p>This works the same way as the synchronized section above and is much more effective... but well, there is another little devil hidden in the code.</p> \r\n<p>Take a look at the code above including the synchronized variants. What happens when another caller calls the method in the same time as the first method entered the critical section to initialize the object?</p> \r\n<p>The second caller doesn\'t get the access to the critical section and continues in the code execution. But this means he works with a potentially uninitialized object and the last line of code will throw a <code>NullPointerException</code> trying to call <code>toString()</code> method on the <code>null</code> value.</p> \r\n<p>Our job is not done here yet...</p> \r\n<p>In the package <code>java.util.concurrent</code> we can find a class<code>CountDownLatch</code> designed to synchronize concurrency of thread processing:</p> \r\n<pre class=\"brush: java\">private final AtomicBoolean initializationStarted = new AtomicBoolean();\r\nprivate final CountDownLatch initializationFinished = new CountDownLatch(1);\r\n\r\npublic void callFromClient(Object... globalParams) throws InterruptedException {\r\n    if (initializationStarted.compareAndSet(false, true)) {\r\n        toBeInitialized = initialize(globalParams);\r\n        initializationFinished.countDown();\r\n    } else {\r\n        initializationFinished.await();\r\n    }\r\n    // ...\r\n}\r\n</pre> \r\n<p>So, what happens here? We use the atomic boolean to check if the object was already initialized or not. If not we access the critical section. Because the set and set of the atomic boolean is done atomically only one thread can access the initialization.</p> \r\n<p>The initialization is first ready when the first caller counts down the latch. Before the latch was count-downed all the concurrent callers wait on its <code>await()</code> method.</p> \r\n<p>All this doesn\'t look as nice and easy as with the <code>synchronized</code> keyword. So, is it really so effective?</p> \r\n<p>I did performance tests using all the above mentioned techniques. I tried to access the critical section one million times for each of them:</p> \r\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"3\"> \r\n<tbody> \r\n<tr> \r\n<td><strong>technique</strong></td> \r\n<td><strong>execution time</strong></td> \r\n</tr> \r\n<tr> \r\n<td>not thread-safe </td> \r\n<td style=\"text-align: right;\">2 ms</td> \r\n</tr> \r\n<tr> \r\n<td>synchronized class</td> \r\n<td style=\"text-align: right;\">32 ms</td> \r\n</tr> \r\n<tr> \r\n<td>synchronized object</td> \r\n<td style=\"text-align: right;\">26 ms</td> \r\n</tr> \r\n<tr> \r\n<td>atomic-latch</td> \r\n<td style=\"text-align: right;\">16 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n<p>Of course there a great penalty of synchronization but this shouldn\'t be a big surprise, I guess.</p> \r\n<p>The more important thing is the <em>AtomicBoolean-CountDownLatch</em> synchronization is over 40 % more effective. In environments like servers where the calls come for every request hundred thousand times in second this is a pretty nice optimization.</p> \r\n', 'Programming,Java', 'false', 'false', 1, 1),
(23, 'managing-asynchronous-tests', 1500480000, 'Managing Asynchronous Tests', '<p>It\'s recommended to avoid any asynchrony within the scope of the test. Unfortunately, this is not possible everywhere.</p>', '<p>Testing asynchrony could be pretty tricky, not always is clear how long must a test wait for a result to be delivered - if it\'s too little the test fails event when the tested functionality works, if it\'s too long the test is just wasting time (so expensive especially in the commit stage).</p> \r\n\r\n<p>So, how to deal with this problem?</p>\r\n\r\n<p>Consider a simple test in JUnit:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldDeliverResult() {\r\n&nbsp; &nbsp; calculateResult();\r\n&nbsp; &nbsp; confirmResultWasDelivered(); &nbsp; &nbsp; &nbsp; &nbsp;\r\n}\r\n\r\nprivate void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; if (!resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; fail(\"No result was delivered!\");\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>In this unit test we test a state of an object after calling the method <code>calculateResult()</code> with the method <code>resultWasDelivered()</code>. If the method <code>calculateResult()</code> is designed in the asynchronous manner, the test will fail almost in 100 % cases. </p> \r\n<p>A naive solution is to <strong>add a timeout</strong>:</p> \r\n<pre class=\"brush: java\">private void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; sleep(TIMEOUT);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; if (!resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; fail(\"No result was delivered!\");\r\n&nbsp; &nbsp; }\r\n}\r\nprivate void sleep(long milliseconds) {\r\n&nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; Thread.sleep(milliseconds);\r\n&nbsp; &nbsp; } catch (InterruptedException ignore) {\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>But again, what is the <em>right </em>value of the timeout here?</p> \r\n<p>We want to be sure that our test passes always when a correct result is delivered, so we set the timeout to a maximal waiting period. On the other hand in some cases the result could be delivered quickly in a small amout of time: </p> \r\n<pre class=\"brush: java\">private void confirmResultWasDelivered() {\r\n&nbsp; &nbsp; long testStarted = System.currentTimeMillis();\r\n&nbsp; &nbsp; do {\r\n&nbsp; &nbsp; &nbsp; &nbsp; if (resultWasDelivered()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return;\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; sleep(SMALL_AMOUT_OF_TIME);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; } while (TIMEOUT &gt; System.currentTimeMillis() - testStarted);\r\n&nbsp; &nbsp; \r\n&nbsp; &nbsp; fail(\"No result was delivered!\");\r\n}</pre> \r\n<p>The last approach is still based on the pull princip and active waiting, but it\'s much more efficient as the previous one.&nbsp;</p> \r\n<p>We try to check if a result was delivered, if not we try it again after a small amout of time till the timeout exceeds.</p> \r\n<p> </p>', 'Testing,Programming,Java', 'false', 'false', 1, 1),
(24, 'synchronized-methods-vs-semaphore', 1501685000, 'Synchronized Methods vs. Semaphore', '<p>When one-thread-access for synchronized methods is too restrictive, semaphores come to mind.</p> ', '<p>There is one important difference in these approaches.</p> \r\n\r\n<p>Let\'s consider a code:</p> \r\n<pre class=\"brush: java\">private Object synch = new Object();\r\n\r\npublic void synchronizedMethod1() {\r\n    synchronized (synch) {\r\n        synchronizedMethod2();\r\n    }\r\n}\r\n\r\npublic void synchronizedMethod2() {\r\n    synchronized (synch) {\r\n        System.out.println(\"Access to the resource acquired.\");\r\n    }\r\n}</pre> \r\n<p>What happened by calling the <code>synchronizedMethod1()</code>?</p> \r\n<p>Of course the access to the resource will be acquired because the synchronized block is in Java shared in within the thread. In other words, if the thread is already in the synchronized block will get all the others as well (protected by the same object).</p> \r\n\r\n<p>Applying a semaphore (with the counter set to one) on the same code:</p> \r\n<pre class=\"brush: java\">private Semaphore semaphore = new Semaphore(1);\r\n\r\npublic void synchronizedMethod1() throws InterruptedException {\r\n    semaphore.acquire();\r\n    synchronizedMethod2();\r\n    semaphore.release();\r\n}\r\n\r\npublic void synchronizedMethod2() throws InterruptedException {\r\n    semaphore.acquire();\r\n    System.out.println(\"Access to the resource acquired.\");\r\n    semaphore.release();\r\n} </pre> \r\n<p>And calling <code>synchronizedMethod1()</code> will freeze forever.</p> \r\n\r\n<p>In this case acquire the <code>synchronizedMethod1</code> the semaphore but <code>synchronizedMethod2</code>&nbsp;fails on trying to acquire it again.</p> \r\n\r\n<p>In general, this problem is caused by a weak design and can be solved by a simple refactoring:</p> \r\n<pre class=\"brush: java\">public void synchronizedMethod1() throws InterruptedException {\r\n    semaphore.acquire();\r\n    resourceMethod();\r\n    semaphore.release();\r\n}\r\n\r\npublic void synchronizedMethod2() throws InterruptedException {\r\n    semaphore.acquire();\r\n    resourceMethod();\r\n    semaphore.release();\r\n}\r\n\r\nprivate void resourceMethod() {\r\n    System.out.println(\"Access to the resource acquired.\");\r\n}</pre>', 'Programming,Java', 'false', 'false', 1, 1),
(25, 'thistledb-simple-json-database-released', 1505935000, 'ThistleDB - Simple JSON Database - Released!', '<p>Simple JSON database based on the <strong>file-access</strong> and <strong>server-client</strong> approach with the non-blocking server and the reactive (asynchronous non-blocking) client.</p>', '<p>All about:&nbsp;<a href=\"http://thistledb.net21.cz\"><strong>http://thistledb.net21.cz</strong></a></p> \r\n<p>Have fun!</p>', 'Database,Java,Releases', 'false', 'true', 1, 2),
(26, 'pitfalls-of-processing-a-stream-from-an-external-program', 1506962000, 'Pitfalls of Processing a Stream from an External Program', '<p>How to design a standalone program that produces a big amount of binary data, and what are the pitfalls of the approach?</p>', '<p>A good example is a file converter (images, mp3s, documents, etc).</p>\r\n\r\n<h2>Standalone Producer Application</h2> \r\n<p>There is many ways how to create a standalone application and one of the easiest and the most straight-forward approaches is Spring-Boot (<code>pom.xml</code>):</p> \r\n<pre class=\"brush: xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\r\n&nbsp; &nbsp; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\r\n&nbsp; &nbsp; &lt;parent&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;!-- Your own application should inherit from spring-boot-starter-parent --&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;/parent&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.eval&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;StandaloneBytesProducer&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;properties&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;java.version&gt;1.8&lt;/java.version&gt;\r\n&nbsp; &nbsp; &lt;/properties&gt;\r\n&nbsp; &nbsp; &lt;dependencies&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;commons-logging&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.2&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;lombok&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.16.18&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;scope&gt;provided&lt;/scope&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &lt;/dependencies&gt;\r\n&nbsp; &nbsp; &lt;build&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;plugins&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;3.7.0&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;source&gt;1.8&lt;/source&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;target&gt;1.8&lt;/target&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugins&gt;\r\n&nbsp; &nbsp; &lt;/build&gt;\r\n&lt;/project&gt;</pre> \r\n<p><code>StandaloneBytesProducerApplication.java</code>:</p> \r\n<pre class=\"brush: java\">package cz.net21.ttulka.eval.bytesproducer;\r\n\r\nimport org.springframework.boot.ApplicationArguments;\r\nimport org.springframework.boot.ApplicationRunner;\r\nimport org.springframework.boot.SpringApplication;\r\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\r\nimport lombok.extern.apachecommons.CommonsLog;\r\n\r\n@SpringBootApplication\r\n@CommonsLog\r\npublic class StandaloneBytesProducerApplication implements ApplicationRunner {\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void run(ApplicationArguments args) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; log.info(\"StandaloneBytesProducerApplication started.\");\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; int bytesAmount = 1000;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (args.containsOption(\"bytes\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bytesAmount = Integer.parseInt(args.getOptionValues(\"bytes\").get(0));&nbsp; &nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; bytesAmount; i++) {&nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.write(i % Byte.MAX_VALUE);&nbsp; &nbsp;// we\'re writing on the standard output stream\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(\"Unexpected error.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; System.exit(0);\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; public static void main(String[] args) throws Exception {\r\n&nbsp; &nbsp; &nbsp; &nbsp; SpringApplication.run(StandaloneBytesProducerApplication.class, args);\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>Compile it and run:</p> \r\n<pre>mvn clean package\r\nmvn spring-boot:run</pre> \r\n<p>It looks good. Of course a consumer will run it direct from a JAR:</p> \r\n<pre>java -jar target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar</pre> \r\n<p>The result is what we expected, Spring Boot ASCII logo, some log messages and our bytes stream.</p> \r\n<p>And this is exactly one pitfall because all this junk destroys our result, actually all and only we need is the bytes stream.</p> \r\n<p><br /></p> \r\n<p>Spring Boot uses it own logging (based on <code><a href=\"https://commons.apache.org/proper/commons-logging/\" target=\"_blank\">commons-logging</a></code>) hidden in the artifact <code>spring-boot-starter-logging</code>. To get rid of it we can exclude this artifact from the build:</p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n&nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;exclusions&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;exclusion&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/exclusion&gt;\r\n&nbsp; &nbsp; &lt;/exclusions&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>When we now run the program, the log messages look different. After excluding the Spring Boot logging the <code>commons-logging</code> uses its default fall-back implementation <code>SimpleLog</code>.</p> \r\n<p><code>SimpleLog</code> then sends all messages, for all defined loggers, to <code>stderr</code>. We can prove it by forwarding the standard output into a file:</p> \r\n<pre>java -jar target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar &gt; out.dat</pre> \r\n<p>Indeed, the log messages are still written in the console and the file includes only the Spring Boot logo and our bytes.</p> \r\n<p>To get rid of the logo is easy, just put the <code>application.yml</code> into the <em>resources </em>directory:</p> \r\n<pre>spring:\r\n&nbsp; main:\r\n&nbsp; &nbsp; banner-mode: \"off\"</pre> \r\n<p> \r\n<p>Now the standard output contains only the result bytes. It\'s time to implement a consumer...</p> \r\n</p> \r\n<h2>Standalone Consumer Application</h2> \r\n<p>Consumer could be done in the same manner, this time we don\'t case about logging much (<code>pom.xml</code>):</p> \r\n<pre class=\"brush: xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\r\n&nbsp; &nbsp; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\r\n&nbsp; &nbsp; &lt;parent&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;!-- Your own application should inherit from spring-boot-starter-parent --&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;/parent&gt;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.eval&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;StandaloneBytesConsumer&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;\r\n&nbsp; &nbsp; &lt;properties&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;java.version&gt;1.8&lt;/java.version&gt;\r\n&nbsp; &nbsp; &lt;/properties&gt;\r\n&nbsp; &nbsp; &lt;dependencies&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;commons-logging&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.2&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;dependency&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;lombok&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;1.16.18&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;scope&gt;provided&lt;/scope&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/dependency&gt;\r\n&nbsp; &nbsp; &lt;/dependencies&gt;\r\n&nbsp; &nbsp; &lt;build&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;plugins&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;version&gt;3.7.0&lt;/version&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;source&gt;1.8&lt;/source&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;target&gt;1.8&lt;/target&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/configuration&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugin&gt;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &lt;/plugins&gt;\r\n&nbsp; &nbsp; &lt;/build&gt;\r\n&lt;/project&gt;&nbsp;</pre> \r\n<p>Important pitfall here to be aware about: <strong>all (stdout, stderr) the streams must be consumed</strong>. If you forget to consume the <code>stderr</code> stream the program will freeze forever.</p> \r\n<p>The error log can be either consumed and forgotten or consumed and print into the log:</p> \r\n<pre class=\"brush: java\">package cz.net21.ttulka.eval.bytesconsumer;\r\n\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport java.util.Scanner;\r\nimport org.springframework.boot.ApplicationArguments;\r\nimport org.springframework.boot.ApplicationRunner;\r\nimport org.springframework.boot.SpringApplication;\r\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\r\nimport lombok.extern.apachecommons.CommonsLog;\r\n\r\n@SpringBootApplication\r\n@CommonsLog\r\npublic class StandaloneBytesConsumerApplication implements ApplicationRunner {\r\n&nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; public void run(ApplicationArguments args) {&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; String pathToJar = System.getProperty(\"PATH_TO_JAR\");\r\n&nbsp; &nbsp; &nbsp; &nbsp; log.info(\"StandaloneBytesConsumerApplication started: \" + pathToJar);\r\n&nbsp; &nbsp; &nbsp; &nbsp; ProcessBuilder builder = new ProcessBuilder(\"java\", \"-jar\", pathToJar);&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Process process = builder.start();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; processErrors(process.getErrorStream());&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; processStream(process.getInputStream());&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\r\n&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(\"Unexpected error.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.exit(1);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; System.exit(0);\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; private void processStream(InputStream stream) throws IOException {\r\n&nbsp; &nbsp; &nbsp; &nbsp; int b;\r\n&nbsp; &nbsp; &nbsp; &nbsp; while ((b = stream.read()) != -1) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // TODO do something with the stream\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; stream.close();\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; private void processErrors(final InputStream in) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; new Thread(new Runnable() {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; @Override\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public void run() {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; int logLevel = 3;&nbsp; &nbsp;// 0 - ERROR, 1 - WARN, 2 - INFO, 3 - DEBUG\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Scanner scanner = new Scanner(in);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (scanner.hasNextLine()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; String line = scanner.nextLine();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"ERROR\") || line.startsWith(\"FATAL\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 0;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"WARN\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 1;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"INFO\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 2;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (line.startsWith(\"DEBUG\") || line.startsWith(\"TRACE\")) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logLevel = 3;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; switch (logLevel) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 0:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.error(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 1:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.warn(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; case 2:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.info(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; default:\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; log.debug(line);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; }).start();\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; public static void main(String[] args) throws Exception {\r\n&nbsp; &nbsp; &nbsp; &nbsp; SpringApplication.run(StandaloneBytesConsumerApplication.class, args);\r\n&nbsp; &nbsp; }\r\n}&nbsp;</pre> \r\n<p>Compile and run it:</p> \r\n<pre>mvn clean package\r\nmvn spring-boot:run -DPATH_TO_JAR=..\\StandaloneBytesProducer\\target\\StandaloneBytesProducer-1.0.0-SNAPSHOT.jar</pre> \r\n\r\n<p>Source codes: <a href=\"https://github.com/ttulka/eval/tree/master/StandaloneBytesProducer\" target=\"_blank\" title=\"StandaloneBytesProducer\">StandaloneBytesProducer</a> and <a href=\"https://github.com/ttulka/eval/tree/master/StandaloneBytesConsumer\" target=\"_blank\" title=\"StandaloneBytesConsumer\">StandaloneBytesConsumer</a>.&nbsp;</p> \r\n<p>Happy byting!</p>\r\n', 'Programming,Java', 'false', 'false', 1, 1),
(27, 'process-watch-dog-released', 1507387000, 'Process Watch Dog Released!', '<p>It\'s a good idea to work with external processes to prevent the system from failure especially when the process is very resources-demanding. \r\n<br/>But what if something goes wrong?\r\n<br/>Let the <em>watch dog</em> out!</p>', '<pre class=\"brush: xml\">&lt;dependency&gt;\r\n&nbsp; &nbsp; &lt;groupId&gt;cz.net21.ttulka.exec&lt;/groupId&gt;\r\n&nbsp; &nbsp; &lt;artifactId&gt;process-watch-dog&lt;/artifactId&gt;\r\n&nbsp; &nbsp; &lt;version&gt;1.0.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Consider a very simple tooling class for executing new processes. Put the watch dog to kill processes after 2 minutes:</p> \r\n<pre class=\"brush: java\">import cz.net21.ttulka.exec.ProcessWatchDog;\r\n\r\npublic class ProcessExecutorTool {\r\n\r\n&nbsp; &nbsp; private static int TIMEOUT = 2 ✱ 60 ✱ 1000; // 2 minutes\r\n\r\n&nbsp; &nbsp; private static ProcessWatchDog watchDog = new ProcessWatchDog();\r\n\r\n&nbsp; &nbsp; public static int executeExternalProcess(String... cmd) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; try {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProcessBuilder builder = new ProcessBuilder(cmd);\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Process process = builder.start();\r\n\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; watchDog.watch(process, TIMEOUT);   // release the dog!</pre> \r\n<pre class=\"brush: java\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; process.waitFor();\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return process.exitValue();</pre> \r\n<pre class=\"brush: java\">&nbsp; &nbsp; &nbsp; &nbsp; } catch (Exception e) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; throw new RuntimeException(\"Exception by executing \'\" + cmd + \"\'.\", e);\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; }\r\n}</pre> \r\n<p>All about:&nbsp;<a href=\"https://github.com/ttulka/process-watch-dog\"><strong>https://github.com/ttulka/process-watch-dog</strong></a> </p> \r\n<p>Happy watching!</p>', 'Java,Releases', 'false', 'true', 1, 2),
(28, 'boundary-io-streams-released', 1509558000, 'Boundary I/O Streams Released!', '<p>New Java library for working with <strong>boundary I/O streams</strong> was released.</p>\r\n', '<p>This library provides classes for processing multiple streams withing a single stream bounded by a specific boundary content.\r\n</p> \r\n<p>Let\'s take a deeper look at its functionality...</p>\r\n<p>First of all, put the Maven dependency into your <code>pom.xml</code>:</p> \r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.io&lt;/groupId&gt;\r\n    &lt;artifactId&gt;boundary-io-streams&lt;/artifactId&gt;\r\n    &lt;version&gt;1.0.0&lt;/version&gt;\r\n&lt;/dependency&gt;&nbsp;</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p>Consider a simple use-case: a files streaming service into the <code>stdout</code>.</p> \r\n<pre class=\"brush: java\">public class FileStreamService {\r\n\r\n    public static void main(String[] args) throws IOException {\r\n        try (BoundaryOutputStream bos = new BoundaryOutputStream(System.out)) {\r\n\r\n            for (String filename : args) {\r\n                try (InputStream is = new FileInputStream(filename)) {\r\n\r\n                    org.apache.commons.io.IOUtils.copy(is, bos);  // see https://commons.apache.org/io\r\n                    bos.boundary();                               // separate the stream with the boundary\r\n                }\r\n            }\r\n        }\r\n    }\r\n}&nbsp;</pre> \r\n<p>When you run the service from the command line</p> \r\n<pre>java FileStreamService /data/text1.txt /data/text2.txt&nbsp;</pre> \r\n<p>You get something similar like:</p> \r\n<pre>Content of the text1.txt-----StreamBoundary-----Content of the text2.txt-----StreamBoundary-----&nbsp;</pre> \r\n<p>Notice that the class&nbsp;<code>BoundaryOutputStream</code> is nothing more than a convenient class, you don\'t need this class to create such a stream with standard Java I/O classes.</p> \r\n<h2>Reading a Multiple Stream&nbsp;</h2> \r\n<p>For reading a multiple stream you can use the class&nbsp;<code>BoundaryInputStream</code> direct or wrap it into a convenient iterable class <code>IterableBoundaryInputStream</code>. We show the second approach:</p> \r\n<pre class=\"brush: java\">InputStream in = callFileStreamingService();\r\n\r\ntry (BoundaryInputStream bis = new BoundaryInputStream(in)) {\r\n    IterableBoundaryInputStream ibis = new IterableBoundaryInputStream(bis);\r\n\r\n    for (InputStream is : ibis) {                \r\n        // do something with the stream\r\n    }\r\n}</pre> \r\n<p>The method&nbsp;<code>callFileStreamingService</code> calls the service above and returns its stream as the input for the <code>BoundaryInputStream</code>.</p> \r\n<p>More details at&nbsp;<a href=\"https://github.com/ttulka/boundary-io-streams\" target=\"_blank\" title=\"ttulka/boundary-io-streams\">https://github.com/ttulka/boundary-io-streams</a>.</p> \r\n<p>Happy streaming!</p> \r\n<p> </p> \r\n<p> </p>', 'Java,Releases', 'false', 'true', 1, 2),
(29, 'process-watch-dog-with-heartbeat-released', 1510336000, 'Process Watch Dog with Heartbeat Released!', '<p>It\'s nice to watch a process execution and kill frozen processes after timeout, but when a process is supplying data is not frozen even when it takes a long time.</p>\r\n', '<p>Release 1.1.0 introduces a new process wrapper to send a <strong>heartbeat to reset the timeout</strong> explicitly or automatically with every read byte.</p>\r\n<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.exec&lt;/groupId&gt;\r\n    &lt;artifactId&gt;process-watch-dog&lt;/artifactId&gt;\r\n    &lt;version&gt;1.1.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>Putting a process into watching returns a process wrapper:</p> \r\n<pre class=\"brush: java\">Process process = ...\r\n\r\nProcessWatchDog watchDog = new ProcessWatchDog();\r\n\r\nWatchedProcess watchedProcess = watchDog.watch(process, 1000);</pre> \r\n<p>Because the class <code>WatchedProcess</code> extends stadard Java class Process, we can do simply:</p> \r\n<pre class=\"brush: java\">process = watchDog.watch(process, 1000);</pre> \r\n<p>Now, a heartbeat will be sent every time we read from the process input stream:</p> \r\n<pre class=\"brush: java\">InputStream is = process.getInputStream();\r\nint b;\r\nwhile ((b = is.read()) != -1) {\r\n    // heartbeat is sent implicitly with every successful call of `read()` \r\n}</pre> \r\n<p>To send a heartbeat explicitly, we can do it via the WatchedProcess object:</p> \r\n<pre class=\"brush: java\">wp.heartBeat();</pre> \r\n<p>Or simply tell the Watch Dog to do it:</p> \r\n<pre class=\"brush: java\">watchDog.heartBeat(process);   // we don\'t need the `WatchedProcess` object here</pre> \r\n<p>Previous post about the Watch Dog: <a href=\"/process-watch-dog-released\">Process Watch Dog Released!</a></p> \r\n<p>All about:&nbsp;<a href=\"https://github.com/ttulka/process-watch-dog\"><strong>https://github.com/ttulka/process-watch-dog</strong></a></p> \r\n<p>Happy heartbeating!</p> \r\n<p> </p>', 'Java,Releases', 'false', 'true', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(30, 'stop-boundary-io-streams-released', 1512587000, 'Stop Boundary I/O Streams Released!', '<p>What if the stream continues further without containing any sub-stream? In this case we can use the new released <strong>Stop Boundary Stream</strong> feature from the same library to ignore the rest of the stream after a boundary was reached.</p>', '<pre class=\"brush: xml\">&lt;dependency&gt;\r\n    &lt;groupId&gt;cz.net21.ttulka.io&lt;/groupId&gt;\r\n    &lt;artifactId&gt;boundary-io-streams&lt;/artifactId&gt;\r\n    &lt;version&gt;1.2.0&lt;/version&gt;\r\n&lt;/dependency&gt;</pre> \r\n<p>To stop consuming a stream after a boundary was reached it is possible to use the <code>StopBoundaryInputStream</code> class and the convenience class <code>StopBoundaryOutputStream</code> to generate such a stream. The stop boundary streaming is still using a boundary to separate sub-streams, but when a stop boundary occurs the rest of the input stream is ignored.</p> \r\n<p>Let\'s take a look at a simple example:&nbsp;</p> \r\n<pre class=\"brush: java\">String[] values = {\r\n    \"abcde\", \"ABCDE\", \"12345\"\r\n};\r\n\r\nbyte[] boundary = \"|\".getBytes();\r\nbyte[] stopBoundary = \"#\".getBytes();\r\n\r\nByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n\r\nStopBoundaryOutputStream out = new StopBoundaryOutputStream(bytes, boundary, stopBoundary);\r\n\r\nfor (String s : values) {\r\n    out.write(s.getBytes());\r\n    out.boundary();\r\n}\r\nout.stopBoundary();\r\nout.write(\"xyz\".getBytes());    // some junk at the end\r\n\r\n//System.out.println(bytes);    // prints `abcde|ABCDE|12345|#xyz`\r\n\r\nStopBoundaryInputStream in = new StopBoundaryInputStream(\r\n        new ByteArrayInputStream(bytes.toByteArray()), boundary, stopBoundary);\r\n\r\nfor (InputStream is : in) {\r\n    int b;\r\n    while ((b = is.read()) != -1) {\r\n        System.out.print((char) b);\r\n    }\r\n    System.out.println();\r\n}\r\n\r\n// close streams...\r\n</pre> \r\n<p>The code above prints:</p> \r\n<pre>abcde\r\nABCDE\r\n12345\r\n</pre> \r\n<p>Previous post about the library: <a href=\"/boundary-io-streams-released\">Boundary I/O Streams Released!</a></p> \r\n<p>All about: <a href=\"https://github.com/ttulka/boundary-io-streams\"><strong>https://github.com/ttulka/boundary-io-streams</strong></a></p> \r\n<p> </p> \r\n<p>Happy streaming!</p>', 'Java,Releases', 'false', 'true', 1, 2),
(31, 'debugging-jasmine-ts-in-intellij-ide', 1522242000, 'Debugging jasmine-ts in IntelliJ IDE', '<p>TypeScript application wants tests to be written in TypeScript as well, right?</p> ', '<p>There is a great library to achive this: <a href=\"https://www.npmjs.com/package/jasmine-ts\" target=\"_blank\">jasmine-ts</a>. But how to debug such tests within your&nbsp;IntelliJ IDE?</p>\r\n<p>You need to use the Node.js plugin and configure it to run/debug the <code>jasmine-ts</code> like following:</p> \r\n<p><img src=\"/storage/jasmine-ts-debug.png\" alt=\"jasmine-ts debug from IntelliJ\" width=\"100%\" /></p> \r\n<p>I\'m developing under Windows, but for Mac or Linux would be the situation almost the same.</p> \r\n<p>The meaning of the settings is following:</p> \r\n<p> </p> \r\n<ul> \r\n<li><em>Working directory</em> is the location of my TypeScript project, generally it the directory where your Jasmine configuration is located.</li> \r\n<li><em>JavaScript file</em> is the <code>jasmine-ts</code> bootstrap file. My local <code>npm</code>&nbsp;repository is located in my home ordner under&nbsp;<code>AppData\\Roaming\\npm\\node_modules</code>, this is the folder where <code>jasmine-ts</code> is installed into.</li> \r\n<li><em>Application parameters</em> are used to debug a concrete test suite. If you want to debug all the Jasmine tests, leave this field empty. In my case, all my Jasmine test files are located under the <code>test</code> sub-folder in my project directory.</li> \r\n</ul> \r\n<p>Then just set up some breakpoints and click the Run/Debug button.</p> \r\n<p>Here we go...&nbsp;</p> \r\n<p>Happy debugging!</p> \r\n<p> </p>', 'Programming,Debugging,JavaScript', 'false', 'false', 1, 2),
(32, 'meaning-of-queues-and-topics-in-aws', 1523468000, 'Meaning of Queues and Topics in AWS', '<p>What\'s the difference between queues and topics in AWS? It\'s not as obvious as it seems to be...</p>', '<p><em>Queues</em> and <em>topics</em> are standard communication channels in messaging systems. \r\nHow to use then in <strong>AWS</strong>, for example to implement the&nbsp;<a href=\"http://www.enterpriseintegrationpatterns.com/patterns/messaging/CompetingConsumers.html\" target=\"_blank\" title=\"Competing Consumers\">Competing Consumers</a> pattern?</p>\r\n\r\n<p>Let\'s consider a following scenario:</p> \r\n<p>A REST endpoint (Amazon API Gateway‎) initiates a time expensive processing served by a serverless function (AWS Lambda). Not only because of the max timeout 30 seconds on the gateway it is not a good idea to process the request synchronously (see <a href=\"https://www.reactivemanifesto.org\" title=\"The Reactive Manifesto\">The Reactive Manifesto</a>).&nbsp;</p> \r\n<p>Let\'s build a queue of working tasks into the middle between the gateway and the worker function:</p> \r\n<pre>Request --&gt; Queue --&gt; Worker&nbsp;</pre> \r\n<p>Immediately after a request comes it is put into the queue and taken by a worker function to the processing.</p> \r\n<p>To implement this in AWS the tasks queue is not a queue (SQS) at all, but a topic (SNS). Let\'s&nbsp;explain why.</p> \r\n<p>In a standard non-serverless implementation will be workers listening on the queue and distributing the tasks among each other. It means the queue is a <em>pull</em> mechanism.</p> \r\n<p>Serverless service creates as many worker instances as needed. So there is no need to keep tasks in a queue while they are processed immediately as put into the queue. Initialization of the worker function must be event-driven (a lambda function must never&nbsp;idle!) - implementing a <em>push</em> mechanism.</p> \r\n<p>There is no change to register a lambda function to a queue, for such a usage there is another concept - topics:</p> \r\n<pre>Request --&gt; Topic --&gt; Worker</pre> \r\n<p>As the request comes a task is put into the topic and processed by the lambda function, elastically created on demand.</p> \r\n<p>So, that\'s the meaning of queues and topics in AWS.</p> \r\n<p>Happy clouding!</p>', 'Event-Driven,Cloud,AWS', 'false', 'false', 1, 2),
(33, 'php-restful-api-microservices', 1525189000, 'PHP Restful API Microservices', '<p>How difficult is to properly implement the <strong>microservices design pattern in PHP</strong>?</p>\r\n', '<p>It will definitely need some thinking and maybe to leave some convince behind. At the end of the day we should have an autonomous loosely coupled service with all the benefits (and drawbacks) of microservices.</p>\r\n<p>In this article we do a little walk-through of a development process of a small RESTful microservice in PHP and we take a look at some Domain-Driven Design (DDD) theory as well.</p>\r\n<p>Consider a simple <a href=\"https://en.wikipedia.org/wiki/Create,&#95;read,&#95;update&#95;and&#95;delete\" target=\"_blank\" title=\"CRUD\">CRUD</a> service for a blog articles management. Via this <a href=\"https://martinfowler.com/articles/richardsonMaturityModel.html\" target=\"_blank\" title=\"REST\">REST</a> API you can list, create, update and delete articles in the database (or whatever the persistence is).</p> \r\n<p>We will implement this service following the <a href=\"https://martinfowler.com/articles/microservices.html\" target=\"_blank\" title=\"Microservices\">microservices design pattern</a> so we create a single REST endpoint (<code>/articles</code>) serving all the request.&nbsp;<br />Similarly we would create endpoint for the categories and authors management.</p> \r\n<p>First of all, let\'s prepare the infrastructure.</p> \r\n<h2>Database Schema and Test-Data</h2> \r\n<p>Traditionally, we use a <strong>MySQL database</strong> with a following schema:&nbsp;</p> \r\n<pre class=\"brush: sql\">CREATE SCHEMA `blog` DEFAULT CHARACTER SET utf8;\r\n\r\nCREATE TABLE `blog`.`categories` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `name` VARCHAR(50) NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`));\r\n&nbsp;&nbsp;\r\nCREATE TABLE `blog`.`authors` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `name` VARCHAR(50) NOT NULL,\r\n&nbsp; `email` VARCHAR(50) NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`));\r\n\r\nCREATE TABLE `blog`.`articles` (\r\n&nbsp; `id` INT NOT NULL AUTO&#95;INCREMENT,\r\n&nbsp; `title` VARCHAR(100) NOT NULL,\r\n&nbsp; `summary` TEXT(1000) NOT NULL,\r\n&nbsp; `body` TEXT NOT NULL,\r\n&nbsp; `createdAt` DATE&nbsp; NOT NULL,\r\n&nbsp; `categoryId` INT NOT NULL,\r\n&nbsp; `authorId` INT NOT NULL,\r\n&nbsp; PRIMARY KEY (`id`),\r\n&nbsp; INDEX `categoryId&#95;idx` (`categoryId` ASC),\r\n&nbsp; INDEX `authorId&#95;idx` (`authorId` ASC),\r\n&nbsp; CONSTRAINT `category&#95;fk`\r\n&nbsp; &nbsp; FOREIGN KEY (`categoryId`)\r\n&nbsp; &nbsp; REFERENCES `blog`.`categories` (`id`),\r\n&nbsp; CONSTRAINT `author&#95;fk`\r\n&nbsp; &nbsp; FOREIGN KEY (`authorId`)\r\n&nbsp; &nbsp; REFERENCES `blog`.`authors` (`id`));&nbsp;</pre> \r\n<p>Then, let\'s put one author, two categories and three articles into the database.</p> \r\n<pre class=\"brush: sql\">INSERT INTO `blog`.`authors` (`id`, `name`, `email`)\r\n&nbsp; VALUES (0, \'Tomas Tulka\', \'tomas.tulka@gmail.com\');\r\n\r\nINSERT INTO `blog`.`categories` (`id`, `name`)\r\n&nbsp; VALUES (0, \'PHP\'), (0, \'Java\');\r\n\r\nINSERT INTO `blog`.`articles` (`id`, `title`, `summary`, `body`, `createdAt`, `categoryId`, `authorId`)\r\n&nbsp; VALUES \r\n&nbsp; &nbsp; (0, \'Sample PHP blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Lorem ipsum dolor sit amet.\',\r\n&nbsp; &nbsp; \'Sed vitae tincidunt magna. Sed pretium neque commodo mauris lobortis, quis finibus dolor malesuada.\',\r\n&nbsp; &nbsp; \'2018-05-01\', 1, 1),&nbsp;\r\n&nbsp; &nbsp; (0,&nbsp; \'Another PHP blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Donec id pellentesque elit, sit amet accumsan mi.\',\r\n&nbsp; &nbsp; \'Duis molestie tellus quis orci venenatis, ac pretium quam malesuada. Vivamus congue justo nulla, sit amet pharetra purus condimentum at.\',\r\n&nbsp; &nbsp; \'2018-05-02\', 1, 1),&nbsp;\r\n&nbsp; &nbsp; (0,&nbsp; \'Java blog post\',&nbsp;\r\n&nbsp; &nbsp; \'Praesent porta sagittis diam non interdum.\',\r\n&nbsp; &nbsp; \'Semper a nunc nec dapibus. Sed tristique vel ipsum vitae euismod. Aenean vel nibh ac diam ullamcorper porta id at purus.\',\r\n&nbsp; &nbsp; \'2018-05-02\', 2, 1);</pre> \r\n<p>After creating the database schema and putting some test data, we can discuss the service architecture design.&nbsp;</p> \r\n<h2>REST API&nbsp;</h2> \r\n<p>We will use the <a href=\"https://martinfowler.com/articles/richardsonMaturityModel.html#level2\" title=\"REST Verbs\">Richardson Maturity Model</a> approach of <strong>HTTP verbs</strong>, simply:&nbsp;</p> \r\n<ul> \r\n<li><code>GET endpoint</code> - listing items</li> \r\n<li><code>GET endpoint/{id}</code> - item detail</li> \r\n<li><code>POST endpoint</code> - creating a new item</li> \r\n<li><code>PUT endpoint/{id}</code> -&nbsp;updating an existing item</li> \r\n<li><code>DELETE&nbsp;endpoint/{id}</code> - deleting an existing item</li> \r\n</ul> \r\n<h2>Microservices and Domain-Driven Design</h2> \r\n<p>We will model our microservice around a <a href=\"https://martinfowler.com/bliki/DDD&#95;Aggregate.html\" target=\"_blank\" title=\"DDD Aggregate\">DDD Aggregate</a>, in this case around the&nbsp;<em>Article</em>.</p> \r\n<p>For sake of simplicity we model our Article aggregate as an <a href=\"https://martinfowler.com/bliki/AnemicDomainModel.html\" target=\"_blank\" title=\"Anemic Domain Model\">Anemic Domain Entity</a>. With a maturer domain model the entity should be modeled as immutable object, created only via a constructor or a Factory, containing only getters and behavior methods. Such a domain object shouldn\'t be definitely exposed to the client\'s view (like we are doing), but should be transformed into a <a href=\"https://en.wikipedia.org/wiki/Data&#95;transfer&#95;object\" target=\"_blank\" title=\"DTO\">DTO</a>.</p> \r\n<p><code><strong>domain/Article.php</strong></code> </p> \r\n<pre class=\"brush: php\">class Article { \r\n    public $id;\r\n    public $title;\r\n    public $summary;\r\n    public $body;\r\n    public $createdAt;\r\n    \r\n    public $categoryId;\r\n    \r\n    public $author;         \r\n}\r\n\r\nclass ArticleAuthor {  \r\n    public $id;\r\n    public $name;\r\n    public $email;\r\n}&nbsp;</pre> \r\n<p>Why is there the class <code>ArticleAuthor</code>, why don\'t we create a separate class <em>Author</em> for that purpose? Well, we just don\'t need all the data of the author, an article needs only few of them. If, in the future, more attributes will be added to the <em>Author</em> entity, the Article structure should stay untouched. The object of the class <code>ArticleAuthor</code> is an <a href=\"https://martinfowler.com/bliki/ValueObject.html\" target=\"_blank\" title=\"Value Object\">Value Object</a> and is accessible only thru the aggregate\'s root. So remains the Article consistent event when the Author entity changes its structure. Instead of ArticleAuthor we can use the name Author within different namespaces (<code>articles</code>&nbsp;and&nbsp;<code>authors</code>).</span></p> \r\n<h2>Persistence</h2> \r\n<p>According the DDD theory, <strong>each aggregate has a matching repository</strong>.&nbsp;The repository is the mechanism you should use to retrieve and persist aggregates. Obviously, we will persist the data into a database, but the <strong>persistence is a point of decision</strong> which could be (and should be) made at the latest possible point. And this is exactly what a repository makes possible.</p> \r\n<p><code><strong>domain/ArticleRepo.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/Article.php\';\r\n\r\ninterface ArticleRepo { \r\n    public function fetchAll($categoryId, $authorId, $start, $limit);    \r\n    public function fetchOne($id);    \r\n    public function create(Article $article);    \r\n    public function update($id, Article $article);    \r\n    public function delete($id);\r\n}</pre> \r\n<p>This <strong>interface decouples the client code</strong> (the service) from the persistence decision. It can be for example implemented as an in-memory storage in the early phases of the development.</p> \r\n<p>We create a <a href=\"http://php.net/manual/book.pdo.php\" title=\"PDO\">PDO</a>-based implementation:</p> \r\n<p><code><strong>infrastructure/ArticleRepoPDO.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/Article.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/ArticleRepo.php\';\r\n\r\nclass ArticleRepoPDO implements ArticleRepo {\r\n \r\n    private $conn;\r\n    \r\n    private $articles&#95;table = \'articles\';\r\n    private $articles&#95;categories&#95;table = \'categories\';\r\n    private $authors&#95;table = \'authors\';\r\n  \r\n    public function &#95;&#95;construct(PDO $conn){                                           \r\n        $this-&gt;conn = $conn;\r\n    }\r\n    \r\n    function fetchAll($categoryId = null, $authorId = null, $start = 0, $limit = 10) {   \r\n        $q = \"SELECT a.id, a.title, a.summary, a.createdAt, a.categoryId, a.authorId, au.name authorName, au.email authorEmail\r\n                FROM {$this-&gt;articles&#95;table} a\r\n                    LEFT JOIN {$this-&gt;authors&#95;table} au ON a.authorId = au.id\r\n                WHERE 1=1 \";\r\n                \r\n        $params = array(\'start\' =&gt; (int)$start, \'limit\' =&gt; (int)$limit);\r\n\r\n        if ($categoryId) {\r\n            $q .= \" AND a.categoryid = :categoryId\";\r\n            $params[\'categoryId\'] = (int)$categoryId;\r\n        }\r\n        if ($authorId) {\r\n            $q .= \" AND a.authorId = :authorId\";\r\n            $params[\'authorId\'] = (int)$authorId;\r\n        }                    \r\n                    \r\n        $q .=\"  ORDER BY a.createdAt DESC, a.id DESC\r\n                LIMIT :start,:limit\";\r\n        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);\r\n        \r\n        foreach ($params as $param =&gt; $value) {\r\n            $stmt-&gt;bindValue($param, $value, PDO::PARAM&#95;INT);\r\n        }\r\n        \r\n        $stmt-&gt;execute();\r\n        \r\n        $articles = array();  \r\n               \r\n        while ($row = $stmt-&gt;fetch(PDO::FETCH&#95;ASSOC)){\r\n          $article = new Article();\r\n          \r\n          $article-&gt;id = (int)$row[\'id\'];\r\n          $article-&gt;title = $row[\'title\'];\r\n          $article-&gt;summary = $row[\'summary\'];\r\n          $article-&gt;createdAt = $row[\'createdAt\'];\r\n          $article-&gt;categoryId = (int)$row[\'categoryId\'];\r\n          \r\n          $article-&gt;author = new ArticleAuthor();\r\n          $article-&gt;author-&gt;id = (int)$row[\'authorId\'];\r\n          $article-&gt;author-&gt;name = $row[\'authorName\'];\r\n          $article-&gt;author-&gt;email = $row[\'authorEmail\'];\r\n          \r\n          array&#95;push($articles, $article);\r\n        }\r\n             \r\n        return $articles;\r\n    }\r\n    \r\n    public function fetchOne($id) {\r\n        $q = \"SELECT a.id, a.title, a.summary, a.body, a.createdAt, a.categoryId, a.authorId, au.name authorName, au.email authorEmail\r\n                FROM {$this-&gt;articles&#95;table} a\r\n                    LEFT JOIN {$this-&gt;authors&#95;table} au ON a.authorId = au.id\r\n                WHERE a.id = :id \";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n        \r\n        $article = null;  \r\n               \r\n        if ($row = $stmt-&gt;fetch(PDO::FETCH&#95;ASSOC)){\r\n          $article = new Article();\r\n          \r\n          $article-&gt;id = (int)$row[\'id\'];\r\n          $article-&gt;title = $row[\'title\'];\r\n          $article-&gt;summary = $row[\'summary\'];\r\n          $article-&gt;body = $row[\'body\'];\r\n          $article-&gt;createdAt = $row[\'createdAt\'];\r\n          $article-&gt;categoryId = (int)$row[\'categoryId\'];\r\n          \r\n          $article-&gt;author = new ArticleAuthor();\r\n          $article-&gt;author-&gt;id = (int)$row[\'authorId\'];\r\n          $article-&gt;author-&gt;name = $row[\'authorName\'];\r\n          $article-&gt;author-&gt;email = $row[\'authorEmail\'];\r\n        }\r\n             \r\n        return $article;\r\n    }\r\n    \r\n    public function create($article) {\r\n        $q = \"INSERT INTO {$this-&gt;articles&#95;table} (id, title, summary, body, createdAt, categoryId, authorId)\r\n                VALUES (0, \r\n                  \'{$article-&gt;title}\', \r\n                  \'{$article-&gt;summary}\', \r\n                  \'{$article-&gt;body}\', \r\n                  \'{$article-&gt;createdAt}\', \r\n                  {$article-&gt;categoryId}, \r\n                  {$article-&gt;authorId}\r\n                )\";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;execute(); \r\n        \r\n        $article-&gt;id = $this-&gt;conn-&gt;lastInsertId();\r\n        \r\n        return $article;   \r\n    }\r\n    \r\n    public function update($id, $article) {\r\n        $q = \"UPDATE {$this-&gt;articles&#95;table}\r\n                SET title = \'{$article-&gt;title}\', \r\n                    summary = \'{$article-&gt;summary}\', \r\n                    body = \'{$article-&gt;body}\', \r\n                    createdAt = \'{$article-&gt;createdAt}\', \r\n                    categoryId = {$article-&gt;categoryId}, \r\n                    authorId = {$article-&gt;authorId}\r\n                WHERE id = :id\";\r\n                        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);        \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n                \r\n        $count = $stmt-&gt;rowCount();        \r\n        return $count &gt; 0;   \r\n    }\r\n    \r\n    public function delete($id) {\r\n        $article = $this-&gt;fetchOne($id);\r\n        if ($article === null) {\r\n            return false;\r\n        }\r\n        \r\n        $q = \"DELETE FROM {$this-&gt;articles&#95;table} WHERE id = :id \";\r\n        \r\n        $stmt = $this-&gt;conn-&gt;prepare($q);                                  \r\n        $stmt-&gt;bindValue(\'id\', (int)$id, PDO::PARAM&#95;INT);        \r\n        $stmt-&gt;execute();\r\n        \r\n        $count = $stmt-&gt;rowCount();        \r\n        return $count &gt; 0;\r\n    }\r\n}&nbsp;</pre> \r\n<p>Constructor of the repository class needs a PDO database connection. We deal with that with a help of the <em>Factory</em> pattern.</p> \r\n<p>We create a <code>Database</code> interface which must be implemented by all the persistence providers, MySQL in our case:</p> \r\n<p><code><strong>infrastructure/db/Database.php</strong></code></p> \r\n<pre class=\"brush: php\">interface Database { \r\n    public function getConnection();  // returns a PDO connection object\r\n}&nbsp;</pre> \r\n<p><code><strong>infrastructure/db/DatabaseMySql.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/Database.php\';\r\n\r\nclass DatabaseMySql implements Database {    \r\n    private $conn;\r\n    \r\n    function &#95;&#95;construct($host, $db, $username, $password) {\r\n        try {\r\n            $this-&gt;conn = new PDO(\"mysql:host={$host};dbname={$db}\", $username, $password);\r\n            $this-&gt;conn-&gt;setAttribute(PDO::ATTR&#95;ERRMODE, PDO::ERRMODE&#95;EXCEPTION);\r\n            $this-&gt;conn-&gt;setAttribute(PDO::MYSQL&#95;ATTR&#95;FOUND&#95;ROWS, true);\r\n            $this-&gt;conn-&gt;exec(\"set names utf8\");\r\n            \r\n        } catch(PDOException $e){\r\n            throw new Exception(\'Connection error: \' . $e-&gt;getMessage(), 0, $e);\r\n        }\r\n    }\r\n \r\n    public function getConnection() {\r\n        return $this-&gt;conn;\r\n    }\r\n}&nbsp;</pre> \r\n<p>Our factory has a static method for getting the right PDO connection:</p> \r\n<p><code><strong>infrastructure/db/DatabaseFactory.php</strong></code></p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/DatabaseMySql.php\';\r\n\r\nclass DatabaseFactory {  \r\n  public static function getDatabase($type, $host, $db, $username, $password) {\r\n    switch ($type) {         \r\n      case \'mysql\':\r\n        return new DatabaseMySql($host, $db, $username, $password);        \r\n      default:\r\n        throw new Exception(\'Unknown database type: \' . $type);\r\n    } \r\n  }\r\n}</pre> \r\n<p>The factory can be used as following:</p> \r\n<pre class=\"brush: php\">$db = DatabaseFactory::getDatabase(\'mysql\', \'localhost\', \'blog\', \'root\', \'1234\');</pre> \r\n<h2>Controller</h2> \r\n<p>The term comes from <a href=\"https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller\" target=\"_blank\" title=\"MVC\">MVC</a> design pattern. The responsibility of the controller in the application is to<strong> process the user input into output</strong>. This is the right place where the parameters should be validated (data types, range etc., the business validation is not meant here - that belongs to the domain services), proceeded and transformed back to the client.</p> \r\n<p>The important point here is the controller shouldn\'t know anything about the fact, that we\'re developing a web application. The web-specifics must be present only in the application itself, as we show in a moment.&nbsp;</p> \r\n<p><code><strong>application/ArticleController.php</strong></code> </p> \r\n<pre class=\"brush: php\">require&#95;once &#95;&#95;DIR&#95;&#95; . \'/../domain/ArticleRepo.php\';\r\n\r\nclass ArticleController {\r\n\r\n  private $repo;\r\n\r\n  public function &#95;&#95;construct(ArticleRepo $repo){                                           \r\n    $this-&gt;repo = $repo;\r\n  }\r\n\r\n  public function detailRequest($id) {\r\n    $article = $this-&gt;repo-&gt;fetchOne((int)$id);\r\n    \r\n    return $article;\r\n  }\r\n  \r\n  public function listRequest($params) {\r\n    $limit = 10;\r\n    \r\n    $categoryId = $this-&gt;getIfSet($params, \'categoryId\');   \r\n    $authorId = $this-&gt;getIfSet($params, \'authorId\');   \r\n    $page = $this-&gt;getIfSet($params, \'page\', 0);\r\n    \r\n    $articles = $this-&gt;repo-&gt;fetchAll((int)$categoryId, (int)$authorId, $page &#42; $limit, $limit);\r\n    \r\n    return $articles;\r\n  }\r\n  \r\n  public function createRequest($params) {\r\n    $article = new Article();\r\n    $article-&gt;title = $params[\'title\'];\r\n    $article-&gt;summary = $params[\'summary\'];\r\n    $article-&gt;body = $params[\'body\'];\r\n    $article-&gt;createdAt = $params[\'createdAt\'];\r\n    $article-&gt;categoryId = (int)$params[\'categoryId\'];\r\n    $article-&gt;authorId = (int)$params[\'authorId\'];\r\n    \r\n    if (!$article-&gt;title || !$article-&gt;summary || !$article-&gt;body || !$article-&gt;createdAt || !$article-&gt;categoryId || !$article-&gt;authorId) {\r\n      return array(\'error\' =&gt; \'Incorrect payload.\');\r\n    }\r\n    \r\n    $article = $this-&gt;repo-&gt;create($article);\r\n    \r\n    return (int)$article-&gt;id;\r\n  }\r\n  \r\n  public function updateRequest($id, $params) {\r\n    $article = new Article();\r\n    $article-&gt;title = $params[\'title\'];\r\n    $article-&gt;summary = $params[\'summary\'];\r\n    $article-&gt;body = $params[\'body\'];\r\n    $article-&gt;createdAt = $params[\'createdAt\'];\r\n    $article-&gt;categoryId = (int)$params[\'categoryId\'];\r\n    $article-&gt;authorId = (int)$params[\'categoryId\'];\r\n    \r\n    return $this-&gt;repo-&gt;update($id, $article);\r\n  }\r\n  \r\n  public function deleteRequest($id) {  \r\n    return $this-&gt;repo-&gt;delete($id);\r\n  }\r\n  \r\n  // ///////// HELPER FUNCTIONS /////////////////////////////////////\r\n  \r\n  private function getIfSet($params, $var, $def = null) {\r\n    return isset($params[$var]) ? $params[$var] : $def;\r\n  }\r\n}</pre> \r\n<h2>Putting It All Together&nbsp;</h2> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/BlogArticlesDesign.png\" alt=\"Blog Articles Architecture\" width=\"100%\"/></p> \r\n<p>The picture above shows the service components, double-lines represent layers boundaries. As you can see, <strong>all the inter-boundary communication is through interfaces</strong>, which define an API of every layer.</p> \r\n<p>Finally we have all the blocks we need to build our service.</p> \r\n<p>To keep things clean, we create a database configuration in a separate file:</p> \r\n<p><code><strong>config/db.config.php</strong></code></p> \r\n<pre>define(\'DB&#95;TYPE\', \'mysql\'); \r\ndefine(\'DB&#95;HOST\', \'localhost\'); \r\ndefine(\'DB&#95;NAME\', \'blog\'); \r\ndefine(\'DB&#95;USER\', \'root\'); \r\ndefine(\'DB&#95;PASS\', \'1234\');&nbsp;</pre> \r\n<p>This configuration will be loaded withing the application code.</p> \r\n<p>The application&nbsp;code is here to <strong>serve the request</strong> and&nbsp;<strong>assembly the components</strong>, this the only &quot;dirty&quot; code doing all the injections; a dependency-injection framework could be used here as well:</p> \r\n<p><code><strong>articles.php</strong></code></p> \r\n<pre class=\"brush: php\">header(\"Access-Control-Allow-Origin: &#42;\");\r\nheader(\"Content-Type: application/json; charset=UTF-8\");\r\n  \r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/config/db.config.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/application/ArticleController.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/infrastructure/ArticleRepoPDO.php\';\r\nrequire&#95;once &#95;&#95;DIR&#95;&#95; . \'/infrastructure/db/DatabaseFactory.php\';\r\n\r\n$db = DatabaseFactory::getDatabase(DB&#95;TYPE, DB&#95;HOST, DB&#95;NAME, DB&#95;USER, DB&#95;PASS);\r\n\r\n$repo = new ArticleRepoPDO($db-&gt;getConnection());\r\n\r\n$controller = new ArticleController($repo);\r\n\r\n$response = null;\r\n\r\nswitch ($&#95;SERVER[\'REQUEST&#95;METHOD\']) {  \r\n  case \'GET\':\r\n    if (isset($&#95;GET[\'id\'])) {\r\n      $response = $controller-&gt;detailRequest($&#95;GET[\'id\']);\r\n      \r\n    } else {\r\n      $response = $controller-&gt;listRequest($&#95;GET);\r\n    }\r\n    if ($response === null) {\r\n      http&#95;response&#95;code(404);\r\n      \r\n    } else {\r\n      echo json&#95;encode($response);\r\n    }\r\n    break;\r\n    \r\n  case \'POST\':\r\n    $&#95;DATA = parseRequestData();\r\n    $response = $controller-&gt;createRequest($&#95;DATA);\r\n    \r\n    if ($response === null) {\r\n      http&#95;response&#95;code(400);       \r\n    \r\n    } else {\r\n      http&#95;response&#95;code(201);\r\n      echo json&#95;encode($response);\r\n    }\r\n    break;                           \r\n  \r\n  case \'PUT\':\r\n    if (!isset($&#95;GET[\'id\'])) {\r\n      http&#95;response&#95;code(400);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Missing \"id\" parameter.\'));\r\n    }\r\n  \r\n    $&#95;DATA = parseRequestData();\r\n    $response = $controller-&gt;updateRequest($&#95;GET[\'id\'], $&#95;DATA);\r\n    \r\n    if ($response === false) {\r\n      http&#95;response&#95;code(404);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Article not found.\'));\r\n    } else {\r\n      http&#95;response&#95;code(204);\r\n    }\r\n    break;\r\n    \r\n  case \'DELETE\':\r\n    if (!isset($&#95;GET[\'id\'])) {\r\n      http&#95;response&#95;code(400);\r\n      header(\"Location: {$&#95;SERVER[\'REQUEST&#95;URI\']}/{$response}\");\r\n    }\r\n    \r\n    $response = $controller-&gt;deleteRequest($&#95;GET[\'id\']);\r\n    \r\n    if ($response === false) {\r\n      http&#95;response&#95;code(404);\r\n      echo json&#95;encode(array(\'error\' =&gt; \'Article not found.\'));\r\n    } else {\r\n      http&#95;response&#95;code(204);\r\n    } \r\n    break;\r\n    \r\n  case \'OPTIONS\':\r\n    header(\'Allow: GET POST PUT DELETE OPTIONS\');\r\n    break;\r\n        \r\n  default:\r\n    http&#95;response&#95;code(405);\r\n    header(\'Allow: GET POST PUT DELETE OPTIONS\');\r\n}\r\n\r\n// ///////// HELPER FUNCTIONS /////////////////////////////////////\r\n\r\nfunction parseRequestData() {\r\n  $contentType = explode(\';\', $&#95;SERVER[\'CONTENT&#95;TYPE\']);\r\n  $rawBody = file&#95;get&#95;contents(\'php://input\');\r\n  $data = array();\r\n  \r\n  if (in&#95;array(\'application/json\', $contentType)) {\r\n    $data = json&#95;decode($rawBody, true);\r\n    \r\n  } else {\r\n    parse&#95;str($data, $data);\r\n  }\r\n  \r\n  return $data;\r\n}</pre> \r\n<h2>Production</h2> \r\n<p>In production we want our API to be agnostic of the underlying technology. We can achieve that via setting rules of the HTTP server:</p> \r\n<p><code><strong>.htaccess</strong></code></p> \r\n<pre class=\"brush: plain\">RewriteEngine On\r\nRewriteRule ^articles$ articles.php [NC,L]\r\nRewriteRule ^articles/([0-9]+)$ articles.php?id=$1 [NC,L]&nbsp;</pre> \r\n<p>And now we can call the endpoint without the <code>.php</code> suffix.</p> \r\n<h2>Usage&nbsp;</h2> \r\n<p>The usage is pretty straight-forward:</p> \r\n<pre>curl http://localhost/articles\r\ncurl http://localhost/articles?categoryId=1\r\ncurl http://localhost/articles?authorId=1\r\ncurl http://localhost/articles?categoryId=1&amp;authorId=1\r\n\r\ncurl http://localhost/articles/1\r\n\r\ncurl http://localhost/articles -X POST -H \"Content-Type: application/json\" -d @data.json\r\n\r\ncurl http://localhost/articles/4 -X PUT -H \"Content-Type: application/json\" -d @data.json\r\n\r\ncurl http://localhost/articles/4 -X DELETE&nbsp;</pre> \r\n<p><code><strong>data.json</strong></code></p> \r\n<pre class=\"brush: plain\">{ \"title\": \"New article\", \r\n  \"summary\": \"Lorem ipsum\", \r\n  \"body\": \"Lorem ipsum dolor sit amet\", \r\n  \"createdAt\": \"2018-05-03\", \r\n  \"categoryId\": 1, \r\n  \"authorId\": 1 \r\n}&nbsp;</pre> \r\n<h2>Source Code</h2> \r\n<p>You can find the whole project source code on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/php-rest-articles\" title=\"Source code\">GitHub</a>.</p> \r\n<p>Enjoy!</p> ', 'Microservices,REST,PHP', 'false', 'true', 1, 1),
(34, 'building-a-simple-rest-api', 1525592000, 'Building a simple REST API', '<p>In my previous article I showed how to build a simple RESTful microservice. For sake of simplicity I skipped some good practices for building REST APIs and I feel a bit guilty about that.</p> ', '<p>I think that topic deserves its own article. And here we go... we take a look at the previous API and try to do things better.</p>\r\n<h2>API Requirements</h2> \r\n<p>What is the business case of our API? It\'s a simple <strong>public</strong> API for a blog (actually this blog). The public here means, it allow the unauthorized uses to read the content. Not more, not less.</p> \r\n<ul> \r\n<li>information about the blog (name, description, menu items, ...)</li> \r\n<li>collection of articles (posts)</li> \r\n<li>filtered collection of articles&nbsp;(by the category and author)</li> \r\n<li>pagination of the articles (first, last, next and previous page)</li> \r\n<li>detail of an article (the whole text content)</li> \r\n</ul> \r\n<p>The administration API is built out of this scope and I will maybe talk about that in another article later. For the administration API we can expect:</p> \r\n<ul> \r\n<li>user management</li> \r\n<li>blog management</li> \r\n<li>articles management&nbsp;</li> \r\n</ul> \r\n<h2>Problems with the Old API</h2> \r\n<p>When we don\'t talk about the missing AuthN/AuthZ, we find a lack of <strong>hyper-navigation in the API</strong>. Without knowing how the endpoints look like (even it follows the REST recommendations) there is no way <strong>how to navigate the client automatically</strong>.</p> \r\n<p>Calling the API self results to the response:</p> \r\n<pre>GET /api/ HTTP/1.1\r\nHTTP/1.1 403 Forbidden</pre> \r\n<p>This is not so bad, because we built a microservice API only for the articles management. In this case, we need a billboard endpoint. The billboard shows information about the blog and navigation to other endpoint.</p> \r\n<p>Another problem is with the navigation, resp. no navigation, within the API. When we call the <code>/api/articles</code> endpoint, the response look like:</p> \r\n<pre class=\"brush: json\">[\r\n  {\r\n    \"id\" : 4,\r\n    \"title\" : \"New article\",\r\n    \"summary\" : \"Lorem ipsum\",\r\n    \"body\" : null,\r\n    \"createdAt\" : \"2018-05-03\",\r\n    \"categoryId\" : 1,\r\n    \"author\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\"\r\n    }\r\n  },\r\n  ... another articles come here\r\n]</pre> \r\n<p>First problem is with the structure itself. What does the attribute <code>body</code> there? It\'s always null because we expose the whole business object into the API even when some attributes are never set by the underlying service.</p> \r\n<p>Second, the response contains all the data, but we have no idea how to get the detail of an article, next page of the collection or how to filter the articles by the category ID.</p> \r\n<h2>New API&nbsp;</h2> \r\n<h3>The Billboad</h3> \r\n<p>To&nbsp;fulfill our needs we can design the response of the billboard endpoint <code>/api</code> as following:</p> \r\n<pre class=\"brush: json\">{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api\",\r\n  \r\n  \"title\" : \"My Blog\",\r\n  \"description\" : \"A small blog about programming.\",\r\n  \r\n  \"categories\" : [ {\r\n      \"id\" : 1, \r\n      \"name\" : \"Programming\"\r\n    }, {\r\n      \"id\" : 2,\r\n      \"name\" : \"Another stuff\"\r\n    } ],\r\n\r\n  \"authors\" : [ {\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\",\r\n      \"id\" : 1 \r\n    } ],\r\n      \r\n  \"links\" : [ {\r\n      \"rel\" : \"articles\",\r\n      \"href\" : \"/api/articles\"\r\n    } ] }</pre> \r\n<p> </p> \r\n<p>The JSON document structure is pretty straightforward: the top-level entities are the blog content elements (<em>categories</em>&nbsp;and <em>authors</em>).</p> \r\n<p>The navigation is represented by the entity&nbsp;<code>links</code>, the&nbsp;<code>rel</code> attribute tells the logical name of the item and <code>href</code> attribute points to the resource URL.</p> \r\n<h3>The Articles Collection</h3> \r\n<p>Now, what we take a look at the <code>/api/articles</code> endpoint:&nbsp;</p> \r\n<pre class=\"brush: json\">{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api/articles\",\r\n  \r\n  \"articles\" : [ {\r\n      \"href\" : \"/api/articles/4\",\r\n      \"data\" : {\r\n        \"id\" : 4,\r\n        \"title\" : \"New article\",\r\n        \"summary\" : \"Lorem ipsum\",\r\n        \"createdAt\" : \"2018-05-03\",\r\n        \"category\" : {\r\n          \"id\" : 1,\r\n          \"name\" : \"Programming\"\r\n        },\r\n        \"author\" : {\r\n          \"id\" : 1,\r\n          \"name\" : \"Tomas Tulka\",\r\n          \"email\" : \"tomas.tulka@gmail.com\"\r\n        }\r\n      }      \r\n    },\r\n    ... another articles    \r\n  ],\r\n  \r\n  \"queries\" : [ {\r\n      \"rel\" : \"page\",\r\n      \"name\" : \"page\"\r\n    }, {\r\n      \"rel\" : \"category\",\r\n      \"name\" : \"categoryId\",\r\n    }, {\r\n      \"rel\" : \"author\",\r\n      \"name\" : \"authorId\",\r\n    } ],\r\n  \r\n  \"links\" : [ {\r\n      \"rel\" : \"next\",\r\n      \"href\" : \"/api/articles?page=3\"\r\n    }, {\r\n      \"rel\" : \"previous\",\r\n      \"href\" : \"/api/articles?page=1\"\r\n    }, {\r\n      \"rel\" : \"first\",\r\n      \"href\" : \"/api/articles\"\r\n    }, {\r\n      \"rel\" : \"last\",\r\n      \"href\" : \"/api/articles?page=10\"\r\n    } ] }</pre> \r\n<p> </p> \r\n<p> </p> \r\n<p> </p> \r\n<p>The endpoint have a set of query parameters listed in the <code>queries</code>. Practically it means you can filter the articles by calling the same endpoint with query parameters like <code>/api/articles?categoryId=1&amp;author=1</code>. The values for every query type are known already from the billboard response.</p> \r\n<p>The entity&nbsp;<code>links</code> brings the navigation (pagination).</p> \r\n<h3>The Article Detail&nbsp;</h3> \r\n<p>Last but not least is the <code>/api/articles/{id}</code> article detail endpoint:</p> \r\n<pre class=\"brush: json\">{\r\n  \"version\" : \"1.0\",\r\n  \"href\" : \"/api/articles/4\",\r\n  \r\n  \"data\" : {\r\n    \"id\" : 4,\r\n    \"title\" : \"New article\",\r\n    \"summary\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit...\",\r\n    \"body\" : \"Lorem ipsum\",\r\n    \"createdAt\" : \"2018-05-03\",\r\n    \"category\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Programming\"\r\n    },\r\n    \"author\" : {\r\n      \"id\" : 1,\r\n      \"name\" : \"Tomas Tulka\",\r\n      \"email\" : \"tomas.tulka@gmail.com\"\r\n    }\r\n  } }</pre> \r\n<h2>Further Reading</h2> \r\n<p> </p> \r\n<ul> \r\n<li>The probably best source is the great book&nbsp;<a href=\"http://shop.oreilly.com/product/0636920028468.do\" target=\"_blank\" title=\"RESTful Web APIs book\">RESTful Web APIs</a> by Leonard Richardson.</li> \r\n<li>If you are to build a more sophisticated search API you should consider using&nbsp;<a href=\"https://graphql.org/\" target=\"_blank\" title=\"GraphQL\">GraphQL</a>.&nbsp;</li> \r\n</ul> \r\n<h2>Source Code</h2> \r\n<p>You can find the current implementation (PHP) on <a href=\"https://github.com/net21cz/blog-backend\" title=\"Source code\">GitHub</a>.</p> \r\n<p>Enjoy!&nbsp;&nbsp;</p> \r\n<p> </p>', 'API,REST', 'false', 'true', 1, 1),
(35, 'multiple-ldap-servers-integration', 1528815000, 'Multiple LDAP Servers Integration', '<p>Let\'s integrate a few LDAP servers!</p>', '<p>When an administative solution (like <a href=\"https://technet.microsoft.com/pt-pt/library/how-global-catalog-servers-work.aspx\" target=\"_blank\">Global Catalog</a>) is not possible or wanted and we have to integrate more LDAP servers under one hood there is a simple way how to do it with <a href=\"https://github.com/TremoloSecurity/MyVirtualDirectory\" taregt=\"_blank\">MyVirtualDirectory</a>.</p>\r\n\r\n<p>MyVirtualDirectory (<strong>MyVD</strong>) offers much more than an integration of multiple LDAP servers, actually <em>anything</em> could be exposed as a LDAP service via MyVD. In this tutorial we focus only on LDAP.</p> \r\n<h2>Simple LDAP Integration</h2> \r\n<p>We begin with a simple example of two LDAP servers integration.</p> \r\n<p>We have one LDAP running in <code>server1.com</code> network on the port <code>398</code> and another running in <code>server2.com</code> on the same port.</p> \r\n<p>We integrate the server in out local network on the port <code>50983</code> as shows the following picture:</p> \r\n<p align=\"center\"><img src=\"/storage/myvd-1.png\" alt=\"Simple LDAP Integration\" /></p> \r\n<p>First, let\'s set up the MyVD server (HTTP in our case).</p> \r\n<pre>server.listener.port=50983\r\n</pre> \r\n<p>That\'s it. Now the integration of the servers:</p> \r\n<pre>server.nameSpaces=server1,server2\r\n \r\nserver.server1.nameSpace=dc=server1,dc=com\r\nserver.server1.weight=100\r\nserver.server1.chain=ldap\r\nserver.server1.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server1.ldap.config.host=server1.com\r\nserver.server1.ldap.config.port=389\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=com\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=com\r\nserver.server1.ldap.config.proxyPass=123\r\n \r\nserver.server2.nameSpace=dc=server2,dc=com\r\nserver.server2.weight=100\r\nserver.server2.chain=ldap\r\nserver.server2.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server2.ldap.config.host=server2.com\r\nserver.server2.ldap.config.port=389\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=com\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=com\r\nserver.server2.ldap.config.proxyPass=123\r\n</pre> \r\n<h2>LDAP Client</h2> \r\n<p>As a non-trivial client example let\'s consider the <a href=\"https://spring.io/guides/gs/authenticating-ldap\" target=\"_blank\">Spring Security LDAP</a>.</p> \r\n<p>Simple said, the Spring Security LDAP does two search queries to the LDAP server:</p> \r\n<ul> \r\n<li>Get a user DN by its username (<code>(uid=&lt;username&gt;)</code>),</li> \r\n<li>get groups by user\'s DN (<code>(member=&lt;userDN&gt;)</code>).</li> \r\n</ul> \r\n<p>Spring allow the user to redefine the search bases and filters for different LDAP structures and uses placeholders (so the filter looks like <code>(uid={0})</code>). For the MyVD setting above let\'s set our client as following:</p> \r\n<p>\r\n<table border=\"1\" cellpadding=\"5\" cellspacing=\"0\"> \r\n<tbody>\r\n<tr>\r\n<td>user search base</td>\r\n<td><code>dc=com</code></td>\r\n</tr> \r\n<tr>\r\n<td>user search filter</td>\r\n<td><code>(uid={0})</code></td>\r\n</tr> \r\n<tr>\r\n<td>groups search base</td>\r\n<td><code>dc=com</code></td>\r\n</tr> \r\n<tr>\r\n<td>groups search filter</td>\r\n<td><code>(uid={0})</code></td>\r\n</tr> \r\n</tbody>\r\n</table>\r\n</p>\r\n \r\n<h2>Namespaces Integration</h2> \r\n<p>In our first example we had two server with one base in common <code>dc=com</code>. But what happend when we have to integrate multiple server with different bases? This is what the property <code>server.&lt;server&gt;.nameSpace</code> is meant for.</p> \r\n<p align=\"center\"><img src=\"/storage/myvd-2.png\" alt=\"Namespaces Integration\" /></p> \r\n<pre>...\r\nserver.server1.nameSpace=dc=mycom,dc=com\r\n...\r\nserver.server1.ldap.config.host=server1.org\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=org\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=org\r\n...\r\nserver.server2.nameSpace=dc=mycom,dc=com\r\n...\r\nserver.server2.ldap.config.host=server2.net\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=net\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=net\r\n...\r\n</pre> \r\n<p>Now we can change our search base to <code>dc=mycom,dc=com</code>. Unfortunately this doesn\'t work. The problem is the DN of the result user entity is mapped to the integration namespace. It means for the username <code>testuser1</code> we get instead of <code>uid=testuser1,ou=people,dc=server1,dc=com</code> a DN <code>uid=testuser1,ou=people,dc=mycom,dc=com</code> and that doesn\'t match the value of the <code>member</code> attribute of group entities.</p> \r\n<p>MyVD brings a solution in <a href=\"https://github.com/TremoloSecurity/MyVirtualDirectory/blob/master/doc/myvd.asc#mapping-inserts\" target=\"_blank\">Mapping Inserts</a>:</p> \r\n<pre>...\r\nserver.server1.chain=dnMapper,ldap\r\n...\r\nserver.server1.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server1.dnMapper.config.dnAttribs=member\r\nserver.server1.dnMapper.config.remoteBase=dc=server1,dc=org\r\nserver.server1.dnMapper.config.localBase=dc=mycom,dc=com\r\n...\r\nserver.server2.chain=dnMapper,ldap\r\n...\r\nserver.server2.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server2.dnMapper.config.dnAttribs=member\r\nserver.server2.dnMapper.config.remoteBase=dc=server2,dc=net\r\nserver.server2.dnMapper.config.localBase=dc=mycom,dc=com\r\n...\r\n</pre> \r\n<p>The DN Attribute Mapper maps specified attributted back to the original namespace. Now the search works again.</p> \r\n<h2>Integration of Heterogeneous Services</h2> \r\n<p>As we mentioned in the beginning MyVD provides the possibility to integrate different services. So what happend when we integrate a standard LDAP server (e.g. <a href=\"https://www.openldap.org/\" target=\"_blank\">OpenLDAP</a>) and Active Directory (<strong>AD</strong>)? We are still on LDAP field but the details are different. For example AD\'s user entity holds the username in an <code>sAMAccountName</code> attribute. This means we have to integrate those heterogeneous attributes to be searchable with one client search query. \r\n</p>\r\n<p align=\"center\"><img src=\"/storage/myvd-3.png\" alt=\"Integration of Heterogeneous Services\" /></p> \r\n<p>Sure, we can compose the search filter like <code>(|(uid={0})(sAMAccountName={0}))</code>, which will work fine, but it <strong>exposes implementation details</strong> to the client and breaks so the encapsulation principle. The client shouldn\'t know anything about the backend server, it should treat the service as a <strong>single LDAP server</strong>.</p> \r\n<p>Fortunately, there is MyVD\'s Attribute Mapper:</p> \r\n<pre>...\r\nserver.server2.chain=uidMapper,dnMapper,ldap\r\n...\r\nserver.server2.uidMapper.className=net.sourceforge.myvd.inserts.mapping.AttributeMapper\r\nserver.server2.uidMapper.config.mapping=sAMAccountName=uid\r\n...\r\n</pre> \r\n<p>Now works everything fine. The whole MyVD configuration file:</p> \r\n<pre>server.listener.port=50983\r\n \r\nserver.nameSpaces=server1,server2\r\n \r\nserver.server1.nameSpace=dc=mycom,dc=com\r\nserver.server1.weight=100\r\nserver.server1.chain=dnMapper,ldap\r\nserver.server1.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server1.ldap.config.host=server1.org\r\nserver.server1.ldap.config.port=389\r\nserver.server1.ldap.config.remoteBase=dc=server1,dc=org\r\nserver.server1.ldap.config.proxyDN=uid=testuser1,ou=people,dc=server1,dc=org\r\nserver.server1.ldap.config.proxyPass=123\r\n \r\nserver.server1.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server1.dnMapper.config.dnAttribs=member\r\nserver.server1.dnMapper.config.remoteBase=dc=server1,dc=org\r\nserver.server1.dnMapper.config.localBase=dc=mycom,dc=com\r\n \r\nserver.server2.nameSpace=dc=mycom,dc=com\r\nserver.server2.weight=100\r\nserver.server2.chain=uidMapper,dnMapper,ldap\r\nserver.server2.ldap.className=net.sourceforge.myvd.inserts.ldap.LDAPInterceptor\r\nserver.server2.ldap.config.host=server2.net\r\nserver.server2.ldap.config.port=389\r\nserver.server2.ldap.config.remoteBase=dc=server2,dc=net\r\nserver.server2.ldap.config.proxyDN=uid=testuser2,ou=people,dc=server2,dc=net\r\nserver.server2.ldap.config.proxyPass=123\r\n \r\nserver.server2.dnMapper.className=net.sourceforge.myvd.inserts.mapping.DNAttributeMapper\r\nserver.server2.dnMapper.config.dnAttribs=member\r\nserver.server2.dnMapper.config.remoteBase=dc=server2,dc=net\r\nserver.server2.dnMapper.config.localBase=dc=mycom,dc=com\r\n \r\nserver.server2.uidMapper.className=net.sourceforge.myvd.inserts.mapping.AttributeMapper\r\nserver.server2.uidMapper.config.mapping=sAMAccountName=uid\r\n</pre> \r\n<p>Happy integrating!</p>', 'LDAP', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(36, 'secure-communication-between-services-in-multitenant-systems', 1530465000, 'Secure Communication between Services in Multitenant Systems', '<p>Implementing a SaaS as a multitenant system brings a lot of benefits. As usual, there are some tradeoffs, too. For example security becomes more complex. Let\'s take a look at possible approaches when implementing security in a multitenant architectrure.</p>', '<p>Let\'s consider a trivial example of a communication between two independent services.</p> \r\n<p><img src=\"/storage/multitenant-security.png\" alt=\"communication between two independent services\" /></p> \r\n<p>Service A and Service B are in different realms untrusted to each other.</p> \r\n<h2>Security Levels</h2> \r\n<p>There are several levels of security when talking about a communication between systems:</p> \r\n<h3>1. Security Level: Don\'t trust anyone</h3> \r\n<p><img src=\"/storage/multitenant-security-level-1.png\" alt=\"1. Security level\" /></p> \r\n<p>This is the most secure model. It can be achive only by an active client establishing the communication.</p> \r\n<p>In this case, services don\'t know anything about each other - the client must know all the communication details and control the whole flow.</p> \r\n<p>If you don\'t trust anyone, you have to do everything by yourself...</p> \r\n<h3>2. Security Level:  In your name</h3> \r\n<p><img src=\"/storage/multitenant-security-level-2.png\" alt=\"2. Security level\" /></p> \r\n<p>In this model, a part of the communication is moved to the service code and the calling service acts partially on behalf of the client.</p> \r\n<p>Everytime the client sends its credentials to the Service A he manifests his trust to the service. The Service A uses client\'s identity to build trust between Service B. The identity can be implemented for example via a security token.</p> \r\n<p>The Service A can\'t communicate with the Service B without client\'s identity so it\'s unable to access any data but the client\'s one.</p> \r\n<h3>3. Security Level:  We\'re all friends</h3> \r\n<p><img src=\"/storage/multitenant-security-level-3.png\" alt=\"3. Security level\" /></p> \r\n<p>On this level you practically loose the multitenant-specific secuirity whatsoever. That could be okay in some cases like logging, collecting anonymous statistics etc., but we have to choose out friends carefully.</p> \r\n<p>If such a kind of trust is supported, the Service A can access any data of any client in the Service B. It can even make an identity up which doesn\'t exist (if the Service B is just blindly accepting the input).</p> \r\n<p>This model can be implemented using <em>API keys</em> or application certificates.</p> \r\n<h3>4. Security Level:  Whatever makes you happy</h3> \r\n<p>Pretty obvious one - the Service B has a public API accesible without any needed trust.</p> \r\n<p>There are definitely valid use-cases, but don\'t make your service public as long as you\'re not 100% sure!</p> \r\n<h2>Real-World Example</h2> \r\n<p>We will show a simple model of a multitenant system based on the <b>second security level</b> as described above.</p> \r\n<p>Let\'s consider a Service A consuming client\'s data and processing them somehow (e.g. saving into a database).</p> \r\n<p>The data can be later used for a post-procesing in the Service B.\r\n</p> \r\n<p>Because the Service A plays only the role of a data source and its business logic is completely independent on the post-processing and the Service B is a general service knowing nothing about its data sources we want to decouple both services as much as possible. Using <em>event-driven architecture</em> makes perfectly sense in this use-case.</p> \r\n<p>We build a plugin Adapter in the Service A to consume the data and forward them to the Service B. The Adapter is conceptually a part of the Service A, but it\'s loosly coupled to it and can be plugged-in/out completely independently on the life-cycle of the service.\r\n\r\n</p> \r\n<p><img src=\"/storage/multitenant-security-example-1.png\" alt=\"Real-World Example 1\" /></p> \r\n<p>When the Client sends data to the Service A (1) he uses a token to authorize himself. The Service A accepts the request, processes it and fires an event including the data (or some reference to them) and authorization token (2). The Adapter reacts on the event and uses the client\'s token to forward the data to the Service B (3).</p> \r\n<p>Gray lines separate three different realms of trust: Client\'s, Service A\'s and B\'s realms. Inside a realm everything trusted, outside the realm nothing is trusted.</p> \r\n<h3>Another example</h3> \r\n<p>If you are developing for the cloud, the following scenario should sound familier to you: You want to process data with a large file attachment. First, you make a request to the service with a security token, a data record is created and a signed upload URL is returned in the response. Second, the client puts the file data to the upload URL.</p> \r\n<p>The Adapter them must save the session information to be able to react to the event when the upload is finished.</p> \r\n<p>To save the token internally in the service is not enough in this situation because the upload can take longer than the token expiration time is. The Adapter have to get a temporary but long enough access to the Service B. This can be implemented using API keys.</p> \r\n<p><img src=\"/storage/multitenant-security-example-2.png\" alt=\"Real-World Example 2\" width=\"100%\" /></p> \r\n<p>When the Client sends metadata (like a file-name) to the Service A (1) he uses a token to authorize himself. The Service A accepts the request, processes it, fires an event including the metadata and security token (2), and returns a signed upload URL in the response. The Adapter reacts on the event, uses the client\'s token to demand an API key from the Service B (3A), then saves the API key internal with the metadata. In parallel the Client uses the upload URL to upload a file data to the storage of the Service A (3B). This fires an event containing the file metadata (4). On the event reacts the Adapter by looking up the API key connected to the metadata (5). The Adapter uses then the API key to upload the file data to the Service B (6).</p> \r\n<h4>What about asynchrony?</h4> \r\n<p>One problem could be the absense of order of the received events (that is natural for event-driven systems). It means, the event &quot;file uploaded&quot; can come before the API key retreiving is done or even (but very unlikely) before the &quot;upload requested&quot; event is caught by the Adapter. A solution here is not to consume the &quot;file uploaded&quot; event to be received again and again until the look-up is successful.</p> \r\n<p>Security first!</p>', 'Cloud,Serverless,Security', 'false', 'false', 1, 2),
(37, 'how-aws-lambda-executes-node-js-functions', 1531317000, 'How AWS Lambda Executes Node.js Functions', '<p>Based on <a href=\"https://stackoverflow.com/questions/51037262/aws-lambda-javascript-sdk-async-handler\" target=\"_blank\">my question</a> on StackOverflow I did a bit investigation about how (likely) are Node.js functions called in AWS Lambda containers.</p>', '<p>First, I have to mention I don\'t have the actual implementation of the Node.js AWS Lambda container executors, so all following is only a simple thought experiment. But it seems to be very likely and can definitely help with understanding <em>how are Node.js handler functions executed in AWS Lambda</em>.</p>\r\n\r\n<p>The problem as described in the question is following:</p>\r\n<p>A simple function with an asynchrony inside is not correctly executed when the handler function is declared as <code>async</code>.</p>\r\n<pre class=\"brush: javascript\">\r\nexports.handler = async (event, context, callback) => {\r\n  setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n</pre>\r\n<p>The result of the execution of this Lambda is <code>null</code>.</p>\r\n<p>When the keyword <code>async</code> is removed from the handler everything works fine and the result is <code>resolved</code> as expected.</p>\r\n\r\n<p>So, what is the difference?</p>\r\n\r\n<p>According <a href=\"https://docs.aws.amazon.com/lambda\" target=\"_blank\">the documentation</a>, the asynchronous handlers are executed without using the callback function and the result of the <code>return</code> is used instead.</p>\r\n\r\n<p>I tried to simulate the executor code and came up with this:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar handler1 = async (event, context, callback) => {\r\n  setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n\r\nvar handler2 = async (event, context, callback) => {\r\n  return \"resolved\"\r\n}\r\n\r\nvar handler3 = (event, context, callback) => {\r\n    setTimeout(() => callback(null, \"resolved\"), 100)\r\n}\r\n\r\n// main thread function\r\n(async function main() {\r\n  var handler = handler1 || handler2 || handler3 // set the handler \r\n\r\n  var call = () => new Promise(async (resolve, reject) => {\r\n    var event = undefined\r\n    var context = undefined\r\n    \r\n    if (isAsync(handler)) {\r\n      try {\r\n        // await the result\r\n        var res = await handler(event, context, (err, succ) => {})\r\n        resolve(res)        \r\n      } catch (err) {\r\n        reject(err)\r\n      }      \r\n    } else {\r\n      // use the callback to get the result\r\n      handler(event, context, function(err, succ) {\r\n        if (err) reject(err)\r\n        else resolve(succ)\r\n      })\r\n    }\r\n  })\r\n\r\n  var res = await call()\r\n  console.log(\'RESULT\', res || null)\r\n\r\n})()\r\n\r\nfunction isAsync(fn) {\r\n   return fn.constructor.name === \'AsyncFunction\'\r\n}\r\n</pre>\r\n\r\n<p>When the handler is set to <code>handler1</code> the result <code>null</code> is returned, when <code>handler2</code> is used the result value is <code>resolved</code> as well as with <code>handler3</code> - exactly the same as observed.</p>\r\n\r\n<h2>Final Thoughts</h2>\r\n<p>Well, the simulation above doesn\'t behave 100%, for example when there is no promising (like <code>setTimeout(...)</code>) in an <code>async</code> handler, the AWS Lambda executor will still take account of the callback function.</p>\r\n<p>And this is exactly what I find so confusing - it\'s easy to overlook that and be suprised of the very new (and very wrong) results...</p>\r\n<p>I wrote the simulation code to understand its behaviour as a rule of thumb - keep in mind:</p>\r\n<ul>\r\n  <li><code>async</code> &rArr; <code>return</code></li>\r\n  <li>no <code>async</code> &rArr; <code>callback</code></li>\r\n</ul>\r\n\r\n<p>Happy serverlessing!</p>  ', 'Cloud,Serverless,AWS,JavaScript', 'false', 'true', 1, 1),
(38, 'testing-serverless-systems', 1536921000, 'Testing Serverless Systems', '<p>Testing serverless systems is hard. In this article, we will discuss some practices which work well for my project.</p>', '<p>Designing a system architecture is always about making tradeoffs. Microservices resp. serverless architecture has a lot of benefits, but some drawbacks as well. One of them is testing.</p>\r\n\r\n<h2>Serverless Systems</h2> \r\n<p>Because the view to serverless microservices can differ, let\'s start with a bit theory to define the terms we will use.</p> \r\n<p>A <em>serverless architecture</em> is built upon <strong>managed elastic simple components</strong>. AWS offers for example components like Lambda functions, DynamoDB NoSQL database, SNS message service etc.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-BigPicture.png\" /></p> \r\n<p>A <em>microservice</em> is my understanding a <strong>logical unit owning all the components</strong> to carry out its whole functionality. Microservices are built around a boundary context (domain-specific) and contains all the functions and resources to be able to run as an autonomous service. All the components are private for the microservice and the functionality is accessible only via an API. If you are adapting Infrastructure as code (and you should) you build a microservice as one stack (1:1). Typically you have an integration build pipeline per microservice. Microservices can communicate via domain events (preferred way) or API calls.</p> \r\n<p>A fictitious on-line store can have for example these microservices:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-StoreExample.png\" /></p> \r\n<p>Very important role in a serverless architecture have <em>functions</em>. Functions (like AWS Lambda or Google Cloud Functions) are the simplest deployable components, typically representing an action around an aggregate (what has to be processed within a transaction).</p> \r\n<p>Applications are clients outside the system using the system functionality via API calls.</p> \r\n\r\n<h2>What and How to Test</h2> \r\n<p>The most problematic term in testing theories is a <em>Unit Test</em>, resp. the <em>Unit</em>. Some articles say it\'s the smallest functional block. Some add that a unit test must run in isolation.</p> \r\n<p>As the best definition I consider: <strong>Unit is a functionality with well-defined boundaries (via API) which carries out a business requirement</strong>. This definition goes thru all the types of tests and should be always borne in mind by writing tests. Depending on the level of abstraction the unit can be a class, function, microservice, even the entire system.</p>\r\n<p>This definition is tricky because it doesn\'t match the general picture we have about what Unit Testing is. Unfortunately, the name is so common that it\'s impossible to change it. Further in the text we will distinguish <em>test unit</em> (unit under test) from <em>Unit Testing</em> (testing on the lowest level of abstraction).</p>    \r\n\r\n<p>Another tricky term is <em>isolation</em>. Because in real system there are almost no truly isolated components - components work together as a system to provide a feature - so it makes no sense to try to isolate the components by stubbing or mocking them. Such tests have no big value. Consider following pseudo code:</p> \r\n<pre class=\"brush: java\">public class GetProductFunction {\r\n    private static final DbClient db = DbClient.build();\r\n\r\n    public Response handle(Request request, Context context) {\r\n        DbResult result = db.query(\"ProductTable\", \"name, description\", \"id = ?\", request.getProductId());\r\n        return result.found()\r\n               ? new Response(200, result.getSingleItem())\r\n               : new Response(400);\r\n    } }\r\n</pre> \r\n<p>Building a database client is an expensive operation so we keep the instance as a static variable while the function container is warm.</p> \r\n<p>Now we want to test this simple function in isolation. The problem is the variable <code>db</code> is private and final so if we want to stub it we propably end up with a code change like following:</p> \r\n<pre class=\"brush: java\">public class GetProductFunction {\r\n    static DbClient db; \r\n         \r\n    private static DbClient getDbClient() { return db == null ? db = DbClient.build() : db; }\r\n    \r\n    public Response handle(Request request, Context context) {\r\n        DbResult result = getDbClient().query(\"ProductTable\", \"name, description\", \"id = ?\", request.getProductId());\r\n        return result.found()\r\n               ? new Response(200, result.getSingleItem())\r\n               : new Response(404);\r\n    } }\r\n</pre> \r\n<p>Changing the code for sake of a test sucks. There is a lot of problems with this code, but at least we can mock our resource now:</p> \r\n<pre class=\"brush: java\">public class GetProductFunctionTest { \r\n    @Mock\r\n    private DbClient dbClient;\r\n    @Mock\r\n    private Request request;\r\n    @Mock\r\n    private Context context;\r\n\r\n    @Before\r\n    public void setup() { GetProductFunction.db = dbClient;  /* mock the expensive resource */ }\r\n    \r\n    @Test\r\n    public void testSuccess() {\r\n        when(request.getProductId()).thenReturn(\"test-id\");\r\n\r\n        when(dbClient.found()).thenReturn(true);\r\n        when(dbClient.getSingleItem()).thenReturn(\r\n                Result.builder()\r\n                        .put(\"name\", \"test-name\")\r\n                        .put(\"description\", \"test-description\")\r\n                        .build());\r\n\r\n        Response response = new GetProductFunction().handle(request, context);\r\n\r\n        assertEquals(\"test-name\", response.get(\"name\"));\r\n        assertEquals(\"test-description\", response.get(\"description\")); \r\n    }\r\n        \r\n    @Test\r\n    public void testNotFound() {\r\n        when(request.getProductId()).thenReturn(\"test-id\");\r\n\r\n        when(dbClient.found()).thenReturn(false);\r\n        when(dbClient.getSingleItem()).thenThrow(RuntimeException.class);\r\n\r\n        Response response = new GetProductFunction().handle(request, context);\r\n\r\n        assertEquals(404, response.getStatusCode());\r\n    } }\r\n</pre> \r\n<p>When we execute this test it\'s green - cool, great job! Wait, really? What do we actually test here? Apart from the trivial logic (found ⇒ 200, not found ⇒ 404) we test only the behavior we stubbed with the mocks. What\'s the value of testing mocks? - not a big one I guess...</p> \r\n<p>As we saw in the previous example, besides a few exceptions it makes only sense to test components in the system. <b>The unit of test isolation is the test</b>. It means no other test must have any impact on execution or results - tests should run isolated from each other (possibly in parallel). The unit of test isolation is <b>not</b> a class, function or component.</p> \r\n\r\n<p>We <strong>always test only the API (black-boxing), never the implementation (white-boxing)!</strong> There can be cases where implementation testing makes sense, or when we have to mock expensive resources in integration tests, but event in such situations coupling tests with the implementation should be used as little as possible.</p> \r\n\r\n<h2>Test Categories</h2> \r\n<p>Bases on the levels of abstraction of components under test we can distinguish several test categories and which APIs should be visible for the test:</p> \r\n<p> \r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"1\"> \r\n  <tr> \r\n    <th>Test category</th> \r\n    <th>Component under test</th> \r\n    <th>API</th>\r\n  </tr> \r\n  <tr> \r\n    <td>Unit Testing</td>\r\n    <td>Function</td> \r\n    <td>Public methods / exported functions</td> \r\n  </tr> \r\n  <tr> \r\n    <td>Integration Testing</td> \r\n    <td>Microservice</td> \r\n    <td>Events (REST requests / messages)</td>\r\n  </tr> \r\n  <tr> \r\n    <td>End-to-End Testing</td> \r\n    <td>System</td> \r\n    <td>API Gateway</td>\r\n  </tr> \r\n</table> \r\n</p> \r\n<p>What about testing of the (client) applications? Well, an application is actually an individual system so we can apply the entire testing model on it.</p> \r\n<p>The test category determines how and where in the build pipeline the test is executed in:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/testing-serverless/master/assets/ServelessMicroservices-Pipeline.png\" /></p> \r\n<p>A software delivery strategy cloud look like following scenario (an example):</p> \r\n<ul> \r\n  <li>a commit triggers the pipeline</li>\r\n  <li>the code source is fetched</li> \r\n  <li>unit tests are being executed for each function (A, B, ... X)</li>\r\n  <li>the code is being built for each function (A, B, ... X)</li> \r\n  <li>the service is staged into a development environment, all resources are created</li>\r\n  <li>integration tests are being executed for the entire service</li> \r\n  <li>the component is deployed into a QA environment</li> \r\n  <li>end-to-end testing pipeline is triggered or scheduled to run tests with a new version of the system</li> \r\n  <li>application pipeline is triggered to be tested with a new version of the system</li> \r\n  <li>the system is ready to be released into production</li> \r\n</ul> \r\n\r\n<h2>Unit Testing</h2> \r\n<p><em>Unit Tests</em> operate on the lowest level. Unit Tests are parts of the codebase, having direct access to code they can test units in different layers without necessarily being deployed. As all the other tests, <strong>Unit Tests verify correct behavior only thru units API - interfaces or public functions -  they never test the implementation</strong>.</p>\r\n<p>Unit Tests verify business requirements. They are executed in the build phase, because it makes no sense to deploy code which doesn\'t match business rules.</p>\r\n<p>Unit Tests should be written by developers in the same programming language as the function and be a part of the function code base.</p> \r\n<p>As an example we have an AWS Lambda <code>create-product</code>* and the function <code>createProduct()</code> as the test unit; the function implements business rules saying the product price must be greater than zero and the product name must be included in the product description (probably for SEO reasons). Consider this Node.js code:</p> \r\n<pre class=\"brush: javascript\">// create-product.js\r\nconst uuidv1 = require(\'uuid/v1\')\r\n\r\nmodule.exports = ({name, description, price}) =&gt; {\r\n    if (price <= 0) throw new Error(\'Price must be greater than zero.\')\r\n    if (!description.includes(name)) throw new Error(\'Description must include name.\')\r\n    return { id: uuidv1(), name, description, price}\r\n}\r\n\r\n// handler.js\r\nconst AWS = require(\'aws-sdk\')\r\nconst dynamoDb = new AWS.DynamoDB.DocumentClient({apiVersion: \'2012-08-10\'})\r\n\r\nconst createProduct = require(\'./create-product\')\r\n\r\nexports.handler = async (event) =&gt; {\r\n    try {\r\n        const product = createProduct(event)\r\n        persistProduct(product)\r\n        return { statusCode: 201, body: product.id }\r\n    } catch (err) {\r\n      return { statusCode: 400, body: err.message } \r\n    }\r\n}\r\n\r\nasync function persistProduct(product) {\r\n    const params = {\r\n        TableName: \'ProductTable\',\r\n        Item: product\r\n    }\r\n    await dynamoDb.put(params).promise()\r\n}\r\n</pre>\r\n<p>And a test for it:</p> \r\n<pre class=\"brush: javascript\">describe(\'Unit test to create a product.\', () =&gt; {\r\n    const testProduct = {\r\n        name: \'Product 123\',\r\n        description: \'Product 123 Desc\',\r\n        price: 123.4\r\n    }                                              \r\n\r\n    const createProduct = require(\'../create-product\')\r\n      \r\n    it(\'Product should be created.\', () =&gt; {\r\n        const product = createProduct(testProduct)    \r\n        expect(product.id).toBeDefined()\r\n    })\r\n    \r\n    it(\'Product must be greater than zero.\', () =&gt; {\r\n        const product = { ...testProduct, price: 0 }\r\n        expect(() =&gt; createProduct(testProduct)).toThrow()\r\n    }) \r\n    \r\n    it(\'Description must include product name.\', () =&gt; {\r\n        const product = { ...testProduct, description: \'junk\' }\r\n        expect(() =&gt; createProduct(testProduct)).toThrow()\r\n    })\r\n})\r\n</pre>\r\n\r\n<h2>Integration Testing</h2> \r\n<p><em>Integration Testing</em> in serverless systems operates on level of <strong>microservices</strong>. We test the whole use-case by verifying communication and interactions between functions and other resources.</p> \r\n \r\n<p>Because <strong>Integration Tests are testing already deployed services</strong> there is no more the requirement to use the same programming language as the service is written in (we can all it \"polyglot microservices\"). But as well as Unit Tests the Integration Tests should be created and maintained by developers.</p> \r\n<pre class=\"brush: javascript\">const AWS = require(\'aws-sdk\')\r\nconst lambda = new AWS.Lambda({apiVersion: \'2015-03-31\'})\r\n\r\ndescribe(\'Integration test to create and persist a new product.\', () =&gt; {\r\n	  const testProduct = {\r\n        name: \'Product 123\',\r\n        description: \'Product Desc 123\',\r\n        price: 123.4\r\n    }       \r\n\r\n    it(\'Product should be available after created.\', async () =&gt; {\r\n        // create a new product\r\n        const createParams = {\r\n            FunctionName: \'create-product\',\r\n            Payload: JSON.stringify({\r\n                name: testProduct.name, \r\n                description: testProduct.description,\r\n                price: testProduct.price\r\n            })\r\n        }\r\n        const createResponse = await lambda.invoke(createParams).promise()\r\n    \r\n        expect(createResponse).toBeDefined()\r\n        expect(createResponse.statusCode).toBe(201)\r\n        expect(createResponse.body).toBeDefined()\r\n        \r\n        const productId = createResponse.body\r\n\r\n        // the product must be available now\r\n        const getParams = {\r\n            FunctionName: \'get-product\',\r\n            Payload: JSON.stringify({\r\n                id: testProduct.id\r\n            })\r\n        }\r\n        const getResponse = await lambda.invoke(getParams).promise()\r\n\r\n        expect(getResponse).toBeDefined()\r\n        expect(getResponse.statusCode).toBe(200)\r\n        expect(getResponse.body).toBeDefined()\r\n        \r\n        const foundProduct = JSON.parse(getResponse.body)\r\n        expect(foundProduct).toEqual(testProduct)\r\n    })\r\n\r\n    function sleep(ms) { /* ... */ }\r\n})\r\n</pre> \r\n\r\n<h2>End-to-End Testing</h2> \r\n<p>The <em>end-to-end tests</em> validate behavior of the <strong>entire system or its sub-systems</strong> and run apart from a feature (microservice\'s) build pipeline.</p> \r\n<p>They can have a form of <em>acceptance tests</em> where whole paths are tested (1. search a product, 2. order the product, 3. create an invoice, 4. delivery etc.) or <em>automated GUI tests</em> which are testing the system from the client\'s point of view.</p>\r\n<p>End-to-end tests could be generally written by someone else than by the developer. Especially GUI tests can be fully in hands of testers.</p> \r\n<p>Of course, the <em>manual testing</em> belongs to this category as well.</p> \r\n\r\n<p>In the previous integration-testing example both functions for saving and searching products were a part of one microservice - it means they lie in one build pipeline. If these functions were parts of different microservices the test would belong to the end-to-end testing category.</p>\r\n\r\n<p>As an example let\'s consider a Catalog microservice - after creating a new product via <code>create-product</code> function a domain event like <code>NEW_PRODUCT_CREATED</code> should be fired. The <strong>event is a part of the internal API</strong> of the service and can be consumed by any other service registered to it. Typically an indexing service reacts to the event by extracting meta-data from the product and provides a full-search upon them. There should be an integration test validating this behavior.</p>\r\n<p>End-to-end Testing doesn\'t access components directly but only via an API Gateway (e.g. REST).</p> \r\n<pre class=\"brush: java\">public class CreateProductServiceTestE2E {\r\n\r\n    import ...\r\n\r\n    private final String SERVICE_API = System.getenv(\"SERVICE_API\");\r\n    private final String PRODUCT_CREATE_ENDPOINT = SERVICE_API + \"/catalog/product\";\r\n    private final String SEARCH_ENDPOINT = SERVICE_API + \"/search\";\r\n\r\n    private HttpClient httpClient;\r\n\r\n    @Before\r\n    public void setup() { this.httpClient = createHttpClient(); }\r\n\r\n    @Test\r\n    public void testProductMetadataShouldIndexed() throws Exception {\r\n        Product product = new Product(\"Product 123\", \"Product Desc 123\", 123.4);\r\n\r\n        // create a new product\r\n        HttpPost createRequest = httpClient.post(PRODUCT_CREATE_ENDPOINT);\r\n        createRequest.setHeader(\"Content-type\", \"application/json\");\r\n        createRequest.setBody(product.toJson());\r\n\r\n        HttpResponse createResponse = httpClient.execute(createRequest);\r\n\r\n        assertEquals(201, createResponse.getStatusCode());\r\n        assertNotNull(createResponse.getBody());\r\n        \r\n        String productId = createResponse.getBody();\r\n\r\n        Thread.sleep(1000); // waiting necessary due to eventual consistency\r\n\r\n        // the product must be indexed and available for searching\r\n        HttpPost searchRequest = httpClient.post(SEARCH_ENDPOINT);\r\n        searchRequest.setHeader(\"Content-type\", \"application/json\");\r\n        searchRequest.setBody(\"{\\\"keyword\\\":\\\"\" + product.getName() + \"\\\"}\");\r\n\r\n        HttpResponse searchResponse = httpClient.execute(searchRequest);\r\n\r\n        assertEquals(200, searchResponse.getStatusCode());\r\n        assertNotNull(createResponse.getBody());\r\n        \r\n        Collection&#x3C;String&#x3E; foundProductIds = toList(createResponse.getBody());\r\n        assertTrue(foundProductIds.contains(productId));\r\n    }\r\n}\r\n</pre>\r\n\r\n<h2>Summary</h2> \r\n<p>In this article I wanted to recap my experience with functional testing of serverless systems and simplify the levels of tests. I reviewed terms Unit-, Integration- and End-to-end Testing to show what is under test and where such tests lie in the development process.</p> \r\n<p>Despite the fact that I focused only on functional testing doesn\'t mean that testing non-functional requirements like performance, security, scalability, etc. are not important...:-)</p> \r\n\r\n<p>Happy testing!</p> <hr /> \r\n\r\n<p><em>* I use verbs to name functions because it sounds natural to me. A function is in fact an action.</em></p>', 'Testing,Cloud,Serverless,AWS', 'false', 'false', 1, 1),
(39, 'function-separation-in-a-microservice', 1538200000, 'Function Separation in a Microservice', '<p>Talking about serverless microservices, functions are the basic building blocks of the service functionality. How to design them from the code and deployment perspektive?</p>', '<h2>Microservices Anatomy</h2>\r\n<p>Let\'s simply define a microservice as an <strong>autonom service built from one or more resources</strong>. The resource mean functionality (functions*), storage (database), communication (message topic), etc. All the service needs must be included in the stack or given as a parameter (environment variable), API is exposed and besides are all the resource private for the microservice. One microservice &hArr; one stack &hArr; one deployment pipeline.</p>\r\n\r\n<p>According the Single Responsibility Principle a component should do one and only one thing. Applying this rule to the functions level we get a function for an action. Let\'s consider a simple CRUD microservice; all the CRUD operation are represented by simple functions:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-code-samples/master/nodejs-crud-microservice/doc/FunctionSeparation-Product-Service-Overview.png\" width=\"50%\" /></p>\r\n\r\n<p>How does this look like from the dev-ops point of view? We can create a new project artifact for every function. Such code is <strong>easy to reason about, easy to test and deploy</strong>. The last one could be written with a question mark, of course it\'s easy to deploy a function, but don\'t forget there must be pipeline actions for build, deploy and test execution which is really only doing, but it\'s a strenuous and boring job and brings additional complexity.</p>\r\n                                                                                                                                                                               \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/FunctionSeparation-pipeline.png\" width=\"100%\" /></p>\r\n\r\n<p>Similar for the codebase. Take a look at this simple AWS Lambda function in Node.js:</p>\r\n\r\n<pre>\r\n.gitignore\r\ngruntfile.js\r\njasmine.json\r\npackage.json\r\npackage-lock.json\r\nREADME.md\r\n</pre>\r\n\r\n<p>As you can see there several files only to set up the module. This must be multiplied for every function.</p>\r\n\r\n<p>So, a function for an action sounds well in theory but in practise it could be an overkill.</p>\r\n\r\n<h2>Modularization \'till the last step</h2>\r\n<p><strong>Decoupling</strong> is propably the most important concept in the design of software systems. So we never surrender this principle. Anyway we can distinguish between logical and physical coupling. <strong>Two modules can be perfectly decoupled even while physically existing in a same artifact</strong> - when they have no shared code and dependencies only to APIs (following principles of hexagonal architecture).</p>\r\n\r\n<p>Let\'s consider the following implementation of the CRUD microservice:</p>\r\n\r\n<pre>\r\nsrc/\r\n   index.js\r\n   list-products.js\r\n   get-product.js\r\n   create-product.js\r\n   update-product.js\r\n   remove-product.js\r\n</pre>              \r\n\r\n<p><code>index.js</code></p>\r\n<pre class=\"brush: javascript\">\r\nconst list = require(\'./list-products\').handler\r\nconst get = require(\'./get-product\').handler\r\nconst create = require(\'./create-product\').handler\r\nconst update = require(\'./update-product\').handler\r\nconst remove = require(\'./remove-product\').handler\r\n\r\nexports.handler = async function(event) {\r\n    if (!event || !event.resource || !event.httpMethod) {\r\n        return buildResponse(400, {error: \'Wrong request format.\'})\r\n    }\r\n\r\n    if (event.resource === \'/\') {\r\n\r\n        switch (event.httpMethod) {\r\n            case \'GET\':\r\n                return await list(event)\r\n            case \'POST\':\r\n                return await create(event)\r\n            default:\r\n                return buildResponse(405, {error: \'Wrong request method.\'})\r\n        }\r\n    }\r\n    else if (event.resource === \'/{id}\') {\r\n\r\n        switch (event.httpMethod) {\r\n            case \'GET\':\r\n                return await get(event)\r\n            case \'PUT\':\r\n                return await update(event)\r\n            case \'DELETE\':\r\n                return await remove(event)\r\n            default:\r\n                return buildResponse(405, {error: \'Wrong request method.\'})\r\n        }\r\n    } else {\r\n        return buildResponse(404, {error: \'Resource does not exist.\'})\r\n    }\r\n}\r\n\r\nfunction buildResponse(statusCode, data = null) {\r\n    return {\r\n        isBase64Encoded: false,\r\n        statusCode: statusCode,\r\n        headers: {\r\n            \'Content-Type\': \'application/json\'\r\n        },\r\n        body: JSON.stringify(data)\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>As you can see, the main handler is only a router to other functions, which look like following:</p>\r\n\r\n<p><code>get-product.js</code></p>\r\n<pre class=\"brush: javascript\">\r\nconst AWS = require(\'aws-sdk\')\r\n\r\nconst PRODUCT_TABLE = process.env.PRODUCT_TABLE\r\n\r\nconst dynamoDb = new AWS.DynamoDB.DocumentClient({apiVersion: \'2012-08-10\'})\r\n\r\nexports.handler = async function(event) {\r\n    if (!event || !event.httpMethod) {\r\n        return buildResponse(400, {error: \'Wrong request format.\'})\r\n    }\r\n    if (event.httpMethod !== \'GET\') {\r\n        return buildResponse(405, {error: \'Wrong request method - only GET supported.\'})\r\n    }\r\n    if (!event.pathParameters || !event.pathParameters.id) {\r\n        return buildResponse(400, {error: \'Wrong request - parameter product ID must be set.\'})\r\n    }\r\n\r\n    try {\r\n        const response = await dispatch(event.pathParameters.id)\r\n        if (response) {\r\n            return buildResponse(200, response)\r\n        } else {\r\n            return buildResponse(404, {error: \'Product was not found.\'})\r\n        }\r\n        \r\n    } catch (ex) {\r\n        console.error(\'ERROR\', JSON.stringify(ex))\r\n    }\r\n}\r\n\r\nasync function dispatch(id) {\r\n    const product = getProduct(id)\r\n    return product\r\n}\r\n\r\nasync function getProduct(id) {\r\n    const params = {\r\n        TableName: PRODUCT_TABLE,\r\n        Key: { productId: id }\r\n    }\r\n    const res = await dynamoDb.get(params).promise()\r\n    \r\n    return (res.Item) \r\n        ? { id: res.Item.productId, \r\n            name: res.Item.name, \r\n            description: res.Item.description, \r\n            price: res.Item.price }\r\n        : null\r\n}\r\n\r\nfunction buildResponse(statusCode, data = null) {\r\n    return {\r\n        isBase64Encoded: false,\r\n        statusCode: statusCode,\r\n        headers: {\r\n            \'Content-Type\': \'application/json\'\r\n        },\r\n        body: JSON.stringify(data)\r\n    }\r\n}\r\n</pre>\r\n\r\n<p>The Single Responsibility Principle is applied here. The function has its own handler which takes care of the incoming HTTP request. All the HTTP-related checks and transformations must be done here. The handler is the only one place for such a dirty code. After the request is checked, dispatching is executed. This is something like a main method, an entry point to the business logic. In this case just getting the product details.</p> \r\n\r\n<p>And other functions in the same spirit...</p>\r\n\r\n<p>The actions are decoupled from each other and are ready to be separated into individual functions in the next last step. But staying here we still have a very clear code easy to test and reason about and we don\'t need to adapt the pipeline onto this level of granularity.</p>                                       \r\n  \r\n<h2>Test it!</h2>\r\n<p>It\'s very easy to test a service built in this way. We write a Unit Test Suite for every action (exported function), an Integration Test Suite for the service API (index.js) and End-to-End Test Suite for the Gateway API facade.</p> \r\n  \r\n<h2>Source Code</h2>\r\n<p>The whole stack could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/nodejs-crud-microservice\">GitHub</a>.</p>\r\n\r\n<p>Happy coding!</p>  \r\n\r\n<hr/>\r\n<p>* <em>Functions are sometimes called \"nanoservices\" as they are smaller than microservices. Actually, I don\'t even like the term \"microservice\" because it confuses people regarding its size (\"How small should a microservice be?\"), so I don\'t like to develop this terminology on in the same manner.</em>', 'Microservices,Cloud,Serverless,AWS', 'false', 'false', 1, 1),
(40, 'rollback-and-microservices', 1538900000, 'Rollback and Microservices', '<p>Is it even possible to roll microservices back?</p>', '<p>When thinking about a deployment strategy for microservices-based systems, it is natural to consider a <em>rollback</em>. Before looking for a technical solution, let\'s discuss this idea conceptually.</p>\r\n\r\n<h2>Microservices</h2>\r\n<p>One of a plenty definitions of a microservice is an <strong>independently deployable</strong> component. This says actually a pretty lot. To understand this definition we have to go back to the very first motivation for the microservices design pattern: <em>We want to deliver our product as soon as possible</em>. A microservice could be in hands of one team and have a completely different and unsynchronous delivery process. A team developing service A doesn\'t want to be bothered by waiting for the next release of a service B if it doen\'t need its new functionality. Service A could be deployed into the production once a day and the service B once a month.</p>\r\n<p>For all this to be possible we have to follow a few basic principles, especially the most important one in our context: <strong>No Breaking Changes</strong>. We may find the same principle under different names like <a href=\"https://www.allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\" target=\"_blank\">\"APIs are forever\"</a> etc., but the idea is the same.</p>\r\n\r\n<h2>No Breaking Changes</h2>\r\n<p>How does this principle makes the microservices deployment independent? Let\'s consider two services, A and B. B is calling A via A\'s API. In the time of development of a version v1 of the service B, there is a released API of service A in the version v1. So the service B v1 makes its call against the service A v1:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-1.png\" width=\"70%\" alt=\"Service Bv1 calls Av1\" /></p>\r\n\r\n<p>During the development of service B v1 a new version of service A - v2 - was released. An additional endpoint was added, but it\'s okay for the service B because A\'s API was not broken (in other words, A v2 is compatible with A v1). Actually, so far there are no breaking changes the service B doesn\'t care which newly realeased version of the service A is talking to:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-2.png\" width=\"70%\" alt=\"Service Bv1 calls Av2\" /></p>\r\n\r\n<h2>What about Rollbacks?</h2>\r\n<p>Now consider that the service B in some future version (let\'s say v2) is using an endpoint of the service A added in its release v2:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-3.png\" width=\"70%\" alt=\"Service Bv2 calls Av2\" /></p>\r\n\r\n<p>After some time the developers of service A find an error in the v2. They decide to rollback to the previous version (v1). What happens to the system? Service B stops working because it keeps calling the endpoint of service A v2 which doesn\'t exist anymore. The <strong>rollback to a previous version is a potential breaking change</strong>:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/rollback-4.png\" width=\"70%\" alt=\"Service Bv2 calls Av1\" /></p>\r\n\r\n<p>Because it can introduce a breaking change as the rest of the system depends on the current API a <strong>rollback should be avoided in microservices-based systems</strong>.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>As long as we can\'t rollback microservices, we have to find a confident way how to deploy them into production. There are a lot of good practices like separated deployment stages (development, testing, QA, ...), testing in a production-like environment, blue-green and canary deployment, and so on.</p>\r\n<p>It\'s also very important to resist the temptation of hot fixing a bug directly in the production environment. We alywas have to adapt the continuous-delivery principles: Every change, bug-fixes included, must be properly built and tested thru the standard deployment process (delivery pipeline).</p> \r\n<p>Microservices are living organisms and should be treated like that.</p>\r\n\r\n<p>Happy deploying!</p>', 'Microservices,Deployment', 'false', 'false', 1, 2),
(41, 'serverless-blue-green-deployment', 1540023000, 'Serverless Blue-Green Deployment', '<p>Blue-Green Deployment is a very good technique that has been successfully used for managing releases of cloud applications. Now it\'s time to rethink it a bit for serverless systems.</p>', '<h2>Blue-Green Deployment</h2>\r\n<p>The concept is pretty easy: We have <strong>two identical production environments</strong> (<i>blue</i> and <i>green</i>), the green is &quot;live&quot; as default. When we release a new version, we deploy first into the blue environment. There we can perform some tests and then switch routing. The blue enviroment becomes &quot;live&quot;. If something goes wrong we should be able to <strong>switch back in less than a second</strong>.</p>\r\n   \r\n<h2>Problems with Serverless</h2>\r\n<p>So, why don\'t we use the same strategy for serverless deployment? We have several challenges to solve:</p>\r\n<ul>\r\n  <li><strong>No database integration</strong> - a service should contain its own data storages as a part of the stack.</li>\r\n  <li><strong>Independent deployment lifecycles</strong> - there is no synchronization between deployments.</li>\r\n  <li><strong>Location transparency</strong> - the topology of serverless systems is dynamic.</li> \r\n</ul>\r\n<p>Is there any solution for those problems? Maybe yes, but the cost of complexity would be too high. On the other hand, this doesn\'t mean we have to abandon the blue-green deployment completely. We can still benefit from the idea, just on another level.</p>\r\n\r\n<h2>Blue-Green for Test-Isolation</h2>\r\n<p>Let\'s image the following scenario: We have a test stage where all the integration tests are executed in. This stage is shared by all the service deployment pipelines.</p>\r\n<p>What happens when a test of the service A runs at the same time as another test of the service B, which uses the service A as an external resource? When the new version of the service A doesn\'t work, it has a negative impact on the test results. And that\'s the point where the blue-green deployment comes to rescue!</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Serverless-BlueGreen-Deployment.png\" width=\"100%\" alt=\"Serverless Blue-Green Deployment\" /></p>\r\n\r\n<p>As you can see in the picture above, when a new version of the service A (Av2) is about to be tested, it is deployed parallel to the previous version stack (Av1). The <i>blue</i> version is integrated in the system as well as the <i>green</i> version. If another test comes along, it sees only the already tested stack Av1.</p>\r\n\r\n<h2>Source Code</h2>\r\n<p>You can find a simple implementation of the serverless blue-green development for test-isolation in my <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/serverless-blue-green-deployment\">Github account</a>. The example uses AWS CodePipeline.</p>\r\n\r\n<p>Be blue-green, be happy!</p>', 'Cloud,Serverless,Deployment', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(42, 'use-case-driven-testing', 1541155000, 'Use-Case-Driven Testing', '<p>Why shouldn\'t we test the implemetation? How to decouple our tests from the code? What is the reason to add a new test? Why is mocking a code smell? In this article I will try to find answers to those questions.</p>', '<p>In <a href=\"/testing-serverless-systems\">my previous article</a> I opened the topic and defined some important terms and principles I find good to follow. Let me summarize some of them and continue in the theory how we should design our tests.</p>\r\n\r\n<h2>Contracts Testing vs. Implementation Testing</h2>\r\n<p>We can see a test as a friend of our application but never as its part (tests codebase is never a part of the production application). As a good friend <strong>a test must talk to the application via a contract</strong>. We can call that contract an API. Every well-defined component offers an API.</p>\r\n<p>If we couple our test with the implementation, we have to adapt the test everytime we refactor the application code, which is not only annoying and impractical but unnecessary and even rude (we don\'t snoop around in friend\'s wardrobe). After some time people get sick of double work and start to write fewer tests and/or ignore broken tests.</p>\r\n<p>Tests should <b>not</b> care <em>how</em> is a feature implemented, they <strong>should care only if the behavior is correct</strong>. When we implement a sorting function we should test if the sorting results are correct regardless which sorting algoritm is used.</p>\r\n<p>For example when we test if a record was saved we can use another service to retrieve the record, again via its API. When there is no API to retrieve a record there is no possibility to validate if the record was really save - but such a \"black hole\" system makes no sense.</p>\r\n\r\n<h2>Code-Drive Testing vs. Use-Case-Driven Testing</h2>\r\n<p>It\'s a very popular practice to have a test per class, in Java like following:</p>\r\n<pre>\r\n/src/main/java/Foo.java\r\n              /Bar.java\r\n/src/test/java/FooTest.java\r\n              /BarTest.java              \r\n</pre>\r\n<p>This is a typical example of code-driven testing design. Here is a new class a reason to add a new test. But a class is just an implementation detail. Maybe after some time you find out the class is too big and should be refactored into two classes. Then you have to refactor your test as well although there is no functional change and the test validates always the same behavior. But if you didn\'t do it your test wouldn\'t propably compile. An unhappy situation. <strong>Code-driven testing design makes maintenance hard, expensive and boring.</strong></p>\r\n<p>It is much better to <strong>drive your tests by use-cases</strong>. With this strategy not a class or a method is a reason to add a new test but a new requirement is. The structure looks like following:</p>\r\n<pre>\r\n/src/main/java/Foo.java\r\n              /Bar.java\r\n/src/test/java/UseCase1Test.java\r\n              /UseCase2Test.java              \r\n</pre>\r\n<p>When you decided to split a class into two or to remove one, the tests remain without any change:</p>\r\n<pre>\r\n/src/main/java/Foo1.java\r\n              /Foo2.java\r\n/src/test/java/UseCase1Test.java\r\n              /UseCase2Test.java              \r\n</pre>\r\n<p>In fact, there are only two reasons for a test change: 1. the requirement has changed, 2. there was a bug in the test self.</p>\r\n<p>The design of the test has the following pattern:</p>\r\n<pre class=\"brush: java\">\r\nclass UseCaseTest {\r\n\r\n  @Test\r\n  public void requirementOneTest() { \r\n    assert(/* acceptance criterium 1 */);\r\n    assert(/* acceptance criterium 2 */);\r\n    ...\r\n  }\r\n\r\n  @Test\r\n  public void requirementTwoTest() { \r\n    assert(/* acceptance criterium 1 */);\r\n    assert(/* acceptance criterium 2 */);\r\n    ...\r\n  }\r\n  ...\r\n}\r\n</pre>\r\n<p>The API is defined by an inteface (each public method of a class should implement an interface) and the implementation should be initialized via a factory, for example:</p>\r\n<pre class=\"brush: java\">\r\n/** src/main/java/MyUseCase.java */\r\npublic interface MyUseCase {\r\n\r\n  int func1(String param);\r\n  String func2(int param);\r\n}\r\n\r\n/** src/main/java/Foo.java */\r\nclass Foo implement MyUseCase {\r\n\r\n  private Bar bar = new Bar();\r\n    \r\n  public int func1(String param) {\r\n    ...\r\n  }\r\n  \r\n  public String func2(int param) {\r\n    ...\r\n    return bar.func();\r\n  } \r\n}\r\n\r\n/** src/main/java/Bar.java */\r\nclass Bar {\r\n\r\n  String func(int param) {\r\n    ...\r\n  }\r\n}\r\n\r\n/** src/test/java/MyUseCaseTest.java */\r\nclass MyUseCaseTest {\r\n\r\n  private MyUseCase myUseCase;\r\n    \r\n  @Before\r\n  public void initialize() {\r\n    myUseCase = new MyDefaultUseCaseFactory().createMyUseCase();\r\n  }    \r\n\r\n  @Test\r\n  public void func1Test() { \r\n    assertEquals(/* expected */, myUseCase.func1(...));\r\n    assertEquals(/* expected */, myUseCase.func1(...));\r\n    ...\r\n  }\r\n\r\n  @Test\r\n  public void func2Test() { \r\n    assertEquals(/* expected */, myUseCase.func2(...));\r\n    assertEquals(/* expected */, myUseCase.func2(...));\r\n    ...\r\n  }\r\n  ...\r\n}\r\n</pre>\r\n \r\n<h2>Code Coverage vs. Use-Case Coverage</h2>\r\n<p>Another popular practice is code coverage. In some companies there is a code coverage check as a part of the build pipeline and a commit is rejected if its code coverage is less than a particular percentage. <strong>Code coverage is a wrong metric</strong> - you can have 100% code coverage without actually to test anything.</p>\r\n<p>Much more important is <strong>use-case coverage</strong>, telling us how much of our requirements are covered with test. <strong>We should always try to achive 100% use-case coverage</strong>, because satisfaction of use-cases determine the product quality.</p>  \r\n\r\n<h2>Test Categories</h2>\r\n<p>We distinguish several test categories based on the level of granularity the test operates on.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/TestingGranularity.png\" width=\"80%\" /></p>\r\n\r\n<p>As showned in the picture, the <strong>unit testing</strong> operates on the level of functions, it can access the functionality of the smallest components via their APIs defined by public methods or exported function. <strong>Integration testing</strong> sees a service as a black box as well as <strong>system testing</strong>* has no idea about the services structure hidden behind the system\'s facade (gateway).</p>\r\n\r\n<h3>Unit Testing</h3>\r\n<p>Unit tests operate on the level of functions and <strong>should test only business functionality</strong>. If there is no business logic and a requirement is implemented only with composition of another resources (like saving into a database or calling another function/service) there\'s no reason for a unit test and the testing should be done as a part of integration testing (otherwise we write unit test as integrations test where all resources are mocked - such tests have no value).</p>\r\n<p>To make unit testing possible we should always follow the Single Responsibility Principle by writting our code. If more responsibilities are mixed into one component, for example when a function transforms the input and saves it into a database, there is no other way how to test the transformation function but to mock the database. <strong>If we need a mock an dependency to be able to test our business logic in separation, the code probably needs refactoring</strong>.</p>\r\n<p>Another thing which makes writting tests much easier is to avoid side-effects. A function with side-effects cannot be easily tested as a black box with inputs and outputs (both can be absent) and need a knowledge about its interns resp. implementation details. Use pure functions as much as possible.</p>\r\n\r\n<h3>Integration Testing</h3>\r\n<p>Integration Testing operates on the level of services, it means autonomous components, and tests their interactions.</p>\r\n<p>Integration tests must run in the system environment so all the needed resources are available (and we don\'t have to mock anything).</p>\r\n<p>Integration tests as well as unit tests belong to a service and should access only the service\'s API. If a test needs to access more services we call it a system test and it should run accross services (e.g. in a separate testing pipeline).</p>\r\n\r\n<h3>System Testing</h3>\r\n<p>System testing sees the entire system as a black box with a facase (gateway) API. Whole scenarios accross services are under the test.</p>\r\n\r\n<h2>Mocking is a Code Smell</h2>\r\n<p>If you follow the principles above you don\'t need to mock anything in your tests. If you can\'t test without mocking there must be something \"smelly\" in the design.</p>\r\n<p>Of course, there could be exceptions like when a resource is too expensive or slow, but we should avoid mocking as much as possible, because it always removes a value from the tests.</p>\r\n<p>Mocks are coupled to the implementation which makes them brittle. If you really have to mock out an expensive dependency, consider to use a fake object (simple implementations with business behavior) instead. Fake objects, in contrast to mocks, contain domain logic and are a part of the domain. Read more about <a href=\"https://blog.cleancoder.com/uncle-bob/2014/05/14/TheLittleMocker.html\" target=\"_blank\">test doubles</a> and <a href=\"https://martinfowler.com/articles/mocksArentStubs.html\" target=\"_blank\">mocks vs. stubs</a> to understand the difference.</p>\r\n\r\n<h2>Example: Money</h2>\r\n<p>Do you remember the Money example from Kent Beck\'s <a href=\"https://www.oreilly.com/library/view/test-driven-development/0321146530/\" target=\"_blank\">Test-Driven Development By Example</a>? Let\'s use it as a base for our simple Money microservices system implemented with <a href=\"https://aws.amazon.com\" target=\"_blank\">AWS</a>.</p>\r\n<p>The Money service offers an API for multiplication of money amounts and reduction into different currencies. The Exchange service provides information about the actual rate; we can implement it for example as pulling the data from the New York Stock Exchange via its API.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/MoneyExampleMicroservices.png\" width=\"80%\" /></p>\r\n\r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/money\" target=\"_blank\">GitHub</a>. The implementation is far from perfect, but the idea is clear. Handlers (<code>index.js</code>) are separeted from the business (<code>money.js</code> and <code>exchange.js</code>), unit tests focus on the business, integration tests on handlers (which make APIs of the services) and system tests on the facade.</p>\r\n\r\n<p>Each tests have a different place in the build pipeline. Unit tests are executed for every function, integration tests for every service (a bunch of functions) and system tests for the entire system (a bunch of services).</p>\r\n<p>In our example there is only one function per service, but in a real-world scenario there would be much more. We can even think of a function per use-case. In such a case we would have two function in the Money service: <code>times</code> function and <code>reduce</code> function.</p>\r\n\r\n<p>Happy testing!</p>\r\n\r\n<hr />\r\n<p><strong>*</strong>) <em>System testing belongs together with GUI and manual testing into the group End-to-end testing which all operate on the same level of granularity. By system testing is in this article meant automatic testing of back-end functionality e.g. via system\'s REST API.</em></p> ', 'Testing', 'false', 'false', 1, 1),
(43, 'javascript-async-await-in-a-loop', 1541242000, 'JavaScript async/await in a Loop', '<p>Async/await syntax is a great technique how to deal with promises in modern JavaScript. Unfortunately it\'s not always easy to understand how it works which can lead to strange bugs. Let\'s investigate one of them.</p>', '<p>There is a service in our system cleaning up unused resources. Those resources are grouped upon an ID. So the request looks like \"remove all the resources for the ID 123\".</p>\r\n<p>In the service code I have found the following line:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nentryKeys.forEach(async key => await removeResource(key))\r\n</pre>\r\n\r\n<p>Why this doesn\'t work? Let\'s analyse the code a bit. The <code>forEach</code> is just a stream variant of the <code>for</code> loop and it\'s definitely synchronous. So, there shouldn\'t be the problem even when it\'s a bit ineffective. Inside the <code>forEach</code> there is a call of the <code>removeResource</code> function with <code>await</code>. The <code>await</code> tells us that the function returns a promise. The <code>await</code> can be used only within an <code>async</code> function which is fulfilled because the inline (lambda) function is really marked with the <code>async</code> keyword (we can rewrite the same function as <code>async function(key){ await removeReource(key) }</code>). The point is a function declared with <code>async</code> returns always a promise. It means, the <code>forEach</code> fires several <b>asynchronous</b> calls but <b>doesn\'t wait for them</b> (<code>await removeResource(key)</code> is another asynchronous call inside that asynchronous call).</p>\r\n\r\n<p>Now, when we understand why the code doesn\'t work, we can fix it:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nfor (const key of entryKeys) { await removeResource(key) } \r\n</pre>\r\n\r\n<p>This works fine, the only problem is, it\'s synchronous and slow. The loop is waiting for an execution to finish before starting a new one even when it is possible to run them all in parallel.</p>\r\n<p>Can we fix it? Sure!</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nawait Promise.all(entryKeys.map(key => removeResource(key))) \r\n</pre>\r\n\r\n<p>We have changed <code>forEach</code> to <code>map</code>, so we map an array of IDs into an array of promises. Then we pack the promises into a one big promise via <code>Promise.all()</code>. Now we wait for the big promise (and that\'s exactly what we missed before) to be completed. All the call are executed in parallel. Well done.</p>\r\n\r\n<p>You can play with the different variants via the following snippet:</p> \r\n\r\n<pre class=\"brush: javascript\">\r\n(async function(){\r\n    console.log(\"START\")\r\n    \r\n    // doesn\'t work\r\n    //[\"A\",\"B\",\"C\"].forEach(async key => await doSomethingAsync(key))\r\n    \r\n    await Promise.all([\"D\",\"E\",\"F\"].map(key => doSomethingAsync(key)))\r\n    \r\n    // synchronous\r\n    //for (const key of [\"G\",\"H\",\"I\"]) { await doSomethingAsync(key) }   \r\n    \r\n    console.log(\"END\")  \r\n})()\r\n\r\nfunction doSomethingAsync(key) {\r\n    return new Promise(function (resolve, reject) {\r\n        setTimeout(() => {\r\n            console.log(\"RESOLVED \" + key)\r\n            resolve(key)\r\n        }, 1000)\r\n    })\r\n}\r\n</pre>\r\n\r\n<h2>What about Reduce?</h2>\r\n\r\n<p>How could we reduce the value from the promises? Well, of course the reduce needs the already resolved values:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\n// \"JKL\"\r\nvar result = await [\"J\",\"K\",\"L\"]\r\n      .map(key => doSomethingAsync(key))\r\n      .reduce(async (sum,v) => await sum + await v)\r\n</pre>\r\n\r\n<p>Because the whole reduce function is <code>async</code> we have to use <code>await</code> as well to retrieve the value of <code>sum</code>.</p>\r\n\r\n<p>Happy promising!</p>', 'Programming,JavaScript', 'false', 'false', 1, 1),
(44, 'lesson-learned-principles-of-serverless-development', 1546126000, 'Lesson Learned: Principles of Serverless Development', '<p>After some time of developing <strong>serverless</strong> systems (especially on AWS) I take a look back and try to summarize what I have learned so far.</p>', '<p>First, I should explain what I mean by saying <em>serverless</em> as long as this became already a buzzword and could have different meanings.</p> \r\n<p>Let think about a serveless system simply as about <strong>a system with only managed, elastic (auto-scaling) resources</strong>. This means, hardware (CPU, memory, storage) and platform (OS, runtime) provisioning is always included as a part of the cloud-provider service.</p> \r\n<h2>Development</h2> \r\n<ol> \r\n<li>Everything is as serverless as possible, only managed, auto-scaling services are used.</li> \r\n<li>The development of a service is finished, once an infrastructure template exists and runs without error.</li> \r\n<li>Every client communication with the cloud services is possible only via an HTTPS facade API.</li> \r\n<li>Design of an API is use-case- and business-driven as well as the architecture-components.</li> \r\n<li>Implementation has no impact on an API and it\'s completely hidden behind it.</li> \r\n<li>In the implementation, the business-logic is decoupled from the cloud-provider.</li> \r\n<li>References to resources are handed over to services via environment variables.</li> \r\n<li>For each service there are unit + integration tests.</li> \r\n<li>Each service is accessible only via API and events.</li> \r\n<li>Multitenancy (when used) is a solid, mandatory part of every service.</li> \r\n</ol> \r\n<h3>Everything is as serverless as possible</h3> \r\n<p>On can spin-up thousands of functions in a second and doesn\'t have to take care of load-balancing or any other resource management - this all is done by the provider of the serverless service (by the definition). The natural consequence is that if only one part of the system is non-serverless, the whole system becomes failure-prone.</p> \r\n<p>Consider a serverless function connected to a non-serverless database. After spinning-up an amount of function greater than the access limit of the database, next calls of the function will freeze and eventually fail with a timeout.</p> \r\n<p>Isolation of a failure and fall-back strategy for dealing with a non-serverless resource in a severless system is crucial for success.</p> \r\n<h3>The development is finished, once an infrastructure template exists</h3> \r\n<p><em>Infrastructure as Code</em> is an important part of a serverless deployment. The template of the service infrastructure as an input for the deployment process encourages the so-called <em>DevOps</em> culture.</p> \r\n<h3>Client communicates with the services only via its HTTPS API</h3> \r\n<p>The client code should never use any provider-related SDKs. The communication must be agnostic and independent.</p>\r\n\r\n<p>This frees your clients from the vendor lock-in, which is definitely a good idea, because the client code is usually not in the hand of the services developers - freeing clients makes services more independent (for example, a service doesn\'t break any client by changing a cloud provider).</p> \r\n</h3> \r\n<h3>Design is use-case- and business-driven</h3> \r\n<p>Following principles of <em>Domain-Driven Design (DDD)</em> will provide a great insight of how to separate code into services.</p> \r\n<p>If helps to keep the API stable as domain seems to be more relevant for clients (customers) than technical aspects.</p> \r\n<h3>Implementation has no impact on an API</h3> \r\n<p>Connected to the previous one, not only an API must be domain-driven, but any implementation details must not leak into it.</p> \r\n<p>I guess this is not surprising as it counts to the very basic principles of software design in general.</p> \r\n<h3>Business-logic is decoupled from the cloud-provider</h3> \r\n<p>Serverless is a kind of deployment which means it should be implemented in the very outer layer of code calling domain logic as its dependency (never the other way around).</p> \r\n<h3>Resources are referenced via environment variables</h3> \r\n<p>The point III of <a href=\"https://12factor.net/\" target=\"_blank\">The Twelve Factors</a> in practice.</p> \r\n<p>The references should be resolved automatically in service templates in deployment time.</p> \r\n<h3>Each service has unit and integration tests</h3> \r\n<p>Test-driven Development (TDD) is a great method to build a software of high quality.</p> \r\n<p>Don\'t accept any service without tests as finished.</p> \r\n<p>For the whole product there should be a set of end-to-end test for testing whole scenarios from a client point of view on APIs.</p> \r\n<h3>Each service is accessible only via API and events.</h3> \r\n<p>All the service\'s internal resources (e.g. file storage, database etc.) are hidden and from outside denied for access.</p> \r\n<p>It has been already said that implementation must not have any impact on the API, this point says the same from the other side: no other service running in the same environment must access a service\'s internals (even if this is technically possible).</p> \r\n<h3>Multitenancy is a solid, mandatory part of a service.</h3> \r\n<p>When sharing services among customers (the opposite would be to build a &quot;silo&quot; stack for each customer), the multitenancy must be a part of every service - considered from design and the very beginning of development.</p> \r\n<h2>Deployment</h2> \r\n<ol> \r\n<li>Names of the deployment stacks follow the pattern &quot;&lt;service-name&gt;-&lt;stage&gt;&quot;.</li> \r\n<li>Automatic tests are executed in the development-stage DEV, acceptance and manual tests in the test-stage QA.</li> \r\n<li>Developers (stage Dev) and testers (stage QA) have no access to the production (stage Prod).</li> \r\n<li>The deployment of product artifacts is realized and implemented as a build pipeline (Continuous Delivery).</li> \r\n</ol> \r\n<h3>Names of the deployment stacks follow the pattern &quot;&lt;service-name&gt;-&lt;region&gt;-&lt;stage&gt;&quot;</h3> \r\n<p>It\'s important to bring order to the system resources and make the management of them human-friendly. In this case, similar as in code, the naming is very helpful.</p> \r\n<p>Each resource must be deployable in every region within the account, which is enabled by using the region name a a suffix.</p> \r\n<p>The name of the stage (<code>dev</code>, <code>test</code>, <code>prod</code>, etc.) as a suffix allows the developer to deploy multiple stages inside a single account (for test purposes or costs optimizing).</p> \r\n<h3>Developers and testers have no access to the production</h3> \r\n<p>This is possible thru multiple deployment stages strategy where different stages are deployed as a continuous process for different purposes till the last - production - stage.</p> \r\n<h3>Deployment implemented as a build pipeline</h3> \r\n<p>Building, testing, deploying from templates into stages is realized via a single (per product) pipeline.</p> \r\n<h2>Security</h2> \r\n<ol> \r\n<li><em>Principle of least privilege</em> is used for every resource.</li> \r\n<li>Encrypt everything.</li> \r\n</ol> \r\n<h3>Principle of least privilege is used for every resource</h3> \r\n<p>Don\'t give a service rights to do more than it actually should do. This could save you from an unpleasant surprise.</p> \r\n<p>Similar for multi-tenancy systems: the isolation of customer data must be enforced directly by underlying constrains, not only by the logic in code.</p> \r\n<h3>Encrypt everything</h3> \r\n<p>All the communication and all the data must be encrypted.</p> \r\n<p>It\'s a part of the contract where the keys are to be found.</p> \r\n<h2>Conclusion</h2> \r\n<p>Generally, the serverless development is not much different from a standard software development. The biggest difference is in the possibility (duty as well) for a developer to be an active part of the deployment process.</p> \r\n<p>Above I tried to summarize a few basic principles useful to follow in such a development.</p> \r\n<p>But the biggest lesson I have learned: (software) principles should never lead to dogma; they should provide a hint on unclear crossroads.</p> \r\n<p>Have a serverless day!</p>', 'Cloud,Serverless', 'false', 'true', 1, 2),
(45, 'package-by-component-with-clean-modules-in-java', 1546337000, 'Package by Component with Clean Modules in Java', '<p>Let\'s find the best combination of two good architectural approaches: Package by component and Clean architecture.</p>', '<p>Software architecture is about tradeoffs. Even when the theory is good the implementation details can break it.</p>\r\n\r\n<p>Package by component, as proposed by Simon Brown in his <a href=\"https://www.oreilly.com/library/view/clean-architecture-a/9780134494272/\" target=\"_blank\">Missing chapter</a>, is an architectural approach to <strong>organizing code based on bundling together everything related to a component</strong>. If done properly, it enforces architectural rules, such as not-bypassing the business logic in the infrastructure layer, by using only standard Java compiler mechanisms.</p> \r\n\r\n<h2>Enforced Separation of Concepts</h2> \r\n\r\n<p>In my experience, the absence of a strict mechanism like this will lead to skipping the rules eventually, especially when working under time pressure. Such a stress-driven architecture results in the unfamous <em>Big ball of mud</em>.</p> \r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component.png\" alt=\"Package by component\" /><br /><i class=\"caption\">Package by component</i></p> \r\n\r\n<p><em>Package by component</em> hides code private for the component under the <em>package</em> accessor and exposes API code with the <code>public</code> accessor. This makes details inaccessible from other components that don’t share the same package. In the example above, there is no way <code>OrderController</code> could access <code>OrderRepository</code>, which is exactly what we want.</p> \r\n\r\n<p>Unfortunately, there is still no reliable mechanism to prevent <code>OrderRepositoryJdbc</code> infrastructure adapter from accessing   <code>OrderServiceImpl</code> domain object directly. The obvious ease of using the implementation directly is very tempting. It seems that <strong>in the Package by component approach is enforcement of concepts separation not sufficient</strong>.</p> \r\n\r\n<h2>Modules to the Rescue</h2> \r\n\r\n<p>Fortunately, Java packages are not the only mechanism to separate concepts on the level of code. Build tools such as Maven and Gradle comes with their own mechanisms for creating code modules (projects). The direction of the artifact dependencies defines the separation of the <em>outside</em> from the <em>inside</em> explicitly.</p> \r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component_modules.png\" alt=\"Package by component with modules\"/><br /><i class=\"caption\">Package by component with modules</i></p> \r\n\r\n<p>As you can see in the picture, there are <strong>no changes in the packages</strong> structure. We’re still following the Package by component approach, but this time the <strong>domain and infrastructure parts are both separated into individual modules</strong>. Now, the domain artifacts has no dependencies on any infrastructure artifacts which makes accessing <code>OrderRepositoryJdbc</code> from <code>OderServiceImpl</code> cause a compilation error.</p> \r\n\r\n<p>We don’t have to stick with two modules only. A finer modules structure like <code>web</code>, <code>db</code>, and similar would be plausible.</p> \r\n\r\n<h2>Working Example</h2> \r\n\r\n<p>As a non-trivila example, we create a simple web application with a product catalog, a shopping cart and an ordering process.</p> \r\n\r\n<p>Spring framework is used to implement the web part and Spring Boot to glue all components together.</p> \r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Package-by-component-with-clean-modules.png\" alt=\"Package by component with clean modules\"/><br /><i class=\"caption\">Package by component with clean modules</i></p> \r\n\r\n<p>Modules create the familiar <em>onion</em> structure:</p> \r\n\r\n<ul> \r\n  <li>On the top there is the Spring Boot bootstrap layer - it represents a <strong>deployment scenario</strong>. Alternatively, there could be a WAR module for a web-container such as WildFly, an EAR module, and similar.</li> \r\n  <li>Spring framework (not Spring Boot), web UI, and a database implementation create the second infrastructure layer - <strong>technical details</strong>.</li> \r\n  <li>The domain layer lies always in the most inner circle - it containes <strong>business objects and policies</strong>.</li> \r\n</ul> \r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Clean-modules_MyShop.png\" alt=\"Clean onion structure\"/><br /><i class=\"caption\">Clean onion structure</i></p> \r\n\r\n<p>One more benefit emerges. With modules we can get rid of unnecessary dependencies in a declarative way. For example, if we want to add another implementation of the <code>OrderRepository</code> interface, such as <code>OrderRepositoryJpa</code>, we can split the <code>db</code> module into two separate modules <code>db-jdbc</code> and <code>db-jpa</code>. Then, the <strong>implementation can be easily changed just by adding or removing dependencies</strong>. <a href=\"/spring-boot-custom-components\">Spring Boot starters</a> use this approach very successfully.</p>\r\n\r\n<p class=\"warning\"><span class=\"title\">Warning:</span> Albeit clean modules are nice, they required additional maintenance. In extreme cases, this could lead to an explosion of modules and rapid increase of development costs. Therefore, use with caution!</p> \r\n\r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/myshop\" target=\"_blank\" title=\"MyShop source code on GitHub\">GitHub</a>.</p> \r\n\r\n<p class=\"note\"><span class=\"title\">Implementation note:</span> The purpose of the example code is to show the Package by component with modules approach. Nevertheless, it is not a perfect Domain-driven design code. Following the DDD principles seriously, we should see packages such as <code>com.ttulka.myshop.cart</code>, classes such as <code>ShoppingCart</code>, <code>OrderItem</code>, and, of course, tests!</p>\r\n\r\n<p>Happy packaging!</p> ', 'Software Architecture,Java', 'false', 'false', 1, 1),
(46, 'transducers-compose-top-to-bottom', 1548177000, 'Transducers Compose Top-to-Bottom', '<p>Let\'s code some transducers and take a deeper look at how they work under the hood to see differences from pipes.</p>', '<p>Transducers are composable reducers. A transducer takes a reducer and returns another reducer. Transducers compose via simple function composition. But, there is a tiny difference between function and transducer composition: functions compose bottom-to-top while transducers top-to-bottom.</p>\r\n\r\n<p class=\"note\"><span class=\"title\">Disclaimer:</span> This text presumes basic knowledge of <a href=\"https://medium.com/javascript-scene/transducers-efficient-data-processing-pipelines-in-javascript-7985330fe73d\" target=\"_blank\" title=\"Transducers\">transducers</a> and <a href=\"https://medium.com/javascript-scene/curry-and-function-composition-2c208d774983\" target=\"_blank\" title=\"Function composition\">function composition</a>.</p>\r\n\r\n<h2>Function Composition</h2>\r\n\r\n<p>First of all we discuss the basics: <em>function composition</em>.</p> \r\n<p>Function composition is the process of chaining the output of a function to the input of another function. In algebra, composition of functions <code>f</code> and <code>g</code> means <code>(f&nbsp;&sdot;&nbsp;g)(x)&nbsp;=&nbsp;f(g(x))</code>.</p> \r\n<p>In JavaScript we can write:</p> \r\n<pre class=\"brush: javascript\">const inc = (x) =&gt; x + 1;\r\nconst double = (x) =&gt; x * 2; \r\n\r\nconst incAndDouble = (x) =&gt; double(inc(x)); // compound function\r\n\r\nincAndDouble(2); // 6\r\n</pre> \r\n<p>Of course, we can do better! Let\'s create a general function compose:</p> \r\n<pre class=\"brush: javascript\">const compose = (...fns) =&gt; x =&gt; fns.reduceRight((y, f) =&gt; f(y), x);\r\n\r\nconst incAndDouble2 = compose(double, inc);\r\n\r\nincAndDouble2(2); // 6\r\n</pre> \r\n<p>As you can see, the functions are applied from bottom-to-top as in the definition <code>(f&nbsp;&sdot;&nbsp;g)(x)&nbsp;=&nbsp;f(g(x))</code>. It means, first <code>g(x)</code> is applied and, then, the output is used as the input for <code>f()</code>. If we want a composition applying from top-to-bottom, we can do it: this kind of composition is called a <em>pipe</em>:</p> \r\n<pre class=\"brush: javascript\">const pipe = (...fns) =&gt; x =&gt; fns.reduce((y, f) =&gt; f(y), x);\r\n\r\nconst incAndDouble3 = pipe(inc, double);\r\n \r\nincAndDouble3(2); // 6\r\n</pre>\r\n \r\n<h2>Transducer Composition</h2> \r\n<p>Probably the most useful is the mapping transducer. Let\'s define it:</p> \r\n<pre class=\"brush: javascript\">const map = f =&gt; step =&gt; (a, c) =&gt; step(a, f(c));\r\n</pre> \r\n<p>The <code>map</code> takes two curried parameters <code>f</code> and <code>step</code>. <code>f</code> is a mapping function and <code>step</code> is a reducer function to calculate (reduce) the result. We can peep at it with the following code:</p> \r\n<pre class=\"brush: javascript\">const testReducer = map(double)((a, c)&nbsp;=&gt; console.log(c));\r\n[1,2,3].reduce(testReducer, 0); // prints 2, 4, 6 into the console\r\n</pre> \r\n<p>Now we can use the composite function from above:</p> \r\n<pre class=\"brush: javascript\">const incAndDouble4 = compose(map(inc), map(double));\r\n\r\nconst concat = (a, c) =&gt; a.concat([c]);\r\n\r\nconst&nbsp;incAndDoubleReducer = incAndDouble4(concat);\r\n\r\n[1,2,3].reduce(incAndDoubleReducer, []); // 4, 6, 8\r\n</pre> \r\n<p>If you paid attention you maybe noticed one thing. Compare these two lines:</p> \r\n<pre class=\"brush: javascript\">const incAndDouble2 = compose(double, inc);\r\nconst incAndDouble4 = compose(map(inc), map(double));\r\n</pre> \r\n<p>We didn\'t redefine the compose function, the result remains the same, but the order of functions for the composition is the other way around. How is this possible? Let\'s analyse it...</p> \r\n<p>The composite function is pretty straight-forward: it takes a function from bottom-to-top, applies it (first to the init value <code>x</code>) and uses the output as the input for the next function. The difference between <code>double</code> and <code>map(double)</code> is, that <code>map(double)</code> returns a transducer function, not a scalar value as <code>double</code> does. This transducer function takes a parameter <code>step</code>, which is a reducer. It means, <code>f(y)</code> from the compose function is <code>transducer(reducer)</code>; after evaluating it we get a <code>step</code> reducer function, which is then used as a new input <code>y</code> for the next transducer. Well, it means those two lines above define different kinds of function:</p> \r\n<pre class=\"brush: javascript\">const incAndDouble2 = compose(double, inc); &nbsp;// function (x) =&gt; x\r\nconst incAndDouble4 = compose(map(inc), map(double)); &nbsp;// transducer (reducer) =&gt; reducer\r\n</pre> \r\n<p>Composition uses reducing in the bottom-to-top direction (<code>reduceRight</code>), the result of the composition is the top-most transducer function which contains the next (to-bottom direction) transducer\'s result reducer (in the <code>step</code> parameter). This next reducer is applied after the current reduction (mapping) is applied.</p> \r\n<p>We can write this particular composition function as:</p> \r\n<pre class=\"brush: javascript\">const compose = (...reducers) =&gt; initReducer =&gt; reducers.reduceRight(\r\n&nbsp; &nbsp; (previousReducer, currectTransducer) =&gt; currectTransducer(previousReducer), initReducer);\r\n</pre> \r\n<p>Each reducer in the composition has a reference to the previous reducer (<code>step</code>). The final reduction starts with the top-most reducer which applies its functionality (mapping) and then executes the referenced reducer (<code>step(...)</code>). This brings the reducing in the opposite direction (top-to-bottom) from the transducers composition (bottom-to-top).</p> \r\n<p>We can break down our example execution as following:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Transducing.png\" alt=\"Transducing\" /></p>\r\n<p>As we can see, the result reducer applies functions in order: <code>inc</code>, then <code>double</code>, then <code>concat</code>. As expected.</p> \r\n<p>Neat.</p> \r\n<p>Happy transducing!</p>  \r\n<p> </p>', 'Programming,JavaScript', 'false', 'false', 1, 1),
(47, 'challenges-for-serverless-blue-green-deployment', 1549306000, 'Challenges for Serverless Blue-Green Deployment', '<p>Serverless blue-green deployment is a nice practice. However, it doesn\'t come without challenges.</p>', '<p>I already blogged about the <a href=\"/serverless-blue-green-deployment\">serverless blue-green development</a> some time ago. I used it in practice a lot with very promising results. But there are challenges as well.</p>\r\n\r\n<p>First, let\'s briefly summarize the idea: We want to separately deploy a serverless service (stack of resources) and <strong>test it in isolation from another resources</strong>. Typically, we can achieve this through multi-account deployment, when several stages are separated via accounts (e.g. <code>dev</code>-<code>test</code>-<code>prod</code> standing for development, testing and production). We deploy changes into the <code>dev</code> stage, which is kind of playground for development, and then into the <code>test</code> stage where the tests are executed. If testing is successful, the service will be deployed into the <code>prod</code> account.</p> \r\n<p>This strategy works well until we take a look at the bill. Continuous development teaches us to have all the stages as similar as possible, ideally identical. If you\'re using expensive resources, your bill could be three (or more) times bigger.</p> \r\n<p>Serverless blue-green development is a way how to <strong>save some money</strong> by <strong>deploying only those resources really needed</strong> for the test and use already deployed and tested dependencies within a single account.</p> \r\n<p>Deploying any change results in a new stack of resources (blue), existing parallelly with the previous version (green). When tests succeed, the blue and the green stacks are switched, the blue becomes green and duplicated resources are removed.</p> \r\n<h2>Challenge: Unwanted Interactions</h2> \r\n<p>Consider a situation when a blue stack is triggered by some green resource. For example, a transformer is listening on a topic of upload events. Such &quot;green&quot; events could disturb our testing (our test emits &quot;blue&quot; test events) and devalue the test results. And what\'s more, if the transformation results are saved in a storage, like Amazon S3, the clean-up after the testing could be hard (it is not possible to delete a non-empty bucket).</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/Challenges-for-Serverless-BlueGreen-Deployment.png\" alt=\"Unwanted Interactions\"></p> \r\n<h3>Solution: Conditional Flags</h3> \r\n<p>Of course, it is not difficult to find a workaround for each such use-case (like to empty the whole bucket and distinguish &quot;green&quot; events from &quot;blue&quot; ones by an identifier, prefix or special attribute), but all those bring <strong>too much knowledge into tests</strong>. Knowledge we don\'t want to have and deal with.</p> \r\n<p>A solution is to use <strong>conditional flags</strong> in the stack template. For example, with AWS CloudFormation, it can look like this (Yaml):</p> \r\n<pre class=\"brush: yaml\">Parameters:\r\n  GreenDeployment:\r\n    Type: String\r\n    Description: \"This is a green deployment\"\r\n    AllowedPattern: \"true|false\"\r\n    ConstraintDescription: \"A boolean value\"\r\n    Default: false\r\n\r\nConditions:\r\n  GreenDeploy: !Equals [ !Ref GreenDeployment, true ]\r\n\r\nResources:\r\n  # The subscription to the topic only when \"green\" deployed\r\n  UploadEventsSubscription:\r\n    Type: AWS::SNS::Subscription\r\n    Condition: [GreenDeploy]\r\n    ...\r\n</pre> \r\n<p>For our example, we simply don\'t subscribe to the upload events topic when the stack is not being deployed as green.</p> \r\n<h2>Challenge: Expensive and Slow Resources</h2> \r\n<p>Even when created only for the time of testing, some resources can be really expensive or extremely slow, which can slow testing and the whole deployment process down. A typical example for both of these characteristics is Elasticsearch in AWS. To create such a service is expensive and very slow. So, how to deal with this?</p> \r\n<h3>Solution: Green Resources</h3> \r\n<p>In many cases using green resources (index clusters, databases) does no harm, especially if we clean up afterwards.</p> \r\n<p>We can use a condition for <strong>not to create the expensive resource and inject a green dependency instead</strong> (AWS CloudFormation):</p> \r\n<pre class=\"brush: yaml\">ResourceRef: !If [ GreenDeploy, !Ref MyExpensiveResource, !Ref GreenResourceRef ]\r\n</pre> \r\n<h3>Solution: Fake Resources</h3> \r\n<p>In testing we use <strong>test-doubles for expensive or slow services</strong>, and we can do the same here:</p> \r\n<pre class=\"brush: yaml\">ExpensiveFn:\r\n  Condition: [GreenDeploy]\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: expensive-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt ExpensiveFnRole.Arn\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; {\r\n          // do something very expensive and slow \r\n          // ...       \r\n          return \'success\'\r\n        }\r\n        \r\nFakeFn:\r\n  Condition: [BlueDeploy]\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: fake-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt FakeFnRole.Arn\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; \'success\'\r\n\r\nMyFn:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: my-function\r\n    Runtime: nodejs8.10\r\n    Handler: index.handler\r\n    Role: !GetAtt MyFnRole.Arn\r\n    Environment:\r\n      Variables:\r\n        PROCESSING_FN: !If [ GreenDeploy, !Ref ExpensiveFn, !Ref FakeFn ]\r\n    Code:\r\n      ZipFile: &gt;\r\n        exports.handler = event =&gt; {\r\n          // call the processing function\r\n          // ...\r\n        }\r\n</pre> \r\n<p>All the above listed solutions are implemented in the stack preparation phase, the <strong>tests stay completely agnostic</strong> without any additional knowledge.</p> \r\n<p>Happy deploying!</p>', 'Cloud,Serverless,Deployment', 'false', 'false', 1, 2),
(48, 'how-to-manage-aws-cloudformation-stack-dependencies', 1549475000, 'How to Manage AWS CloudFormation Stack Dependencies', '<p>Automated infrastructure (Infrastructure as Code) is essential to succeed (not only) in the cloud.</p>', '<p>AWS provides its own service for managing resource stacks: AWS&nbsp;CloudFormation. What are the options to manage dependencies between stacks, how to use them and which pros&nbsp;&amp;&nbsp;cons they have?</p>\r\n\r\n<p>In general, we have three options how to link resources from different stacks:</p> \r\n<p> </p> \r\n<ul> \r\n<li><strong>hard-coded</strong> in template code&nbsp;</li> \r\n<li>via <strong>stack parameters</strong></li> \r\n<li>via <strong>exports/imports</strong></li> \r\n</ul> \r\n<p> </p> \r\n<h2>Hard-Coded References</h2> \r\n<p>This is the most trivial variant as well as the most disadvantageous one. Let\'s say, the stack&nbsp;B needs a resource from the stack&nbsp;A:</p> \r\n<pre class=\"brush: yaml\">// stackA.yml\r\n\r\nServiceA:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: \"service-A\"\r\n    ...\r\n    \r\n// stackB.yml\r\n\r\nServiceB:\r\n  Type: AWS::Lambda::Function\r\n  Properties:\r\n    FunctionName: \"service-B\"\r\n    Environment:\r\n      Variables:\r\n        SERVICE_A: \"service-A\"\r\n    ...</pre> \r\n<p>Well, at least the dependency is set via an environment variable (it could be worse: the reference could be hard-coded direct in the function code), but it\'s still very impractical. The value of the variable must be changed either via a template code change, or manually, which breaks principles of Continuous Delivery. The service B is not informed about a potential change in the stack&nbsp;A, there is no validation that the dependency actually exists and is correct. A system built in this way is obviously brittle and can stop working anytime.</p> \r\n<h2>Stack Parameters&nbsp;</h2> \r\n<p>Setting references via stack parameters is not very different from hard-coded values, but it\'s definitely a&nbsp;small progress, because <strong>we can change parameter values via our continuous delivery process</strong> (pipeline). But there is still no guarantee that the value is correct.</p> \r\n<pre class=\"brush: yaml\">// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-A\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    \r\n// stackB.yml\r\n\r\nParameters:\r\n  ServiceA:\r\n    Type: String\r\n    Description: \"Reference to the Service A\"\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: !Ref ServiceA\r\n      ...</pre> \r\n<p>Because the stack&nbsp;A publishes the service A in its outputs, we can set the value even in an automation manner. But the problem with inconsistence, in case the resources has changed, remains.&nbsp;</p> \r\n<h2>Exports/Imports</h2> \r\n<p>The most secure way how to deal with stack dependencies in AWS CloudFormation is to use exports/imports. The exported (and somewhere imported) r<strong>esources are protected from changes</strong> and we get a handy overview of our dependencies out of the box.</p> \r\n<pre class=\"brush: yaml\">// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-A\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    Export:\r\n      Name: \"ServiceA\"\r\n    \r\n// stackB.yml\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: !ImportValue \"ServiceA\"\r\n      ...</pre> \r\n<p>Now, any <strong>change of the exported value will cause an integrity error</strong> and so we can be sure that our dependencies are always correct.&nbsp;</p> \r\n<h2>Parameterized Exports/Imports</h2> \r\n<p>The approach above is fine for small systems with only few stacks. As our system grows there are more and more stacks and we can easily lose the overview which resource belongs to which stack. A good practice here is to use the stack names as &quot;namespaces&quot; to group all the stack resources under the same prefix:</p> \r\n<pre class=\"brush: yaml\">// stackA.yml\r\n\r\nResources:\r\n  ServiceA:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: !Sub \"${AWS::StackName}-service-A-${AWS::Region}\"\r\n      ...\r\n\r\nOutputs:\r\n  ServiceA:\r\n    Description: \"Service A.\"\r\n    Value: !Ref ServiceA\r\n    Export:\r\n      Name: !Sub \"${AWS::StackName}-ServiceA\"</pre> \r\n<p>The question is, how to pass the name of the exported variable? We can hard-code it, but it will couple the template code with the stack name, which is&nbsp;undesirable, because the code shouldn\'t have any knowledge how stacks are deployed - named.</p> \r\n<p>Another option is to pass variable names as stack parameters, which could work fine, but it means hard and unnecessary effort, because, all in all, the <strong>names are part of the stack&nbsp;API and therefore mustn\'t change</strong> (only the stack name is variable).</p> \r\n<p>The&nbsp;compromise is to <strong>pass only the stack name as a parameter</strong>:&nbsp;</p> \r\n<pre class=\"brush: yaml\">// stackB.yml\r\n\r\nParameters:\r\n  StackNameA:\r\n    Type: String\r\n    Description: \"Name of the Stack A\"\r\n    \r\nResources:\r\n  ServiceB:\r\n    Type: AWS::Lambda::Function\r\n    Properties:\r\n      FunctionName: \"service-B\"\r\n      Environment:\r\n        Variables:\r\n          SERVICE_A: \r\n            Fn::ImportValue: {\"Fn::Sub\": \"${StackNameA}-ServiceA\"}\r\n      ...</pre> \r\n<p>With this approach we have all the benefits of <strong>exports/imports integrity</strong> while&nbsp;<strong>variability and deployment independence</strong> is preserved.</p> \r\n<p>Happy infrastructure coding!</p>', 'Cloud,AWS,Deployment', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(49, 'dealing-with-an-exception-is-one-thing', 1549708000, 'Dealing with an Exception Is One Thing', '<p>Nothing new, but I keep seeing violating of this rule again and again. Let\'s explain why it is bad.</p>', '<p>In his famous book Clean Code, Robert C. Martin writes:</p>\r\n<blockquote class=\"quote\">Functions should do one thing. Error handling is one thing.</blockquote>\r\n<p>This is a very simple principle, but I see it in action only rarely. What\'s actually wrong with that?:</p>\r\n<pre class=\"brush: java\">class TextFile {\r\n  private final Path path;\r\n    \r\n  TextFile(Path path) { this.path = path; }\r\n    \r\n  boolean save(String content) {\r\n    try {\r\n      byte[] bytes = content.getBytes(\"UTF-8\"); \r\n      Files.write(this.path, bytes);\r\n            \r\n      return true;\r\n            \r\n    } catch (IOException e) {\r\n      // log the exception\r\n    }\r\n    return false;\r\n  } \r\n}\r\n</pre>\r\n<p>The thing here is we are trying to inform the client about the fact the save operation was successful or not. But this is not what the method is supposed to do, <strong>the method should save the content, period</strong>.</p>\r\n<p>Exposing a boolean return value the method says to its client that the error is some kind of correct behavior. But the client doesn\'t expect such a behavior, the client simply expect the content to be saved. If not, it is an incorrect behaviour and an exception should occur.</p>\r\n<p>The <code>save</code> method is different from, for example, a <code>boolean&nbsp;readable()</code> method. We expect this method to answer the question if the file is readable or not. And <code>false</code> is a valid answer in this case.</p>\r\n\r\n<p>What\'s the right approach? If you prefer unchecked exceptions (I do), consider the following:</p>\r\n<pre class=\"brush: java\">class TextFile {\r\n  private final Path path;\r\n    \r\n  TextFile(Path path) { this.path = path; }\r\n    \r\n  void save(String content) {\r\n    try {\r\n      byte[] bytes = content.getBytes(\"UTF-8\"); \r\n      Files.write(this.path, bytes);\r\n            \r\n    } catch (IOException e) {\r\n      throw new FileNotSavedException(\"Cannot save a file: \" + path, e);\r\n    }\r\n  }\r\n}\r\n\r\nclass FileNotSavedException extends RuntimeException {\r\n  \r\n  FileNotSavedException(String message, Throwable cause) {\r\n     super(message, cause);\r\n  }\r\n} \r\n</pre>\r\n<p>Now, the <code>save</code> method is doing exactly what we expect from it: saves a context in a file. When this isn\'t for whatever reason possible, an exception is thrown and it\'s up to <strong>the client to deal with the situation</strong>.</p>\r\n\r\n<p>Have an exceptional day!</p>', 'Programming', 'false', 'false', 1, 1),
(50, 'product-releasing-pipeline-in-aws', 1550770000, 'Product Releasing Pipeline in AWS', '<p>Continuous delivery (CD) brings a lot of ideas essential for a modern software product deployment. In this article we discuss how to follow CD principles by building CD pipelines with an example in AWS.</p>', '<p>Taking CD principles seriously we have to keep in mind following: <strong>CD pipeline is a stream of changes and stages (actions) act like a filter</strong>.</p> \r\n<p>An example of a filter action is testing or a manual approval. When testing fails, the pipeline run is rejected and the delivery ends. Similarly, an approval is a barrier, allowing or permitting the pipeline to go on, rather than a trigger.</p> \r\n<p>Consider a simple CD pipeline for a service delivery:</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ServicePipeline.png\" alt=\"Service Pipeline\" /></p> \r\n<p>After a source change triggers a new pipeline instance, code is built and goes into a DEV stage. The stage consists of three actions: the stack is deployed, tested and approved eventually. The approval could be triggered automatically via an external process like some system testing or manually by a developer. <strong>No new changes go into the stage as long as the stage is not either approved or rejected</strong>. This prevents a newer commit from modifying the stage while testing it.</p> \r\n<p>When a commit is approved in the DEV stage, it can go on into the QA stage. In this stage QA tests and validations are proceeded. The approval actions prevents again new changes to be deployed while still testing. If no bugs are found, the stage is approved and the commit goes into the PROD stage (production), where it is deployed.</p> \r\n<h2>Continuous Delivery Example Scenario</h2> \r\n<p>Consider the following scenario of a continuous delivery process:</p> \r\n<ol> \r\n<li>The first commit <code>c1</code> was pushed. It triggers the pipeline and it\'s automatically built and deployed into the DEV stage.</li> \r\n<li>Integration tests run successfuly and the DEV stage was approved. The pipeline moves into another stage - QA - where the commit <code>c1</code> is deployed.</li> \r\n<li>Another commit <code>c2</code> was pushed. The pipeline is triggered, the commit is built and deployed in the DEV stage. The QA stage stays on the <code>c1</code>.</li> \r\n<li>The <code>c2</code> was approved in the DEV stage. QA testers are still busy with the <code>c1</code>, status quo.</li> \r\n<li>A new commit <code>c3</code> was pushed, built and deployed into the DEV stage. QA testers still have no results from the <code>c1</code>.</li> \r\n<li>The QA testers found a bug in the <code>c1</code> and the commit was rejected in the QA stage. The pipeline goes on and the next commit <code>c2</code> is deployed into the QA stage.</li> \r\n<li>A new commit <code>c4</code> was pushed, built and deployed into the DEV stage. QA testers are busy with the <code>c2</code> now.</li> \r\n<li>Meanwhile the <code>c4</code> was approved in the DEV stage and the next commit was deployed.</li> \r\n<li>A bug was found in the <code>c4</code> already in the DEV stage - propably via a system/end-to-end/GUI test - the commit was rejected.</li> \r\n<li>Hardworking QA testers found a bug in the <code>c2</code> as well and the commit was rejected. The pipeline goes on and the next commit <code>c3</code> is deployed into the QA stage.</li> \r\n<li>After some time of testing, QA testers validated and approved the <code>c3</code> and the commit was deployed into the PROD stage.</li> \r\n</ol> \r\n<p>The following table summarize the above scenario:</p> \r\n<p>\r\n<table cellspacing=\"0\" cellpadding=\"3\" border=\"1\" width=\"100%\"> \r\n<tbody> \r\n<tr> \r\n<th>Action</th> \r\n<th>Source+Building</th> \r\n<th>StagingDEV</th> \r\n<th>StagingQA</th> \r\n<th>StagingPROD</th> \r\n<th>Change</th> \r\n</tr> \r\n<tr> \r\n<td>commit c1</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c1 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c1 in DEV</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c1 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>commit c2</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c2 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c2 in DEV</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c2 &#10003;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>-</td> \r\n</tr> \r\n<tr> \r\n<td>commit c3</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c1 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c3 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>reject c1 in QA</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c2 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>commit c4</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>- </td> \r\n</tr> \r\n<tr> \r\n<td>approve c3 in DEV</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#9711;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c4 in DEV deployed</td> \r\n</tr> \r\n<tr> \r\n<td>reject c4 in DEV</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c2 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>- </td> \r\n</tr> \r\n<tr> \r\n<td>reject c2 in QA</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c3 &#9711;</td> \r\n<td style=\"text-align: center;\">-</td> \r\n<td>c3 in QA deployed</td> \r\n</tr> \r\n<tr> \r\n<td>approve c3 in QA</td> \r\n<td style=\"text-align: center;\">c4 &#10003;</td> \r\n<td style=\"text-align: center;\">c4 &#10007;</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td style=\"text-align: center;\">c3 &#10003;</td> \r\n<td>c3 in PROD deployed</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p>\r\n<ul> \r\n<li>&#9711; - in progress</li> \r\n<li>&#10003; - succeeded</li> \r\n<li>&#10007; - rejected</li> \r\n</ul> \r\n<h2>Product Releasing Pipeline</h2> \r\n<p><em>Continuous deployment</em> is a practice of deploying changes into the production automatically when developed, tested and validated via a DevOps team. This is not always the best idea, because a <strong>production release is actually a management decision</strong>.</p> \r\n<p>A <em>product</em> is a group of services forming an independent unit, typically delivered under a single name. The <strong>product services should be tested, validated and released at once</strong>.</p> \r\n<p>For such a scenario we would like to have a mechanism for approving all the related services together - a <em>product releasing pipeline</em>.</p> \r\n<p>A product releasing pipeline typically consists of system/end-to-end/GUI testing and several manual approval steps.</p> \r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ReleasingPipeline.png\" alt=\"Product Releasing Pipeline\" /></p> \r\n<p>When triggered, tests are run for the whole product (sytem) in the DEV environment. If the tests succeed, the changes are ready to go to the next stage - QA. This command action &quot;stage into QA&quot; is implemented as an approval of the previous (DEV) stage. Similarly, when QA testing is successful, changes are ready to be deployed into the production - implemented as an approval of the QA stage in service pipelines.</p> \r\n<h2>Example in AWS</h2> \r\n<p>AWS provides a fully managed CD pipeline solition: <a href=\"https://aws.amazon.com/codepipeline\" target=\"_blank\">AWS CodePipeline</a>.</p> \r\n<p>The releasing pipeline is triggered via a CloudWatch event (for example every nicht at 1:00), end-to-end tests are executed and when succeed a Lambda is invoked. The Lambda puts an approval result programmatically into all the service pipelines.</p> \r\n<p>Service pipeline names are prefixed with a product name - a parameter in the releasing pipeline. Service pipelines must follow this convention. Service pipeline stages must follow naming conventions as well (<code>approve</code> actions in the <code>StagingDEV</code> and <code>StagingQA</code> stages).</p> \r\n<p>Approving the QA stage and releasing into the production is implemented via a manual approval action. Then the Lambda is invoked as well as in the previous step.</p> \r\n<p>When a bug is found, the problem must be worked out individually - a problematic commit should be rejected in a particular service pipeline (the releasing pipeline don\'t reject delivery globally for all the service pipelines).</p> \r\n<p>You can find a sample implementation of a releasing pipeline and two service pipelines in <a href=\"https://github.com/ttulka/aws-samples/tree/master/releasing-pipeline\" target=\"_blank\">my GitHub</a>.</p> \r\n<p>Happy releasing!</p>', 'Cloud,AWS,Deployment', 'false', 'false', 1, 2),
(51, 'glass-box-testing-does-not-need-mocking', 1553245000, 'Glass-Box Testing Does Not Need Mocking', '<p>Black-box testing is testing of a component via its API without any knowledge of its implementation details. As the opposite there is the white-box testing. And it about testing implementation, right? Well, no...</p>', '<p>Indeed, while white-box testing we do see into the component, but that doesn\'t mean we access implementation details <em>directly</em>. What\'s more, <em>white</em> is the opposite of black, but white is not transparent after all...</p> \r\n<p>To avoid confusion, further we will strictly use its alternative name <em>glass-box testing</em>. Glass is transparent but impermeable - exactly like a real glass-box test! </p> \r\n<h2>Glass-Box Testing</h2> \r\n<p>What is glass-box testing actually? Even by glass-box testing we still <strong>access the component only via its API</strong>, but we do have some <strong>internal knowledge</strong> about what\'s going on inside (as glass is transparent), which makes use of it for better &quot;informed&quot; tests.</p> \r\n<p>For example to find interesting test cases and input combinations (as we usually can\'t test all possible variants of input exhaustively).</p> \r\n<p><strong>Without this internal knowledge we can\'t do code coverage</strong> - how can we know how many lines of code are test-covered, when we actually don\'t know the code?!</p> \r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/glassbox.png\" alt=\"Glass-box testing\" /> </p> \r\n<p>Consider an example code (in Java):</p> \r\n<pre class=\"brush: java\">interface Saver {\r\n\r\n  void save(UUID id, Object input);\r\n}\r\n</pre> \r\n<p>As the method <code>save</code> doesn\'t return anything (<code>void</code>), it is very difficult to test it only using its API contract (interface) - all we can do is to call the method with a combination of input parameters and check if no error occurs.</p> \r\n<p>Nevertheless, we can assume existence of a <code>load</code> method somewhere else:</p> \r\n<pre class=\"brush: java\">interface Loader {\r\n\r\n  Object load(UUID id);\r\n}\r\n</pre> \r\n<p>And, theoretically, we can use this interface to test if the record can be loaded after being saved.</p> \r\n<p>Such a design obviously sucks, operations for saving and loading objects should stick together in a single coherent contract:</p> \r\n<pre class=\"brush: java\">interface Repository {\r\n\r\n  void save(UUID id, Object input);\r\n    \r\n  Object load(UUID id);\r\n}\r\n</pre> \r\n<p>Now, we can easily test the whole scenario, because it really doesn\'t make sense anyway to test functions that are supposed to work with each other in isolation:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void inMemoryRepositoryTest() {\r\n  Repository repo = new InMemoryRepository();\r\n  UUID id = UUID.randomUUID();\r\n    \r\n  assertNull(repo.load(id));    \r\n    \r\n  repo.save(1, \"test\");\r\n        \r\n  assertEquals(\"test\", repo.load(id));\r\n}\r\n</pre> \r\n<h2>We Don\'t Need No Mocks!</h2> \r\n<p>Consider another example of a delivery service:</p> \r\n<pre class=\"brush: java\">interface DeliveryService {\r\n\r\n  void dispatch(String productId, String customerId);\r\n}\r\n</pre> \r\n<p>One domain rule says that a <em>Promo product</em> cannot be delivered:</p> \r\n<pre class=\"brush: java\">interface Product {\r\n  ...\r\n  boolean deliverable();\r\n}\r\n\r\nclass PromoProduct implements Product {\r\n\r\n  PromoProduct(String name, Double price) { ... }\r\n  ...\r\n  public boolean deliverable() { return false; }\r\n}\r\n</pre> \r\n<p>Our simple delivery service loads a product and a customer from their repositories and saves them into its repository to be proceeded:</p> \r\n<pre class=\"brush: java\">class SimpleDeliveryService implements DeliveryService {\r\n\r\n  private final Repository productRepo, customerRepo, deliveryRepo; \r\n\r\n  SimpleDeliveryService(\r\n      Repository productRepo, Repository customerRepo, Repository deliveryRepo) {\r\n    this.productRepo = productRepo;\r\n    this.customerRepo = customerRepo;\r\n    this.deliveryRepo = deliveryRepo;\r\n  }\r\n\r\n  public void dispatch(Long productId, Long customerId) {\r\n    Product product = this.productRepo.load(productId);\r\n        \r\n    if (!product.deliverable()) {\r\n      throw new UndeliverableProductException(product);\r\n    }\r\n                \r\n    Customer customer = this.customerRepo.load(customerId);\r\n        \r\n    Delivery delivery = new Delivery(product,  customer, LocalDateTime.now());\r\n    this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n  }\r\n}\r\n</pre> \r\n<p>For our domain rule we come up with a unit test:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {\r\n  Repository productRepo = mock(Repository.class);\r\n  UUID productId = UUID.randomUUID();\r\n  productRepo.save(productId, new PromoProduct(\"Test\", 12.3));\r\n    \r\n  Repository customerRepo = mock(Repository.class);\r\n  UUID customerId = UUID.randomUUID();\r\n  customerRepo.save(customerId, new Customer(\"John Smith\", \"Evergreen Terrace 123\"));   \r\n             \r\n  DeliveryService service = new SimpleDeliveryService(\r\n      productRepo, customerRepo, mock(Repository.class));    \r\n  try {\r\n    service.dispatch(productId, customerId);\r\n        \r\n    fail(\"Promo product should not be to deliver.\");\r\n\r\n  } catch (UndeliverableProductException ignore) {\r\n    // we expect this to happen    \r\n  }\r\n}\r\n</pre> \r\n<p>Uff, that was a lot of stuff we had to do to test such a simple rule. We had to mock three repository objects!</p> \r\n<p>Maybe we should re-think our contract once more:</p> \r\n<pre class=\"brush: java\">interface DeliveryService {\r\n\r\n  void dispatch(Product product, Customer customer);\r\n}\r\n\r\nclass SimpleDeliveryService implements DeliveryService {\r\n\r\n  private final Repository deliveryRepo; \r\n\r\n  SimpleDeliveryService(Repository deliveryRepo) {\r\n    this.deliveryRepo = deliveryRepo;\r\n  }\r\n\r\n  public void dispatch(Product product, Customer customer) {        \r\n    if (!product.deliverable()) {\r\n      throw new UndeliverableProductException(product);\r\n    }\r\n        \r\n    Delivery delivery = new Delivery(product, customer, LocalDateTime.now());\r\n    this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n  }\r\n}\r\n</pre> \r\n<p>Now, things got much easier, we saved several lines of code (to be test-covered) and two mocks:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {             \r\n  DeliveryService service = new SimpleDeliveryService(mock(Repository.class));    \r\n  try {\r\n    service.dispatch(\r\n        new PromoProduct(\"Test\", 12.3), \r\n        new Customer(\"John Smith\", \"Evergreen Terrace 123\")\r\n    );\r\n        \r\n    fail(\"Promo product should not be to deliver.\");\r\n\r\n  } catch (UndeliverableProductException ignore) {\r\n    // we expect this to happen    \r\n  }\r\n}\r\n</pre> \r\n<p>This is pretty cool, but can we go even further?</p> \r\n<pre class=\"brush: java\">class Delivery {\r\n\r\n  private final Product product;\r\n  private final Customer customer;\r\n  private final LocalDateTime createdAt;\r\n\r\n  Delivery(Product product, Customer customer) {\r\n    if (!product.deliverable()) {\r\n      throw new UndeliverableProductException(product);\r\n    }\r\n    this.product = product;\r\n    this.customer = customer;\r\n    this.createdAt = LocalDateTime.now();\r\n  }\r\n}\r\n\r\ninterface DeliveryService {\r\n\r\n  void dispatch(Delivery delivery);\r\n}\r\n\r\nclass SimpleDeliveryService implements DeliveryService {\r\n\r\n  private final Repository deliveryRepo; \r\n\r\n  SimpleDeliveryService(Repository deliveryRepo) {\r\n    this.deliveryRepo = deliveryRepo;\r\n  }\r\n\r\n  public void dispatch(Delivery delivery) {\r\n    this.deliveryRepo.save(UUID.randomUUID(), delivery);\r\n  }\r\n}\r\n</pre> \r\n<p>Our domain rule is now fully implemented in the <code>Delivery</code> domain object while <code>DeliveryService</code> was reduced just to integration with the external resource (repository).</p> \r\n<p>Consequently, we don\'t need mocks in the test anymore:</p> \r\n<pre class=\"brush: java\">@Test\r\npublic void shouldFailForPromoProductTest() {                \r\n  try {\r\n    new Delivery(\r\n        new PromoProduct(\"Test\", 12.3), \r\n        new Customer(\"John Smith\", \"Evergreen Terrace 123\")\r\n    );\r\n                \r\n    fail(\"Promo product should not be to deliver.\");\r\n\r\n  } catch (UndeliverableProductException ignore) {\r\n    // we expect this to happen   \r\n  }\r\n}\r\n</pre> \r\n<p>The <strong>domain rule is now test covered with a simple unit test</strong> without need to use (or mock) the delivery service.</p> \r\n<p><strong>Services must be tested as a part of integration testing</strong> against real resources (e.g. databases) to check if they are correctly integrated, but domain rules stays in the deepest part of the domain model and could be fully covered with simple unit tests without any need of mocking.</p> \r\n<p>Happy testing!</p>', 'Testing,OOP,Programming', 'false', 'false', 1, 1),
(52, 'double-testing', 1561126000, 'Double Testing', '<p>Write your tests once and run them twice - as both unit and integration tests - sounds like a good deal, let\'s take a look at this practice.</p>', '<p>This article focuses on Java, Spring framework and Maven.</p>\r\n\r\n<p>We have a modular application with two independent String MVC web components. We created a Spring Boot starter for each component to bring them together in a monolithic Spring Boot application.</p>\r\n\r\n<p>Each web component has its own set of tests. We have some <a href=\"http://rest-assured.io/\" target=\"_blank\">REST-Assured</a> tests for REST-based endpoints and Selenium-based tests for testing requests in the end-to-end manner (we check whether expected elements are present on the page).</p>\r\n\r\n<p>To run the tests independently as a part of the component build process we have to simulate the web environment and to mock (or somehow else provide) all needed resources. We can achieve this with <code>@SpringBootTest</code>:</p>\r\n<pre class=\"brush: java\">\r\n@SpringBootTest(\r\n    classes = IndexController.class /* this is what we test */,	\r\n    webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT\r\n)\r\n@EnableAutoConfiguration\r\nclass IndexTest {\r\n\r\n  @Value(\"http://localhost:${local.server.port:8080}\")\r\n  private String url;\r\n\r\n  @Test\r\n  void indexResource_shouldContainWeb1() {\r\n    given()\r\n        .baseUri(url)\r\n        .basePath(\"/\").\r\n    when()\r\n        .get().\r\n    then()\r\n        .statusCode(200)\r\n        .contentType(\"text/plain\")\r\n        .body(containsString(\"Web1\"));\r\n  }\r\n}\r\n</pre>\r\n<p>If you don\'t (or can\'t) include <code>@EnableAutoConfiguration</code> you can create your own annotation with a minimal web configuration from <code>spring-boot-starter-web</code>:\r\n<pre class=\"brush: java\">\r\n@Target(ElementType.TYPE)\r\n@Retention(RetentionPolicy.RUNTIME)\r\n@Documented\r\n@Configuration\r\n@Import({\r\n    ServletWebServerFactoryAutoConfiguration.class,\r\n    DispatcherServletAutoConfiguration.class,\r\n    WebMvcAutoConfiguration.class\r\n})\r\n@interface MinimalWebConfiguration {\r\n}\r\n</pre>    \r\n<p>To be sure that a component works in the application as well as running standalone, we can use the same tests, this time executed against the application. Let\'s call them integration tests and run them in a further phase of the application delivery process.</p> \r\n<p>Let\'s create an abstract class containing the whole test code:</p>\r\n<pre class=\"brush: java\">\r\nabstract class IndexTestBase {\r\n  @Test\r\n  void indexResource_shouldContainWeb1() {\r\n    // the test code...\r\n  }\r\n}\r\n</pre>    \r\n<p>Standalone (unit) tests use this base:</p>\r\n<pre class=\"brush: java\">\r\n// Spring Boot settings as above...\r\nclass IndexTest extends IndexTestBase {\r\n  // noting else in this class\r\n}\r\n</pre>\r\n<p>As well as the integration tests:</p>\r\n<pre class=\"brush: java\">\r\n@SpringJUnitConfig\r\nclass IndexIT extends IndexTestBase {\r\n  @Configuration\r\n  static class ITConfig {\r\n    // intentionally empty\r\n  }\r\n}\r\n</pre>\r\n<p>Alternatively to <code>@SpringJUnitConfig</code> you can use <code>@ExtendWith(SpringExtension.class)</code> together with <code>@ContextConfiguration</code> to gain a better overlook.</p>\r\n\r\n<p><code>IndexTest</code> is executed by Maven Surefire Plugin in the <code>test</code> phase, but <code>IndexIT</code> integration test is ignored by Surefire because it doesn\'t match the default naming pattern (<code>*Test</code>).</p>\r\n\r\n<h2>Application Integration</h2>\r\n<p>Web components are integrated into the application by adding their Spring Boot Starters as dependencies:\r\n<pre class=\"brush: xml\">\r\n&lt;dependency&gt;\r\n  &lt;groupId&gt;com.ttulka.samples.doubletesting&lt;/groupId&gt;\r\n  &lt;artifactId&gt;sample-component-web1-spring-boot-starter&lt;/artifactId&gt;\r\n&lt;/dependency&gt;\r\n</pre>\r\n<p>And setting different URL paths:</p>\r\n<pre class=\"brush: java\">\r\n@RestController(\"indexControllerWeb1\")	/* unique name to avoid a bean collision */\r\n@RequestMapping(\"${doubletesting.path.web1:}\")	/* integrated under an unique url path */\r\npublic class IndexController {\r\n  // endpoints definition...\r\n}\r\n</pre>\r\n<pre class=\"brush: yaml\">\r\n# application.yml\r\ndoubletesting.path:\r\n  web1: web1\r\n  web2: web2\r\n</pre>\r\n\r\n<h2>Running Integration Tests</h2>\r\n<p>Integration tests should run in a later phase of the application delivery process than unit tests. The reason is, integration tests are generally slower than unit tests (mainly due to time needed for building the environment) and running them in an earlier phase would delay the feedback, which is highly undesirable.</p>\r\n<p>We create a separate Maven module not included in the build phase at all, skipped with <code>-DskipITs</code> or alternatively by settings of a build profile.</p>\r\n<p>To run the tests from a component, first we have to collect them and include as a dependency. We use Maven JAR Plugin and its goal <code>test-jar</code> for that:\r\n<pre class=\"brush: xml\">\r\n&lt;!-- pom.xml of a component --&gt;\r\n&lt;build&gt;\r\n  &lt;plugins&gt;\r\n    &lt;plugin&gt;\r\n      &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;\r\n      &lt;executions&gt;\r\n        &lt;execution&gt;\r\n          &lt;id&gt;test-jar&lt;/id&gt;\r\n          &lt;goals&gt;\r\n            &lt;goal&gt;test-jar&lt;/goal&gt;\r\n          &lt;/goals&gt;\r\n        &lt;/execution&gt;\r\n      &lt;/executions&gt;\r\n    &lt;/plugin&gt;\r\n  &lt;/plugins&gt;\r\n&lt;/build&gt;\r\n</pre>\r\n<p>The plugin collects all the tests as a separate artifact, we then include in the integration test module:</p>\r\n<pre class=\"brush: xml\">\r\n&lt;!-- pom.xml of the integration tests module --&gt;\r\n&lt;dependencies&gt;\r\n  &lt;dependency&gt;\r\n    &lt;groupId&gt;com.ttulka.samples.doubletesting&lt;/groupId&gt;\r\n    &lt;artifactId&gt;sample-component-web1&lt;/artifactId&gt;\r\n    &lt;version&gt;0&lt;/version&gt;\r\n    &lt;type&gt;test-jar&lt;/type&gt;\r\n    &lt;scope&gt;test&lt;/scope&gt;\r\n  &lt;/dependency&gt;\r\n  &lt;!-- other dependencies... --&gt;\r\n&lt;/dependencies&gt;	\r\n</pre>\r\n<p>Then, all we need is to copy test classes with Maven Dependency Plugin and prepare the environment. We use Maven Failsafe Plugin and its phases <code>pre-integration-test</code> to start the application and <code>post-integration-test</code> to stop it and clean resources.</p>\r\n<p><code>IndexIT</code> will be executed automatically because it matches Failsafe naming pattern (<code>*IT</code>). Alternatively we can use tagging from JUnit 5 (<code>@Tag(\"integration\")</code>) together with Failsafe configuration (<code>groups</code>) for a finer control of test executions.</p>\r\n<p>We have to include all the test dependencies into the test pom.xml as dependencies with scope <code>test</code> don\'t come transitively.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/double-testing.png\" alt=\"Double Testing\"/></p>\r\n\r\n<p>A working sample project can be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/double-testing\" target=\"_blank\">my Github</a>.</p>\r\n\r\n<p>And that\'s it! One test code base, two different executions - unit and integration.</p>\r\n<p>Keep on testing!</p>', 'Testing,Programming', 'false', 'false', 1, 1),
(53, 'no-binaries-in-the-codebase', 1565110000, 'No Binaries in the Codebase', '<p>Binary data shouldn\'t be a part of the codebase. This is pretty well-known practice. But how to proceed when we do need binaries in our codebase, for instance as test data?</p>', '<p>The solution is straight-forward: Pack binary resources into an artifact separated from the codebase in a repository.</p>\r\n<p>A good practice is to use a <strong>special classifier</strong> for all the artifacts of this kind, for instance <code>testdata</code>.</p>\r\n\r\n<p>After uploading the resources artifact into a repository (manually or via an API), we can use it as a dependency in our codebase (Maven):</p>\r\n<pre class=\"brush: xml\">\r\n&lt;plugin&gt;\r\n  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\r\n  &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\r\n  &lt;executions&gt;\r\n    &lt;execution&gt;\r\n      &lt;id&gt;unpack-test-resources&lt;/id&gt;\r\n      &lt;phase&gt;process-test-resources&lt;/phase&gt;\r\n      &lt;goals&gt;\r\n        &lt;goal&gt;unpack&lt;/goal&gt;\r\n      &lt;/goals&gt;\r\n      &lt;configuration&gt;\r\n        &lt;outputDirectory&gt;${project.build.directory}/test-classes&lt;/outputDirectory&gt;\r\n        &lt;artifactItems&gt;\r\n          &lt;artifactItem&gt;\r\n            &lt;groupId&gt;com.ttulka.samples.testdata&lt;/groupId&gt;\r\n            &lt;artifactId&gt;sample-test-resources&lt;/artifactId&gt;\r\n            &lt;version&gt;1.0.0&lt;/version&gt;\r\n            &lt;classifier&gt;testdata&lt;/classifier&gt;\r\n            &lt;type&gt;zip&lt;/type&gt;\r\n          &lt;/artifactItem&gt;\r\n        &lt;/artifactItems&gt;\r\n      &lt;/configuration&gt;\r\n    &lt;/execution&gt;\r\n  &lt;/executions&gt;\r\n&lt;/plugin&gt;\r\n</pre>\r\n<p>The Maven plugin will download and unpack the resources into the test classpath before the test are actually executed.</p>\r\n<p>Binaries are then available in the codebase (Java):</p>\r\n<pre class=\"brush: java\">\r\nthis.getClass().getResource(\"/resource1.dat\");\r\n</pre>\r\n\r\n<p>Another good practice is to create a <strong>domain-based resources structure</strong> inside the artifacts. Just put the resources into a sub-folder:</p>\r\n<pre>\r\ndomainA.zip\r\n┕ domainA/\r\n  ┕ resource1.dat\r\n  ┕ resource2.dat\r\n</pre>\r\n<p>This organization enables composition of resources in the <a href=\"/double-testing\">integration testing</a>.</p>\r\n\r\n<p>The example code could be found in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/no-binaries-in-codebase/textfile\">my GitHub</a>.</p>\r\n\r\n<p>Happy resourcing!</p>', 'Testing,Programming', 'false', 'false', 1, 2),
(54, 'do-not-share-data-among-threads', 1567445000, 'Don\'t Share Data among Threads', '<p>How to proceed when shared data are needed?</p>', '<p>Distribution of a task among several threads means horizontal scaling - the more computing resources (processors) the less time to work the task out. Sharing data among threads brings the need to <strong>synchronize which kills the scaling capability</strong> of the computing.</p>\r\n\r\n<p>Is there a way out?</p>\r\n\r\n<p>Consider a very simple ETL system where the Extractor produces a finite sequence of numbers, the Transformer converts numbers into strings, and the Loader finally saves the string into a database. Because the database access is expensive, the Loader works in batches: first, collect data up to a limit batch size, then write the whole batch into the database. To use resources efficiently the Loader runs in multiple threads.</p> \r\n<p><img alt=\"Simple multi-thread ETL\" src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/ETL-with-shared-resources.png\" /></p> \r\n<p>And here comes trouble: shared data is introduced (batch collection) and methods must be synchronized. The result is almost the same or event worse than using a single thread without synchronization.</p> \r\n<p>\r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, sync</th> \r\n<th>2 threads, sync</th> \r\n<th>4 threads, sync</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>50.768 ms</td> \r\n<td>51.083 ms</td> \r\n<td>51.674 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p>\r\n<p>Some optimization to minimize the size of the critical section are practicable, but not always. How to avoid synchronization as much as possible?</p> \r\n<h2>Thread Own Data</h2> \r\n<p>One approach is to let each thread to own its data. Such data doesn\'t have to be synchronized. The drawback is a design-shift of the Loader from a simple implementation to a threading-aware one:</p> \r\n<pre class=\"brush: java\">class BatchLoaderThreadOwnData {\r\n\r\n    private final int batchSize;\r\n    private final JdbcTemplate jdbcTemplate;\r\n\r\n    private boolean finished = false;\r\n\r\n    // the object holds a map of data for each thread\r\n    // the only one concurrent access, therefore synchronized\r\n    private final Map&lt;Long, ThreadOwnData&gt; threadOwnDataMap = new ConcurrentHashMap&lt;&gt;();\r\n\r\n    public void load(String result) {\r\n        threadOwnData().resultsBatch.add(result);\r\n\r\n        if (finished || threadOwnData().counter++ &gt;= batchSize) {\r\n            batchLoad(threadOwnData());\r\n        }\r\n    }\r\n\r\n    public void finish() {\r\n        finished = true;\r\n        threadOwnDataMap.values().forEach(this::batchLoad);\r\n    }\r\n    \r\n    private ThreadOwnData threadOwnData() {\r\n        return threadOwnDataMap.computeIfAbsent(\r\n                  Thread.currentThread().getId(), \r\n                  id -&gt; new ThreadOwnData(batchSize));\r\n    }\r\n    \r\n    // ...\r\n}\r\n</pre> \r\n<p>The results are actually much better:</p> \r\n<p>\r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, own data</th> \r\n<th>2 threads, own data</th> \r\n<th>4 threads, own data</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>52.129 ms</td> \r\n<td>29.668 ms</td> \r\n<td>20.050 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p>\r\n<h2>Instance per Thread</h2> \r\n<p>We can go a step further and let a tread own the whole object. This allows us to reuse the first thread-unsafe version just as with a single thread. The drawback is a complicated execution logic and the need to create new instances, which is not always possible:</p> \r\n<pre class=\"brush: java\">// the only synchronization here \r\nMap&lt;Long, BatchLoaderUnsafe&gt; batchLoaders = new ConcurrentHashMap&lt;&gt;();\r\n// ...\r\n// run in an async executor:\r\nBatchLoaderUnsafe loader = batchLoaders.computeIfAbsent(\r\n    Thread.currentThread().getId(),\r\n    id -&gt; new BatchLoaderUnsafe(BATCH_SIZE, jdbcTemplate));\r\n// call it on an unsynchronized thread-owned loader\r\nloader.load(i); \r\n</pre> \r\n<p>The results are, as expected, comparable to the previous solution:</p> \r\n<p>\r\n<table border=\"1\" cellpadding=\"3\" cellspacing=\"0\"> \r\n<thead> \r\n<tr> \r\n<th>Single thread, unsafe</th> \r\n<th>1 thread, thread own</th> \r\n<th>2 threads, thread own</th> \r\n<th>4 threads, thread own</th> \r\n</tr> \r\n</thead> \r\n<tbody> \r\n<tr> \r\n<td>51.446 ms</td> \r\n<td>51.919 ms</td> \r\n<td>30.051 ms</td> \r\n<td>19.814 ms</td> \r\n</tr> \r\n</tbody> \r\n</table> \r\n</p>\r\n<h2>Conclusion</h2> \r\n<p>In the modern world of microservices and distributed computing is scalability one of the most important attributes. Techniques like immutability are not always applicable, but synchronization must still be reduced to minimum. When not done so, the performance could be even worse when running in a single thread as threading overheads must be paid.</p> \r\n<p>When it is not possible to create a new worker instance for a thread, the worker could be designed to create a new instance of data per thread. When every thread owns its data, there is no need for further synchronization.</p> \r\n<p>The whole code could be found in <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/shared-resources-among-threads\">my GitHub</a>.</p> \r\n<p>Happy threading!</p>', 'Programming', 'false', 'false', 1, 1),
(55, 'domain-driven-serverless-design', 1568462000, 'Domain-Driven Serverless Design', '<p>One reason I really like the serverless architecture approach is being pretty selfish: one has to care only about what matters - the code.</p>', '<p>Well, I know code is not everything, but as a developer, I\'m just having more fun coding than scripting infrastructure in YAML or similar. For people like me is the serverless model a dream come true. But how to do serverless without turning the dream into a nightmare?</p>\r\n\r\n<p>It\'s <a href=\"https://martinfowler.com/bliki/MonolithFirst.html\" target=\"_blank\">well known</a> that the microservices-first approach leads often to a failure. The point here is to know the domain well before splitting the system up into autonomous services. Once split up refactoring across boundaries becomes difficult (or even impossible) due to the lack of collective code ownership. Werner Vogels\' famous statement says &quot;<a href=\"https://www.allthingsdistributed.com/2016/03/10-lessons-from-10-years-of-aws.html\" target=\"_blank\">APIs are forever</a>&quot;, once published the interface cannot be changed. Without knowing the domain well one usually ends up with a <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\" target=\"_blank\">CRUD</a>-like <a href=\"https://www.michaelnygard.com/blog/2017/12/the-entity-service-antipattern/\" target=\"_blank\">entity services</a>, which wakes him up&nbsp;every night&nbsp;in a lather of sweat.</p> \r\n<p>Let\'s illustrate this with an example.&nbsp;A CRUD-like entity service looks like this:</p> \r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-1.png\" alt=\"Entity Service\" /> </p> \r\n<p>We have here one (micro)service built around the Car entity and five functions (some people call them <em>nanoservices</em>) implementing its CRUD operations. The potential database or a storage is an internal part of the service and it\'s not accessible or visible to the outer world.</p> \r\n<p>Consider a car rental company with a web page displaying a list of cars available to rent. With only entity services the page controller must retrieve a list of all cars from the Cart service, then a list of all rentals from the Rental service and finally match cars not included in any rentals:</p> \r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-2.png\" alt=\"Multiple requests\" /> </p> \r\n<p>Even in this simple scenario there are several problems:</p> \r\n<p> </p> \r\n<ul> \r\n<li>Knowledge of Car and Rental entities on the client side leads to tight-coupling of the services.</li> \r\n<li>Availability of the feature relies on all involved services.</li> \r\n<li>Multiple synchronous requests result to a lot of overhead and increase costs rapidly.</li> \r\n<li>Transferring more data really needed means throughput waste.</li> \r\n<li>Complex communication makes the system difficult to reason about.</li> \r\n</ul> \r\n\r\n<p>One can easily image a more complex scenario where a function calls a function which call a function... This ends up not only in mess but in a very expensive mess, as synchronous calls in functions are charged for both the blocked caller and the blocking callee.</p> \r\n<p>Synchronization always means coupling. Serverless systems are great for an asynchronous communication, which is however not always possible. Fortunately, there are several options how to tame this beast. Using tools like AWS Step Functions or Azure Logic Apps can optimize the composition of function calls, but it\'s still not applicable everywhere. The solution is to design the services in a way they don\'t need to make any synchronous calls whatsoever - make them <a href=\"https://en.wikipedia.org/wiki/Domain-driven_design\" target=\"_blank\">domain-driven</a>!</p> \r\n<p>How would the scenario be implemented in the domain-driven style? Well, what is the feature here? The controller method already told us: <em>cars to rent</em>.</p> \r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/domain-driven-serverless-3.png\" alt=\"Cars to rent\" /> </p> \r\n<p>Now, the controller makes only one single request to get a list of cars available to rent, exactly what asked for. Further there will be function like <code>rent-a-car</code>, <code>return-a-car</code> or <code>extend-a-rental</code>. All of those are autonomous, which means, they have all they need to work the feature out. Again, the service contains its own data, gathered for example by an event listening function like <code>a-new-car-stored</code>&nbsp;and similar.</p> \r\n<p>Important to notice is the names of new functions - all are domain-driven. Technical concepts like <em>create</em> or <em>delete</em> disappeared from the model completely and that\'s the whole point.</p> \r\n<p>To summarize it:</p> \r\n<ol> \r\n<li>Know your domain well,</li> \r\n<li>build services around the domain,</li> \r\n<li>sleep well in the night.&nbsp;</li> \r\n</ol> \r\n<p>Before we reach the first point, we should forget not only about serverless but microservices as well. First, a <a href=\"https://speakerdeck.com/axelfontaine/majestic-modular-monoliths\" target=\"_blank\">monolith</a> is the right way to go. Growing up enough to know the domain boundaries well, we can start with big services. Splitting them up into serverless functions is the last step.</p> \r\n', 'DDD,Cloud,Serverless,Software Architecture', 'false', 'false', 1, 1),
(64, 'how-i-do-tdd', 1573988050, 'How I Do TDD', '<p>I really like Test-Driven Development (TDD) and apply it almost always. The problem with TDD is that it focuses too much on <i>working software</i>.</p>', '<p><a href=\"https://www.oreilly.com/library/view/test-driven-development/0321146530/\" target=\"_blank\">Kent Beck</a> says that his father told him:</p>\r\n<blockquote class=\"quote\">Make it work, make it right, make it fast.</blockquote>\r\n\r\n<p>Don\'t get me wrong, code must work, but that just shouldn\'t be the number one priority.</p>\r\n\r\n<p align=\"center\"><blockquote class=\"twitter-tweet tw-align-center\"><p lang=\"en\" dir=\"ltr\">It is more important for code to be changeable than that it work. Code that does not work, but that is easy to change, can be made to work with minimum effort. Code that works but that is hard to change will soon not work and be hard to get working again.</p>&mdash; Uncle Bob Martin (@unclebobmartin) <a href=\"https://twitter.com/unclebobmartin/status/1192392951294500864?ref_src=twsrc%5Etfw\">November 7, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\r\n\r\n<p>In TDD, design and refactoring towards a better architecture come first when the feature is already implemented and tests are green. Refactoring then often needs to rewrite and delete a lot of code, activity which not everyone likes to do, especially when the code has just been written.</p>\r\n\r\n<p>Taking TDD dogmatically so often leads to a perfectly working code with a poor design.</p>\r\n\r\n<p>I find more eligible to apply TDD first in the implementation phase. The process of development so looks like follows:</p>\r\n\r\n<ol>\r\n	<li>Understand the problem.</li>\r\n	<li>Design the API clearly without any implementation concerns.</li>\r\n	<li>Implement requirements one by one applying TDD.</li>\r\n</ol>\r\n\r\n<p>This approach ensures a well-designed architecture with all benefits of TDD.</p>\r\n\r\n<p>It\'s still necessary to keep <i>\"make it right\"</i> after <i>\"make it work\"</i> as it means optimization and polishing the code like removing duplicates, restructuring methods, renaming variables etc. The API should remain untouched and the tests ensure all is still working.</p>\r\n\r\n<p>To paraphrase Kent:</p>\r\n\r\n<blockquote class=\"quote\">Understand it, design it, make it work, make it right.</blockquote>\r\n\r\n<p>Happy testing!</p>\r\n', 'Testing', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(65, 'keep-test-code-inside-the-test', 1574553600, 'Keep Test Code inside the Test', '<p>Noone wants to write one thing twice. Reducing duplicates makes code shorter and clearer. How much this applies for test code?</p>', '<p>A few days ago I was pair-programming with another developer in the <a href=\"/how-i-do-tdd\">Test-Driven Development</a> manner. TDD and pair-programming is the best fit. I usually apply TDD after having understood the problem and designed the API, in the implementation phase. This approach helps me reduce the need for later refactoring and avoid testing of trivial code.</p>\r\n\r\n<p>Having the requirement reflected in the API, we wrote a test:</p>\r\n\r\n<pre class=\"brush: java\">\r\npublic interface Account {\r\n\r\n  boolean canLogin(String password);\r\n}\r\n\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n  Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n\r\n  assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>Then we implemented the feature and wrote the next test:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_cannot_login_with_an_invalid_password() {\r\n  Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n\r\n  assertThat(account.canLogin(\"xxx\")).isFalse();\r\n}\r\n</pre>\r\n\r\n<p>After implementing this we continued:</p>\r\n\r\n<pre class=\"brush: java\">\r\npublic interface Account {\r\n\r\n  boolean canLogin(String password);\r\n	\r\n  void changePassword(String newPassword);\r\n}\r\n\r\n@Test\r\nvoid password_is_changed() {\r\n  Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n	\r\n  account.changePassword(\"updated\");\r\n\r\n  assertThat(account.canLogin(\"updated\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>At that moment my partner proposed to move the first line of the tests into a separate method, like this:</p>\r\n\r\n<pre class=\"brush: java\">\r\nprivate Account account;\r\n    \r\n@BeforeEach\r\nvoid setupAccount() {\r\n  this.account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\");\r\n}\r\n</pre>\r\n\r\n<p>It does follow the Don\'t Repeat Yourself principle and makes the test methods shorter, right? Look:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n  assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>Sounds well first, however, there are several issues.</p>\r\n\r\n<h2>Don\'t Share Code among Tests</h2>\r\n\r\n<p>Sharing code among different requirements is a bad idea, because business is volatile and the <a href=\"http://verraes.net/2014/08/dry-is-about-knowledge/\">rules might change independently</a>. Doing so comes with risks and drawbacks:</p>\r\n\r\n<h3>1. It\'s not obvious how the test is set up</h3>\r\n\r\n<p>Looking at the test only, its not clear what is the arrangement of the test, how is the unit initialized and set up. To understand this, we have to scroll up to the setup method.</p>\r\n\r\n<h3>2. Tests are coupled</h3> \r\n\r\n<p>It is impossible to have different setups for different tests, although this is often desired. Consider the following test:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_with_no_email_cannot_login() {\r\n  Account account = new UserAccount(\"test\", null, \"pwd1\");\r\n\r\n  assertThat(account.canLogin(\"pwd1\")).isFalse();\r\n}\r\n</pre>\r\n\r\n<p>Such a dilemma often leads to a complex setup method trying to do more than one thing, which makes the test suite difficult to understand and maintain.</p>\r\n\r\n<h3>3. Tests are not well isolated</h3>\r\n\r\n<p>Tests must run in isolation from each other. Tests must never be meant to run in a particular order. Consider the following tests:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid user_can_login_with_a_valid_password() {\r\n  assertThat(account.canLogin(\"pwd1\")).isTrue();\r\n}\r\n\r\n@Test\r\nvoid password_is_changed() {	\r\n  account.changePassword(\"updated\");\r\n	\r\n  assertThat(account.canLogin(\"updated\")).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>When the second test runs before the first one, it fails.</p>\r\n\r\n<p>Even not frequent in practice, it should always be possible to run the tests in parallel. Bearing this in mind helps you to write well isolated tests.</p>\r\n\r\n<h3>4. Unnecessary work</h3>\r\n\r\n<p>Consider another test in the test suite:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid empty_password_raises_an_error() {\r\n  assertThrows(InvalidPasswordException.class, () -&gt;\r\n      new UserAccount(\"test\", \"test@example.com\", \"\"));\r\n}\r\n</pre>\r\n\r\n<p>Even not needed at all, the setup method is executed and resources created, which is just a waste.</p>\r\n\r\n<h2>Don\'t Share Dependencies amoung Tests</h2>\r\n\r\n<p>The same is true for dependencies. Instead of doing this (with Spring):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Autowired\r\nprivate AccountRegistry registry;\r\n\r\n@Value(\"${test.admin.username}\")\r\nprivate String adminUsername;\r\n    \r\n@Test\r\nvoid registered_user_account_is_in_the_registry() {\r\n  Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\", registry);	\r\n  account.register();\r\n	\r\n  assertThat(registry.byUsername(\"test\").isPresent()).isTrue();\r\n}\r\n\r\n@Test\r\nvoid admin_account_has_admin_rights() {\r\n  Account account = new UserAccount(adminUsername, \"test@example.com\", \"pwd1\");\r\n	\r\n  assertThat(account.hasAdminRights()).isTrue();\r\n}\r\n</pre>\r\n\r\n<p>...it\'s way better to this (with Spring and JUnit 5):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid registered_user_account_is_in_the_registry(@Autowired AccountRegistry registry) {\r\n  Account account = new UserAccount(\"test\", \"test@example.com\", \"pwd1\", registry);	\r\n  account.register();\r\n	\r\n  assertThat(registry.byUsername(\"test\").isPresent()).isTrue();\r\n}\r\n\r\n@Test\r\nvoid admin_account_has_admin_rights(@Value(\"${test.admin.username}\") String username) {\r\n  Account account = new UserAccount(username, \"test@example.com\", \"pwd1\");\r\n	\r\n  assertThat(account.hasAdminRights()).isTrue();\r\n}\r\n</pre>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Write your tests so that everything the test needs is included inside the test. Letting the test code go out of its boundaries makes the test dependent on its context, which brings unnecessary complexity into the test suite.</p>\r\n\r\n<p>Benefits of reducing code duplicity don\'t really count here, because having hard-to-maintain tests pays a much higher price than a few extra lines of code.</p>\r\n\r\n<p>Happy testing!</p>', 'Testing,Programming', 'false', 'false', 1, 1),
(66, 'treat-data-as-data', 1575206000, 'Treat Data as Data', '<p>Object-oriented approach is a mighty concept making software more maintainable, which means cheaper and easier to understand. Problems come at boundaries, where objects have to be passed on into a different layer or another system. There, the objects become just data and should be treated like that.</p>', '<p>This problem is not new, was already noticed by <a href=\"https://blog.ploeh.dk/2011/05/31/AttheBoundaries,ApplicationsareNotObject-Oriented/\">Mark Seemann</a> and many others. Michael Nygard writes in <i>Release It!</i>:</p>\r\n<blockquote class=\"quote\">What appears as a class in one layer should be mere data to every other layer.</blockquote>\r\n\r\n<p>Objects in the domain layer are <strong>\"living\" active entities defined by their behavior</strong>. But what happens when an object needs to be persisted, or for example displayed on the screen? What is an object for a database? How is an object represented in a message or in a response of a REST call? It\'s mere data.</p>\r\n\r\n<p>As an example, consider a REST controller returning an object as JSON. With Spring framework we typically do it as follows:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@GetMapping\r\npublic Account login(String username, String password) {\r\n  return accounts.login(username, password);\r\n}\r\n</pre>\r\n\r\n<p>Because <code>Account</code> provides a <code>getUsername()</code> method, the response looks like:</p>\r\n\r\n<pre class=\"brush: json\">\r\n{\r\n  \"username\": \"test\" \r\n}\r\n</pre>\r\n\r\n<p>The problem with this approach is that the domain layer (<code>Account</code>) is highly coupled to the application layer (controller): a change in the domain object can lead to a broken contract. Another problem is leaking of implementation details: adding a new getter to the domain object will change the response as follows:</p>\r\n\r\n<pre class=\"brush: json\">\r\n{\r\n  \"username\": \"test\",\r\n  \"password\": \"pwd1\" \r\n}\r\n</pre>\r\n\r\n<p>Another obvious problem is, that Spring requires getters to convert the domain object to its serialized representation (e.g. JSON). This breaks encapsulation of the object and leads to a shift from the OOP understanding of objects as a unit of behavior to treating objects as poor data structures with a bunch of attached procedures.</p>\r\n\r\n<h2>Data Is Data</h2>\r\n<p>The solution is to understand this gap and to <strong>treat objects as objects and data as data</strong>. A traditional way of doing this is known as Data Transfer Object (DTO):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@GetMapping\r\npublic LoginDTO login(String username, String password) {\r\n  Account account = accounts.login(username, password);\r\n  return new LoginDTO(account.getUsername());\r\n}\r\n\r\nclass LoginDTO {\r\n\r\n  private final String username;\r\n    \r\n  public LoginDTO(String username) {\r\n    this.username = username;\r\n  }\r\n    \r\n  public String getUsername() {\r\n    return this.username;\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Creating a DTO for all use-cases is a lot of hard work. Is it really worth? What are the actual benefits of DTOs? I can think of two:</p>\r\n<ol>\r\n    <li>Static typing</li>\r\n    <li>Explicit structure</li>\r\n</ol>\r\n\r\n<h3>Static Typing</h3>\r\n<p>Static typing is a big benefit of strongly typed languages that brings a good level of confidence as a lot of bugs are discovered already during compilation. The problem here is, that no input/output is actually strongly typed. Consider the following endpoint:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(\r\n    @RequestBody ToRegisterDTO toRegister) {\r\n  accounts.register(\r\n      toRegister.getUsername(), \r\n      toRegister.getPassword(), \r\n      toRegister.getEmail());\r\n}\r\n\r\nstatic class ToRegisterDTO {\r\n\r\n  private final String username;\r\n  private final String password;\r\n  private final String email;\r\n    \r\n  // constructor and getters...\r\n}\r\n</pre>\r\n\r\n<p>The method expects the input in the following format:</p>\r\n<pre class=\"brush: json\">\r\n{\r\n  \"username\": ...,\r\n  \"password\": ...,\r\n  \"email\": ...\r\n}\r\n</pre>\r\n\r\n<p>But there is nothing to prevent the client to send anything different:</p>\r\n<pre class=\"brush: json\">\r\n{\r\n  \"UserName\": ...,\r\n  \"pass\": ...,\r\n  \"e-mail\": ...\r\n}\r\n</pre>\r\n<p>The typical solution is to validate the input, but <strong>no typing will help us here</strong>. So why should we bother?</p>\r\n\r\n<h3>Explicit Structure</h3>\r\n<p>Explicit is always good, but DTOs create a lot boiler-plate code and are not much different to pure data structures. Compare the following variants:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(\r\n    @RequestBody ToRegisterDTO toRegister) {\r\n  accounts.register(\r\n      toRegister.getUsername(), \r\n      toRegister.getPassword(), \r\n      toRegister.getEmail());\r\n}\r\n\r\n// vs.\r\n\r\n@PostMapping\r\n@ResponseStatus(HttpStatus.CREATED)\r\npublic void register(\r\n    @RequestBody Map&lt;String, String&gt; toRegister) {\r\n  accounts.register(\r\n      toRegister.get(\"username\"), \r\n      toRegister.get(\"password\"), \r\n      toRegister.get(\"email\"));\r\n}\r\n</pre>\r\n\r\n<p>In my opinion there is no big difference, in the second variant a lot of code for DTOs disappeared (the less to maintain the better) and the code tells the reader much more explicitly that data, no objects, are to be found here.</p>\r\n<p>A Data Transfer Objects are, despite the name, no object at all. There are nothing more than strongly typed data structures. As the strong typing doesn\'t bring a great value, explicit data structures like Map and List could be used instead.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>Whether using DTOs or standard data structures, <strong>dealing with data should be explicit and obvious from the code</strong>. Leaking domain beyond layer boundaries increase coupling and can undesirably affect the API. This risk should be avoided by treating data as data clearly and explicitly. Objects are units of behavior which can\'t cross layers. This fact should be reflected in the code.</p>\r\n\r\n<p>The source code could be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/objects-at-boundaries\" target=\"_blank\">GitHub</a>.\r\n\r\n<p>Happy coding!</p>', 'OOP,Design Patterns', 'false', 'false', 1, 1),
(67, 'how-to-test-abstract-classes', 1575653000, 'How to Test Abstract Classes', '<p>Abstract classes typically offer one or more concrete methods. These must be tested as well. There are several ways how to do it, but which one to choose?</p>', '<p>Consider an abstract class with a concrete method (Java):</p>\r\n<pre class=\"brush: java\">\r\nabstract class Person {\r\n\r\n  protected final String firstName;\r\n  protected final String lastName;\r\n\r\n  public Person(String firstName, String lastName) {\r\n    this.firstName = firstName;\r\n    this.lastName = lastName;\r\n  }\r\n\r\n  public String fullName() {\r\n    return String.format(\"%s %s\", firstName, lastName);\r\n  }\r\n  \r\n  public abstract String greeting();\r\n}\r\n</pre>\r\n\r\n<p>What are the possibilities to test such a class?</p>\r\n\r\n<h2>Anonymous Classes</h2>\r\n\r\n<p>We can create an ad-hoc instance of the abstract class and stub the abstract method which are not under test:</p>\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid full_name_is_provided() {\r\n  Person person = new Person(\"John\", \"Smith\") {\r\n    @Override\r\n    public String greeting() {\r\n      return null;\r\n    }\r\n  };\r\n  assertEquals(\"John Smith\", person.fullName());\r\n}\r\n</pre>\r\n\r\n<p>But such a code is pretty cumbersome and hard to read. Just image a class with multiple abstract methods, the test code becomes longer and longer and breaks one of the main properties of a good test - readability.</p>\r\n<p>Another problem is the <strong>coupling between the test and implementation</strong>. Even that the method is marked as <code>final</code>, the code should focus only on the contract and not on such implementation details as details can change in the future and a good test should be resistant to refactoring.</p>\r\n\r\n<p>Creating an anonymous class doesn\'t seem to be the best option. Can we do better?</p>\r\n\r\n<h2>Mocking</h2>\r\n\r\n<p>This is probably the first advice you get if you search on the Internet. With mocking everything is easy (Mockito):</p>\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid full_name_is_provided() {\r\n  Person person = mock(Person.class, withSettings()\r\n      .useConstructor(\"John\", \"Smith\")\r\n      .defaultAnswer(CALLS_REAL_METHODS));\r\n\r\n  assertEquals(\"John Smith\", person.fullName());\r\n}\r\n</pre>\r\n\r\n<p>Mocking an abstract class is practically just like creating an anonymous class but using convenient tools. It has the same drawbacks and, again, it\'s probably not the best option we have.</p>\r\n\r\n\r\n<h2>Concrete Class</h2>\r\n\r\n<p>An abstract class makes actually no sense without being extended with a concrete class. <strong>A concrete class is where the requirements must be met.</strong></p>\r\n<pre class=\"brush: java\">\r\nclass Sailor extends Person {\r\n\r\n  public Sailor(String firstName, String lastName) {\r\n    super(firstName, lastName);\r\n  }\r\n\r\n  @Override\r\n  public String greeting() {\r\n    return \"Ahoy!\";\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>This is the right place for testing:</p>\r\n<pre class=\"brush: java\">\r\nclass SailorTest {\r\n\r\n  @Test\r\n  void full_name_is_provided() {\r\n    Sailor sailor = new Sailor(\"James\", \"Cook\");\r\n\r\n    assertEquals(\"James Cook\", sailor.fullName());\r\n  }\r\n\r\n  @Test\r\n  void greeting_is_provided() {\r\n    Sailor sailor = new Sailor(\"James\", \"Cook\");\r\n\r\n    assertEquals(\"Ahoy!\", sailor.greeting());\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Of course, there will be probably some duplicates in the test code when we have more concrete classes extending the abstract class. But the price is still smaller that losing the value of the test, when the acceptance depends on the inherited (potentially unknown) implementation. All in all, the inherited implementation can and often does (even it should be avoided as much as possible) change in the concrete classes and must be tested anyway.</p>\r\n\r\n<h2>Truly Object-Oriented</h2>\r\n\r\n<p>We can face a situation, for instance when working on a util library, where the provided default implementation is much more complicated than in our simple <code>fullName()</code> method. Such a case tells us that we are probably doing too much in the class. The solution is to introduce a new class and extract the functionality into it:</p>\r\n<pre class=\"brush: java\">\r\nclass Name {\r\n\r\n  private final String first;\r\n  private final String last;\r\n\r\n  public Name(String first, String last) {\r\n    this.first = first;\r\n    this.last = last;\r\n  }\r\n  \r\n  public String full() {\r\n    return String.format(\"%s %s\", firstName, lastName);\r\n  }\r\n}\r\n\r\nabstract class Person {\r\n\r\n  protected final Name name;\r\n\r\n  public Person(String firstName, String lastName) {\r\n    this.name = new Name(firstName, lastName);\r\n  }\r\n\r\n  public final String fullName() {\r\n    return name.full();\r\n  }\r\n  \r\n  public abstract String greeting();\r\n}\r\n</pre>\r\n\r\n<p>We moved the functionality into a specialized concrete class, easy to test. Now we can ensure the default implementation of the abstract class works as expected.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Implementation inheritance is in general not the best practice as it tightly couples children classes with the parent class. Composition should always be preferred over inheritance. However, there are cases where inheritance makes sense. In such cases, testing must be very careful.</p>\r\n\r\n<p><strong>Business is always concrete.</strong> Relying blindly on testing of an abstract class could break concrete requirements. <strong>Test always a concrete class</strong> as the concrete class must satisfy the business requirements.</p>\r\n\r\n<p>Happy testing!</p>', 'Testing,Programming', 'false', 'false', 1, 1),
(68, 'solid-principles-in-java-by-example', 1575814500, 'SOLID Principles in Java by Example', '<p>There are a lot of articles about the SOLID principles. But usually a different example for a particular principle is to be found. Instead, would it be nice to demonstrate all of them on a single code snippet?</p>', '<p>We aren’t going to drive deep into the theory, as a lot was already written. We are interested mainly in code!</p>\r\n\r\n<p>Consider a simple payroll component, an example very favored by Uncle Bob himself:</p>\r\n\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n\r\n  private final static Map&lt;String, Employee&gt; \r\n      registry = new HashMap&lt;&gt;();\r\n\r\n  protected final String personalId;\r\n  protected final String firstName;\r\n  protected final String lastName;\r\n\r\n  /** constructor */\r\n\r\n  public String fullName() {\r\n    return String.format(\"%s %s\", firstName, lastName);\r\n  }\r\n\r\n  public void register() {\r\n    registry.put(personalId, this);\r\n  }\r\n\r\n  public boolean isRegistered() {\r\n    return registry.containsKey(personalId);\r\n  }\r\n}\r\n\r\nclass Paycheck {\r\n\r\n  private final Employee employee;\r\n\r\n  /** constructor */\r\n\r\n  public double amount() {\r\n    if (employee instanceof Manager) {\r\n      return 2000.0;\r\n    }\r\n    if (employee instanceof Developer) {\r\n      return 1000.0;\r\n    }\r\n    return 0.0;\r\n  }\r\n}\r\n</pre>\r\n\r\n\r\n<h2>Single Responsibility Principle (SRP)</h2>\r\n<p>The SRP is about cohesion, it says that a component (function, method, class, module) should have only one reason to change.</p>\r\n\r\n<p>Our code breaks the SRP as any change in the persistence mechanism would require a change of the <code>Employee</code> code. For instance, a timestamp of the registration is persisted as well, etc.</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n  /** ... */\r\n    \r\n  private final static Map&lt;String, Employee&gt; \r\n      registry = new HashMap&lt;&gt;();\r\n    \r\n  public void register() {\r\n    registry.put(personalId, this);\r\n  }\r\n    \r\n  public boolean isRegistered() {\r\n    return registry.containsKey(personalId);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>To fix that we introduce a new specialized class <code>EmployeeRegistry</code> and put the persistence functionality into it.</p>\r\n<pre class=\"brush: java\">\r\nclass EmployeeRegistry {\r\n\r\n  private final static Map&lt;String, Employee&gt; map = new HashMap&lt;&gt;();\r\n\r\n  public void register(Employee employee) {\r\n    map.put(employee.personalId, employee);\r\n  }\r\n\r\n  public boolean isRegistered(Employee employee) {\r\n    return map.containsKey(employee.personalId);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Now, we just delegate the request:</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n  /** ... */\r\n    \r\n  private final static EmployeeRegistry \r\n      registry = new EmployeeRegistry();\r\n    \r\n  public void register() {\r\n    registry.register(this);\r\n  }\r\n    \r\n  public boolean isRegistered() {\r\n    return registry.isRegistered(this);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>We can do even better with the Dependency Inversion Principle, stay tuned.</p>\r\n\r\n<h2>Open-Closed Principle (OCP)</h2>\r\n<p>The OCP states that software components should be open for extension, but closed for modification. It means that we should extend the system functionality by adding new components rather than modifying the existing ones.</p>\r\n\r\n<p>With a new <code>Employee</code> subtype, calculating of the amout in the <code>Paycheck</code> must be modified:</p>\r\n<pre class=\"brush: java\">\r\npublic double amount() {\r\n  if (employee instanceof Manager) {\r\n    return 2000.0;\r\n  }\r\n  if (employee instanceof Developer) {\r\n    return 1000.0;\r\n  }\r\n  return 0.0;\r\n}\r\n</pre>\r\n\r\n<p>Better would be to create a method inside the <code>Employee</code> to return the salary for the paycheck:</p>\r\n<pre class=\"brush: java\">\r\nabstract class Employee {\r\n  /** ... */\r\n    \r\n  public abstract double salary();\r\n}\r\n\r\nclass Manager extends Employee {\r\n  /** ... */\r\n    \r\n  @Override\r\n  public double salary() {\r\n    return 2000.0;\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Now, the <code>Paycheck</code> could be simplified and will work with any additional <code>Employee</code> subclass.</p>\r\n<pre class=\"brush: java\">\r\nclass Paycheck {\r\n  /** ... */\r\n    \r\n  public double amount() {\r\n    return employee.salary();\r\n  }\r\n}\r\n</pre>\r\n<p>By the way, using <code>instanceof</code> violates the Liskov Substitution Principle, too. Avoid <code>instanceof</code> at any price!</p>\r\n\r\n<h2>Liskov Substitution Principle (LSP)</h2>\r\n<p>The LSP says that an object of type T should be replaceable with its subtypes S without affecting the correctness of the program P.</p>\r\n\r\n<p>Although the most of possible violations of LSP are in strongly typed languages caught by the compiler (return types, proper parameter subtypes, etc.), there are still invariants and contracts to take care of.</p>\r\n\r\n<p>From the definition, a volunteer has to salary; it actually makes no sense to talk about any salary:</p>\r\n<pre class=\"brush: java\">\r\nclass Volunteer extends Employee {\r\n  /** ... */\r\n    \r\n  @Override\r\n  public double salary() {\r\n    throw new RuntimeException(\"No salary for volunteers!\");\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>But what happends to the <code>Paycheck</code> (P) when we try to calculate the amount for a volunteer (S)?\r\n<pre class=\"brush: java\">\r\ndouble amount = new Paycheck(\r\n    new Volunteer(\"001\", \"John\", \"Smith\")\r\n).amount();\r\n</pre>\r\n\r\n<p>Unsurprisingly, an exception occurs. The exception breaks the contract we provided via the <code>Employee</code> to the <code>Paycheck</code> as not declared in the method signature. We have to fix it:</p>\r\n<pre class=\"brush: java\">\r\nclass Volunteer extends Employee {\r\n  /** ... */\r\n    \r\n  @Override\r\n  public double salary() {\r\n    return 0.0;\r\n  }\r\n}\r\n</pre>\r\n\r\n<h2>Interface Segregation Principle (ISP)</h2>\r\n<p>The ISP says that no client should be forced to depend on methods it does not use.</p>\r\n\r\n<p>A volunteer is actually not a payed employee and implementing it like that violates a business invariant. We can fix it with the ISP:</p>\r\n<pre class=\"brush: java\">\r\ninterface PayedEmployee {\r\n\r\n  double salary();\r\n}\r\n\r\nclass Manager extends Employee implements PayedEmployee {\r\n  /** ... */\r\n\r\n  @Override\r\n  public double salary() {\r\n    return 2000.0;\r\n  }\r\n}\r\n</pre>\r\n  \r\n\r\n<p>After the <code>PayedEmployee</code> was introduced, the method <code>salary()</code> has disappeared from the <code>Volunteer</code> and the <code>Employee</code> itself.</p>\r\n<p>The <code>Paycheck</code> depends only on what it really needs now:</p>\r\n<pre class=\"brush: java\">\r\nclass Paycheck {\r\n\r\n  private final PayedEmployee employee;\r\n\r\n  /** ... */\r\n}\r\n</pre>\r\n\r\n<h2>Dependency Inversion Principle (DIP)</h2>\r\n<p>The DIP states that high level modules should not depend on low level modules; both should depend on abstractions and abstractions should not depend on details.</p>\r\n\r\n<p>We have introduced the <code>EmployeeRegistry</code> to separate the persistence mechanism from the other code, but the class is still concrete and it\'s a hard dependency of the <code>Employee</code>.</p>\r\n<p>What happends if a different implementation is needed? For instance, consider using a database instead of a <code>Map</code>.</p>\r\n<p>Following the DIP we introduce an abstraction and let <code>Employee</code> depend on it:</p>\r\n<pre class=\"brush: java\">\r\ninterface EmployeeRegistry {\r\n\r\n  void register(Employee employee);\r\n    \r\n  boolean isRegistered(Employee employee);\r\n}\r\n\r\nabstract class Employee {\r\n\r\n  private final EmployeeRegistry registry;\r\n    \r\n  /** ... */\r\n}\r\n</pre>\r\n\r\n<p>Notice that abstractions invert dependencies.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>(Software) principles should never lead to dogma; they should provide a hint on unclear crossroads.</p>\r\n<p>SOLID principles can help recognize a problem in your code, but applying them blindly will likely do more harm than good.</p>\r\n\r\n<p>The example source code with SOLID commits (<a href=\"https://github.com/ttulka/blog-code-samples/tree/f0a5961d8e99772f39047f4a8322d12344acd2a7/solid-by-example\" target=\"_blank\">original</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/be83c69a5b9a75e6a99efdd969e87967c0e26b65/solid-by-example\" target=\"_blank\">SRP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/65d9799e20f2c695ca534eac9e6fd90f5fa823f6/solid-by-example\" target=\"_blank\">OCP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/4f7f59cff17f006b07a0a0c68703b7b918ec0941/solid-by-example\" target=\"_blank\">LSP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/66db7e330dde660b566e500628b2a069794c4934/solid-by-example\" target=\"_blank\">ISP</a>, <a href=\"https://github.com/ttulka/blog-code-samples/tree/a2397ff2ab7b8902456737eb93eb36cc13159136/solid-by-example\" target=\"_blank\">DIP</a>) is on my <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/solid-by-example\" target=\"_blank\">Github</a>.</p>\r\n\r\n<p>Happy coding!</p>', 'Design Patterns,Programming,Java', 'false', 'false', 1, 1),
(69, 'object-oriented-design-vs-persistence', 1579450000, 'Object-Oriented Design vs. Persistence', '<p>From time to time I attend discussions about OOP. Every time someone comes up with the argument of dealing with persistence. The typical question can be reduced to “should an object persist itself or rather be persisted?” I believe the question is fundamentally wrong.</p>', '<h2>Traditional Approach</h2>\r\n\r\n<p>Traditionally (and very wrongly) an object is seen as data with a bunch of procedures dealing upon it. A typical school-book code looks like follows:</p>\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n\r\n  private String personalId;\r\n  private String firstName;\r\n  private String lastName;\r\n\r\n  String getPersonalId() { return personalId; }\r\n  String getFirstName() { return firstName; }\r\n  String getLastName() { return lastName; }\r\n}\r\n</pre>\r\n\r\n<p>How to persist the Person object? One approach is to create a repository:</p>\r\n<pre class=\"brush: java\">\r\nclass PersonRepository {\r\n\r\n  void save(Person person) {\r\n    // persist the object\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Second approach is to let the object persist itself:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n  // ...\r\n\r\n  void save() {\r\n    // persist the object\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Which one is better? None, or more precisely, depends.</p>\r\n\r\n<h2>Domain-Driven Design</h2>\r\n\r\n<p><a href=\"https://en.wikipedia.org/wiki/Domain-driven_design\" target=\"_blank\">Domain-Driven Design (DDD)</a> calls for designing objects driven by their business (domain) meaning rather than by technical aspects. According DDD, <strong>persistence is just an implementation detail</strong>. How this helps us with our dilemma? Actually a lot: the question, where to put the persistence, is driven by the domain as well as all other concerns.</p>\r\n\r\n<p>Just listen to the business, what does it say? Is it talking about \r\n <em>a register</em> for people registration?</p>\r\n<pre class=\"brush: java\">\r\nclass PersonRegister {\r\n\r\n  void register(Person person) {\r\n    // ...\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Or rather about person <em>reporting</em>, <em>checking in</em>, <em>applying</em> or <em>enrolling</em>?</p>\r\n<pre class=\"brush: java\">\r\nclass Person {\r\n  // ...\r\n\r\n  void enroll() {\r\n    // ...\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Don’t think like a technician: \"this object must be persisted\", understand the domain and model around it. <strong>The domain must be found again in the model</strong>.</p>\r\n\r\n<p>Because the theory can never know your domain, it’s impossible to create a framework helping to cut the code vertically. All the tool can only help with the horizontal cutting and technical regards, which encourages the implementers to concentrate too much on techniques (like persistence) and forget about the domain. The biggest problem with <a href=\"https://www.oreilly.com/library/view/domain-driven-design-tackling/0321125215/\" target=\"_blank\">the blue book</a> I have is that it focuses a lot on tactical design and building blocks like Entities, Services and Repositories. At the end of the day that is the only thing some readers get from the text ignoring the real value: the importance of <strong>communication with domain experts and understanding the business</strong> being built.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Pure technical concerns like persistence don’t belong to object-oriented design. These are mere implementation details hidden behind APIs. APIs tell the story of the domain. <strong>Model the API around the domain, don’t let implementation leak to the API</strong>.</p>\r\n\r\n<p>Happy designing!</p>', 'OOP,DDD,Design Patterns,Programming', 'false', 'false', 1, 1),
(70, 'multitenancy-and-the-cloud', 1584280000, 'Multitenancy and the Cloud', '<p>Multitenancy was and still is a very popular and successful architectural pattern of the last decades. But, that is likely to change with the advent of cloud computing.</p>', '<p>I remember well the time when hardware was expensive and uneasy to get. Waiting for weeks or even months to receive a new server I necessarily needed yesterday. Bothersome paperwork, awkward negotiations, hard-to-get approvals, and high costs. It was true pain to be avoided if possible. Multitenancy as an architectural pattern comes exactly from these circumstances. </p>\r\n<p>The idea is pretty simple: deploy and operate a single system supporting multiple customers. The resources are shared, adding a new customer is quick and easy. But, of course, as everything in software architecture, multitenancy has several trade-offs. The biggest one is a lot of additional complexity. <strong>Tenant isolation must be ensured on every system level</strong> from configurations to data. This is not a simple task. The code must be aware of it on all layers, which makes even simple things pretty complicated with negative impact on performance. With increasing numbers of tenants <strong>scalability can easily become an issue</strong> as well.</p>\r\n<p>Cloud computing as the enabler of DevOps, infrastructure automatization and containerization changed the traditional hardware provisioning. Nowadays, it is possible to spin up a new server, create a new database, or set up a new service bus within a second or even less. No more request tickets to an IT department! <strong>Hardware provisioning in the cloud became quick and easy, providing a solution for the problem multitenancy originally addressed</strong>.</p>\r\n<h2>Multi- vs Single-Tenancy</h2>\r\n<p>Compare a single multi-tenant system with multiple single-tenant systems:</p>\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/multi-vs-single-tenant-systems.png\" alt=\"Multi- vs Single-Tenancy\" /></p>\r\n<p>In his book <a href=\"https://leanpub.com/cloudstrategy\" target=\"_blank\">Cloud Strategy</a>, Gregor Hohpe compares these two approaches to <strong>an apartment building vs. single-family homes</strong>. Such a mental model helps us understand the differences in the approaches.</p>\r\n<h3>Development</h3>\r\n<p>It is definitely way simple to project a single-family house rather than a big apartment building. This is the main drawback of multi-tenant systems as multitenancy aspects leak deep into the structure and brings great complexity which slows development down and makes it more expensive and exhausting.</p>\r\n<h3>Building and Deployment</h3>\r\n<p>Having a construction plan and building material should mean no big difference in the building process. But, big apartment buildings tend to be unique and built ad hoc and unsystematic with project changes and adjustments during the construction work, while small family houses are uniform making them a perfect match for automatization of the building process. <strong>Automatization of everything</strong> is an important mindset-shift highly encouraged by the cloud services.</p>\r\n<h3>Maintenance</h3>\r\n<p>If you ever lived in an apartment house, you know how difficult and costly an accident can be. Old big buildings tend to resist change while small houses are easy to be rebuilt from the ground. Finding a bug, implementing a new feature, data migration, a design or functionality change, those activities are very hard in complex systems.</p>\r\n<h3>Efficiency and Scalability</h3>\r\n<p>While single-family houses can be built quickly and easily on demand, planning an apartment building must take into account the desired amount of tenants in advance: when the house is only half occupied it’s very inefficient to be run, the heating, elevator, lighting must still be taken care of and so on. Another trouble comes when the capacity is exceeded. <strong>Multi-tenant systems are based on vertical scaling</strong> with its well-known limitations. Even adopting horizontal scaling could be problematic with multi-tenant systems: Consider an extreme case where a feature is implemented as a serverless function (Function-as-a-Service). Because everything in a multi-tenant system is shared, <strong>constraints and limits are shared</strong> as well. A serverless function allows for example one thousand concurrent executions per second. With one hundred tenants accommodated in the system it makes theoretically only ten executions per second for a tenant, which could be just too little.</p>\r\n<h3>Operations and Management</h3>\r\n<p>It is probably easier to manage one building consisting one thousand apartments than to deal with one thousand small houses. To accommodate a tenant into an existing apartment will be always easier than to build a brand-new house for him. Depends on discipline and maturity of the automatization process. As the <strong>multiple single-tenant approach moves the multi-tenant complexity from development to operations</strong>, demands on the system administrators can grow. The key here is a good automatization of literally everything and adopting the DevOps mindset: divide the work among dev and ops people, bring them together and share the responsibility for the product within the team.</p>\r\n<h2>Conclusion</h2>\r\n<p>Cloud computing doesn’t mean the definite end of the multitenancy architectural pattern, but it does offer an alternative solution with a promise of reducing effort in development by shifting the complexity into operations. This is not thinkable without mature automatization and clean team responsibility.</p>\r\n<p>But, there are valid use-cases for multitenancy as well as there are valid use-cases for computing outside the cloud.</p>\r\n<p>Happy accommodating!</p>', 'Cloud,Software Architecture', 'false', 'false', 1, 2),
(71, 'events-vs-commands-in-ddd', 1585123000, 'Events vs. Commands in DDD', '<p>There are situations where events and commands seem to be a good solution for a problem. Where to use events and where are commands the best fit?</p>', '<h2>Differences between Events and Commands</h2>\r\n<p>Events represent a past, something that already happened and can’t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected. An event has typically multiple consumers, but a command is addressed to only one.</p>\r\n<p>Martin Fowler emphasizes in his <a href=\"https://youtu.be/STKCRSUsyP0\" target=\"_blank\">talk</a> the <strong>semantic difference between events and commands</strong> as a hint to understand the overall behavior by using the most natural terms for a particular business. In his point of view the question “events or commands” is a <em>naming</em> problem.</p>\r\n<p>While this is true, it’s not the only one difference. I believe the key is responsibility and level of abstraction.</p>\r\n\r\n<h2>Domain Meaning</h2>\r\n<p>Consider a problem with two possible solutions, first using an event, second using a command:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-1.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>With events Billing service subscribes to Sales service as a consumer, with commands Sales service <em>actively uses</em> Billing service. Events enable an independent choreography of services while commands create an orchestration where one or a few services control all others. This contradicts the <a href=\"http://udidahan.com/2010/11/15/the-known-unknowns-of-soa/\" target=\"_blank\">definition of a service</a> as a technical authority for a specific business capability. Sales and Billing are two separate business capabilities, which means Sales service should not include any payment considerations. On the other hand, it is fully Billing’s responsibility to deal with a newly placed order.</p>\r\n\r\n<h2>Commands as Infrastructure Messages</h2>\r\n<p>Commands are imperatives to a concrete action, typically the result of a user act. There’s usually no room for the called service to make any business decisions or reasoning about the action. Consider a common usage of a command:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-2.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>MailServer makes no business decisions, it just sends an e-mail. On the other hand, sending an e-mail, SMS, or displaying a message on the screen <em>is</em> a business decision made by Sales service. MailServer is probably a separate component maintained by a different team or an external vendor. We can apply the same for the payment:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-3.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>As already mentioned, we don’t want to make business decisions about payments in Sales service. We can stick with an event and wrap the payments into a separate service:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-4.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>Now, we have two separate business services (Sales, Billing) and one technical service (PayPal), which is <strong>encapsulated</strong> in Billing service from the domain point of view. We can see PayPal service as part of Billing implementation and CollectPayment as an <strong>infrastructure message</strong> living outside of the domain.</p>\r\n<p>We can reason about the MailServer similarly:</p>\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/events-vs-commands-5.png\" alt=\"Events vs. Commands in DDD\" /></p>\r\n\r\n<p>Domain service Customer Notification reacts on OrderPlaced event and sends SendEmail command to MailServer (an infrastructure service) to carry out its business decision of sending an e-mail to the customer.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p>Events and commands are not just two sides of the same coin. <strong>(Domain) events are part of the business domain and its ubiquitous language, while commands are a pure technical concern</strong>.</p>\r\n<ul>\r\n  <li>Ask business to define the service boundaries.</li>\r\n  <li>Use events for communication among domain services.</li>\r\n  <li>Use commands only for communication with technical services.</li>\r\n</ul>\r\n\r\n<p>Happy eventing!</p>', 'DDD,Event-Driven,Design Patterns', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(72, 'domain-collections', 1586325000, 'Domain Collections', '<p>Collection, List and Set are terms very familiar to developers but hardly used by business experts. Therefore, they should not be part of the domain (API).</p>', '<h2>Standard Collections Don’t Speak Language of the Domain</h2>\r\n\r\n<p><strong>Domain (API) must speak the domain language</strong>. Listen to the business experts. They’re probably talking just about <em>products</em> rather than <em>a collection</em> or <em>a list</em> of products. Even when a “list” is used, its meaning differs from the List class which appears as standard in programming languages like Java (<code>java.util.List</code>) or .NET (<code>System.Collections.Generic.List</code>).</p>\r\n\r\n<p>Consider a typical incorrect API (in Java):</p>\r\n<pre class=\"brush: java\">\r\ninterface FindProducts {\r\n\r\n  List&lt;Product&gt; cheaperThan(Money money);\r\n}\r\n</pre>\r\n<p>What is the corresponding requirement?</p>\r\n<ul><li><em>As a user I want to find products cheaper than X amount of money.</em></li></ul>\r\n\r\n<p>Well, the requirement talks about <em>products</em>, not <em>a list of products</em>. It means we have a mismatch between the domain and the code that models it. The flaw is not huge, but there are other problems connected.</p>\r\n\r\n<h2>Standard Collections Have Meaningless Operations</h2>\r\n\r\n<p>Let’s inspect the interface once again. What can a client do with it:</p>\r\n<pre class=\"brush: java\">\r\nfor (Product product : findProducts\r\n    .cheaperThan(fiveDollars)) \r\n  System.out.println(product);\r\n\r\nfindProducts.cheaperThan(fiveDollars)\r\n    .stream()\r\n    .mapToDouble(Product::price)\r\n    .sum();\r\n</pre>\r\n<p>That is okay. And this?:</p>\r\n\r\n<pre class=\"brush: java\">\r\nfindProducts.cheaperThan(\r\n    fiveDollars).size();\r\n\r\nfindProducts.cheaperThan(\r\n    fiveDollars).get(1).price();\r\n</pre>\r\n<p>Does it make sense? Yeah, it could, depends on the use-case.</p>\r\n\r\n<p>What about these?:</p>\r\n<pre class=\"brush: java\">\r\nfindProducts.cheaperThan(\r\n    fiveDollars).remove(1);\r\n\r\nfindProducts.cheaperThan(\r\n    fiveDollars).clear();\r\n</pre>\r\n<p>Nah, those are very likely nonsense. The point is, even when those operations make no sense in the context of the use-case, they are still offered by the use-case API and nothing prevents the client from trying them out.</p>\r\n\r\n<p>Of course, the internal collections are probably not mutable and an exception will be thrown in runtime when the client does so, but this is just too late. Better would be not to provide such methods at all. It would make the client code safe right in compilation time.</p>\r\n\r\n<p>Not only are many methods meaningless, but even when they make sense, for example when we do want to provide the remove operation upon products, calling the method <code>List.remove()</code> will not bring the expected result - a product is maybe removed from the list, but it will still be found in the system.</p>\r\n\r\n<h2>Domain Collections to Rescue</h2>\r\n<p>We can fix this by introducing a new domain object Products:</p>\r\n<pre class=\"brush: java\">\r\ninterface FindProducts {\r\n\r\n  Products cheaperThan(Money money);\r\n}\r\n</pre>\r\n\r\n<p>What methods are in the Products signature? Everything which makes sense in the context of the domain. For example, we can sort the products:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n    \r\n  Products sorted(SortBy by);\r\n}\r\n</pre>\r\n\r\n<p>Sure, in the end we probably have to provide a way to receive the Product entities from the collection. We have several options:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products\r\n    extends Iterable&lt;Product&gt; {\r\n}\r\n</pre>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n  List&lt;Product&gt; asList();\r\n}\r\n</pre>\r\n\r\n<p>The meaningless methods like <code>clear()</code> and <code>remove()</code> are still included on the standard List interface, but now the client knows he works with a list of products and not with the products themselves. The operations are called on the list and not on the original Domain Collection. It means, even when the client erases the list, the found products remain the same.</p>\r\n\r\n<p>We can go a bit further and use <code>java.util.stream.Stream&lt;T&gt;</code> which is immutable and more natural to what the result actually represents:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n  Stream&lt;Product&gt; asStream();\r\n}\r\n</pre>\r\n\r\n<p>Similarly, in reactive systems we can use reactive types:</p>\r\n<pre class=\"brush: java\">\r\ninterface Products {\r\n\r\n  Flux&lt;Product&gt; asPublisher();\r\n}\r\n</pre>\r\n\r\n<h3>Better Performance</h3>\r\n<p>Domain Collections help us not only to improve the API, but can have several technical benefits, too.</p> \r\n\r\n<p>For example, lazy loading can be applied as the data is not required until the collection “collapses”. This can save a lot of throughput in case the collection is loaded from a database or some external resource. Such optimization would be not possible with Standard Collections. Consider a usage:</p>\r\n<pre class=\"brush: java\">\r\nfindProducts\r\n    .cheaperThan(fiveDollars)       // not queried yet\r\n    .sorted(Products.SortBy.PRICE)  // not queried yet\r\n    .asStream()                     // function “collapse” -> data queried \r\n</pre>\r\n\r\n<p>A sample JDBC implementation follows:</p>\r\n<pre class=\"brush: java\">\r\nclass ProductsCheaperThan implements Products {\r\n\r\n  private final Money cheaperThan;\r\n  private final SortBy sortBy;\r\n\r\n  private final JdbcTemplate jdbcTemplate;\r\n\r\n  // constructor ...\r\n\r\n  @Override\r\n  public Products sorted(SortBy by) {\r\n    return new ProductsCheaperThan(cheaperThan, by, jdbcTemplate);\r\n  }\r\n\r\n  @Override\r\n  public Stream&lt;Product&gt; asStream() {\r\n    return jdbcTemplate.queryForList(String.format(\r\n        \"SELECT code, title, price FROM products \" +\r\n        \"WHERE price &lt; ? ORDER BY %s\", sortBy), cheaperThan.amount())\r\n        .stream()\r\n        .map(this::toProduct);\r\n  }\r\n\r\n  // private methods ...\r\n}\r\n</pre>\r\n<p>The full source code can be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/domain-collections\" target=\"_blank\">my Github</a>.</p>\r\n\r\n<h2>Summary</h2>\r\n<p><strong>Domain Collections</strong> encapsulate domain entities in the domain API and provide domain-meaningful operations upon them.</p>\r\n\r\n<p>Using Domain Collections over Standard Collections brings several benefits:</p>\r\n<ul>\r\n  <li>Model speaks domain language,</li>\r\n  <li>safety of operations can be ensured via static typing,</li>\r\n  <li>potential of performance optimization.</li>\r\n</ul>\r\n<p>Happy collecting!</p>\r\n', 'DDD,OOP,Design Patterns,Programming', 'false', 'false', 1, 1),
(73, 'what-is-a-repository', 1586850000, 'What Is a Repository', '<p>Which purpose has a Repository? To which layer does it belong to? And how to implement it correctly?</p>', '<p>A <em>Repository</em> is a term from Domain-Driven Design (DDD), but I am actually not happy about that. Things like <em>Entity</em>, <em>Aggregate</em> and <em>Repository</em> are pure technical concepts and should never appear in the domain, which speaks language of a particular business only. Although DDD puts a Repository to the domain model (saying all Repository methods must have domain meanings), I find that too confusing to follow and very often misunderstood.</p>\r\n\r\n<p>In fact, <strong>persistence is just an implementation detail</strong> of business behavior (e.g. find a customer, register a new customer). There must be no persistence-related methods like <code>save()</code> or <code>load()</code> in the domain API.</p>\r\n\r\n<p>Therefore, I prefer to <strong>refactor Repository functionalities into concrete use-cases</strong>. This approach ends up with much simpler cohesive objects.</p>\r\n\r\n<h2>Do We Need Repositories at All?</h2>\r\n\r\n<p>Well, yes and no. We usually need a way to load and persist domain objects, but we don’t have to have a construct called “Repository” in our codebase.</p>\r\n\r\n<p>Consider the following use-case (in Java):</p>\r\n<pre class=\"brush: java\">\r\ninterface FindCustomer {\r\n\r\n  Customer byEmail(Email email);\r\n}\r\n</pre>\r\n\r\n<p>We can simply implement it with JDBC:</p>\r\n<pre class=\"brush: java\">\r\nclass FindCustomerJdbc implements FindCustomer {\r\n\r\n  private JdbcTemplate jdbcTemplate;\r\n\r\n  @Override\r\n  public Customer byEmail(Email email) {\r\n    return jdbcTemplate.queryForList(\r\n        \"SELECT firstName, lastName, email FROM customers \" +\r\n        \"WHERE email = ?\", email.value())\r\n        .stream()\r\n        .findAny()\r\n        .map(this::toCustomer)\r\n        .orElseGet(this::customerNotFound);\r\n  }\r\n  \r\n  // private methods...\r\n}\r\n</pre>\r\n\r\n<p>Because loading the entity from the database with an SQL query <em>is the actual implementation</em> of the use-case (as the class name says), there is no need to put any other layer in between. In this case <code>FindCustomerJdbc</code> <em>is</em> a Repository for the particular use-case.</p>\r\n\r\n<p>Yet typically we put another layer in between using an object called Repository:</p>\r\n<pre class=\"brush: java\">\r\nclass FindCustomerJdbc implements FindCustomer {\r\n\r\n  private CustomerRepository customerRepository;\r\n\r\n  @Override\r\n  public Customer byEmail(Email email) {\r\n    return customerRepository.findByEmail(email.value())\r\n        .map(this::toCustomer)\r\n        .orElseGet(this::customerNotFound);\r\n  }\r\n    \r\n  // private methods...\r\n}\r\n</pre>\r\n\r\n<p>We just moved the persistence-related code into a single place. What are the benefits here? One can say that 1) all persistence concerns can be changed together and 2) the code is more reusable.</p>\r\n\r\n<p>The first point could be valid when the storage type is volatile. But, how often happens that an application switches from one storage type to another?</p>\r\n\r\n<p>Changing the storage type (e.g. from XML files to a database) or even just the database type (e.g. relational to non-relational) needs usually a nontrivial shift in mindset and an additional persistence abstraction layer itself can hardly fulfil this goal. Such an attempt would likely lead to a too-general clumsy and inefficient solution, expensive to maintain (remember YAGNI), especially because it happens very rarely - I never faced such a situation personally!</p>\r\n\r\n<p>JDBC or JPA are themselves already solid abstractions making it possible to switch easily between database vendors.</p>\r\n\r\n<p>The latter point can be valid if we in fact face a lot of code duplications. For example, mapping from a persistent entry to the domain object, finding entities by ID etc.. In such cases, the abstraction will emerge clearly by itself. We can abstract the common functionality out to a separate object etc.. Otherwise, the Repository becomes quickly a bunch of persistence-related methods across different use-cases. That means the code is cut by technical instead of domain aspects. Independent use-cases become tightly coupled and difficult to maintain. <strong>Only good abstractions are reusable, but use-cases tend to be pretty concrete</strong>.</p>\r\n\r\n<p>Back to the code above. As we can see, the actual use-case implementation becomes a mere proxy to the Repository. One can think of skipping the proxy code altogether and implement the use-case just by the repository:</p>\r\n<pre class=\"brush: java\">\r\nclass CustomerRepositoryJdbc implements FindCustomer {\r\n\r\n  private JdbcTemplate jdbcTemplate;\r\n\r\n  @Override\r\n  public Customer byEmail(Email email) {\r\n    return jdbcTemplate.queryForList(\r\n        \"SELECT firstName, lastName, email FROM customers \" +\r\n        \"WHERE email = ?\", email.value())\r\n        .stream()\r\n        .findAny()\r\n        .map(this::toCustomer)\r\n        .orElseGet(this::customerNotFound);\r\n  }\r\n\r\n  // private methods...\r\n}\r\n</pre>\r\n\r\n<p>The problem is obvious: the object grows with every use-case added. Soon it ends up as a huge object responsible for everything involving persistence of the Customer. Such code is not clean, clumsy and hard to maintain, concrete methods tend to be heavily reused. Rather, the <strong>code should be structured by the domain</strong> not technical concerns!</p>\r\n\r\n<h2>Two Layer Repositories</h2>\r\n\r\n<p>There are situations where some kind of Repository makes sense, for example when using a framework like Spring.</p>\r\n<pre class=\"brush: java\">\r\ninterface CustomerRepository \r\n    extends CrudRepository&lt;CustomerEntry, UUID&gt; {\r\n\r\n  CustomerEntry findByEmail(String email);\r\n\r\n  // other methods...\r\n\r\n  @Entity\r\n  class CustomerEntry {\r\n    @Id\r\n    @GeneratedValue(strategy = GenerationType.AUTO)\r\n    public UUID id;\r\n    public String firstName;\r\n    public String lastName;\r\n    public String email;\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Here the Spring CRUD Repository takes care of JPA implementation and hides its details from the actual use-case implementation. There are still too many responsibilities and potential high coupling among use-cases, but the repository serves at least a good purpose - hiding the complexity of the ORM framework out from the use-case code.</p>\r\n\r\n<p>The Spring Repository here is an infrastructure layer between the use-case implementation and a persistence store. It’s not anymore the Repository from DDD theory which appears in the domain. The Spring Repository is an explicit technical construct which is fine when used as such.</p>\r\n\r\n<p>For more details about the two layer repositories read this <a href=\"https://www.vzurauskas.com/2019/04/07/two-layer-repositories-in-spring\" target=\"_blank\">great article</a>.</p>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>Repositories are highly misunderstood. The <strong>Repository pattern tends to be implemented as a pure technical construct</strong> and becomes easily a bunch of persistence methods with no common domain purpose. Rather they <strong>should be refactored to particular domain</strong> use-cases.</p>\r\n\r\n<p>However, <strong>Repositories can be used to hide complexity of particular persistence solutions</strong> like ORM. In that case, the Repository moves from the domain layer to the infrastructure layer. The cohesion and clarity of such Repositories should still be taken into account and the code should be partitioned by domain behavior. Good abstraction is king.</p>\r\n\r\n<p>You can find the source code on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/repositories\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy coding!</p>', 'DDD,OOP,Design Patterns,Programming', 'false', 'false', 1, 1),
(74, 'colored-services', 1588579000, 'Colored Services', '<p>Defining service boundaries is hard. Let\'s bring some colors and make it fun!</p>', '<p>Trying to define services first and partition their functionality later is often a misleading approach ending up in the <a href=\"https://www.michaelnygard.com/blog/2017/12/the-entity-service-antipattern/\" target=\"_blank\">Entity Services</a> antipattern. <strong>One can\'t really know what the right boundary is up-front, it is a process of discovery</strong>. Better is to  group use-cases that belong together, the services will emerge by themselves. Purely for differentiation we can use colors, numbers or vegetable. Don\'t try to find right names until the process is finished and every use-case has its home.</p>\r\n\r\n<p class=\"note\">I’ve heard about this technique for the first time from <a href=\"http://udidahan.com/\" target=\"_blank\">Udi Dahan</a>, but unfortunately it seems no paper is to be found on the topic. This is my humble attempt.</p>\r\n\r\n<p>First, let\'s define the requirements.</p>\r\n<ul>\r\n    <li>User finds products</li>\r\n    <ul>\r\n        <li>Title, description, price and in stock availability is shown</li>\r\n    </ul>\r\n    <li>User finds products by category</li>\r\n    <li>User adds a product into cart</li>\r\n    <ul>\r\n        <li>User can add a product into the cart multiple times which increases the quantity</li>\r\n        <li>A product already added into cart must not change during the session</li>\r\n        <ul>\r\n            <li>All attributes remain the same, price included</li>\r\n        </ul>\r\n    </ul>\r\n    <li>User removes a single product from the cart</li>\r\n    <li>User empties the entire cart</li>\r\n    <li>User places an order from the cart</li>\r\n    <ul>\r\n        <li>An order can contain additional charges</li>\r\n    </ul>\r\n    <li>User requests a delivery by filling in his name and shipping address</li>\r\n    <li>Payment is requested after the order was placed</li>\r\n    <li>Goods are fetched after the order was placed</li>\r\n    <li>Delivery is accepted after the order was placed</li>\r\n    <li>Delivery is dispatched after was accepted, payment collected and goods fetched</li>\r\n</ul>\r\n\r\n<p>Now, we can analyze those and scratch the first services.</p>\r\n<ul>\r\n    <li><b>Find products</b></li>\r\n    <ul>\r\n        <li>Product ID</li>\r\n    	<li>Title</li>\r\n    	<li>Description</li>\r\n    	<li>Price</li>\r\n    </ul>\r\n</ul>\r\n\r\n<p>That\'s pretty obvious. The service needs to have the product title, description and price.</p>\r\n\r\n<ul>\r\n    <li><b>Find products by category</b></li>\r\n</ul>\r\n\r\n<p>In order to provide this, a list of categories must be shown first:</p>\r\n<ul>\r\n    <li><b>List categories</b></li>        \r\n    <ul>\r\n        <li>Category ID</li>\r\n        <li>Title</li>\r\n        <li>URI</li>\r\n    </ul>\r\n</ul>\r\n\r\n<p>For categories, we probably want some nice looking URIs in our e-shop like <i>/toys</i> or <i>/books</i>. The URI will be used for searching. As we neither want to have URI duplicated in products nor to do two hops to get the Category ID first, we put the <i>List categories</i> use-case into the same service as both <i>Find products</i> use-cases. Let\'s call it <span style=\"color:#1155cc\">“Blue“:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/blue-service.png\" alt=\"Blue service\"/></p>\r\n\r\n<p>Next is the product availability. We want to show how many is left in stock. This information is not necessary for sale. A customer can order a product even when there are no items left in stock. This can just delay delivery or, alternatively, another product can be offered. No items in stock must not prevent the customer from placing an order - this is a business decision. This means we can model the stock information in a separate service, let\'s call it <span style=\"color:#38761d\">“Green“</span>:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/green-service.png\" alt=\"Green service\"/></p>\r\n\r\n<p>There are two important rules for the shopping cart. First says that a product can be added multiple times. This means we need to hold a quantity with each cart item. Second say that the product in cart must not change. This means the cart items are kinda snapshots of the products at the time of adding. That\'s why putting the cart functionality into the blue service wouldn\'t be the best idea: we need a new representation of product - a cart item. Besides, cart items are fully independent on products and categories. Let’s put it into a new <span style=\"color:#990000\">“Red”</span> service:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/red-service.png\" alt=\"Red service\"/></p>\r\n\r\n<p>Order items have similar rules like cart items, both are immutable snapshots of products at some particular time, except we don\'t need all information about the product in the order. The life cycle differs: while cart items can be added and removed, the order is typically immutable. The order total amount is not only sum of item prices but can include additions like shipping fees, taxes, discounts etc. That\'s why we need to model the total amount separately. As we agreed the order didn\'t belong to the same service as shopping cart, we have to put it into a new service, called <span style=\"color:#bf9000\">“Yellow“</span>:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/yellow-service.png\" alt=\"Yellow service\"/></p>\r\n\r\n<p>Although the customer fills up shipping information as a part of the order process, the order process is fully independent on it. An order can be placed even without shipping information - consider an online delivery of an e-book etc. We put it into a new service, let\'s call it <span style=\"color:#351c75\">“Violet“</span>:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/violet-service.png\" alt=\"Violet service\"/></p>\r\n\r\n<p>Similar with payment. Here the amount to pay is needed, because the payment doesn\'t necessarily have to be for the order as whole. Finishing the order process can contain several other payments like additional fees or discounts. A payment is not necessarily connected to an order as it could be used for different purposes like loyalty program, membership fees etc later. Let\'s call the new service <span style=\"color:#b45f06\">“Orange“</span>:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/orange-service.png\" alt=\"Orange service\"/></p>\r\n\r\n<p>Fetching goods should have an obvious impact: the amount in stock decreases. We put this use-case into the <span style=\"color:#38761d\">Green</span> service:</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/green-service-2.png\" alt=\"Green service\"/></p>\r\n\r\n<p>When all the use-cases are partitioned into services, it’s pretty straightforward to give them appropriate names.</p>\r\n\r\n<p><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/colored-services/services-named.png\" alt=\"Colored services\"/></p>\r\n\r\n<p>You can see this idea in action on <a href=\"https://github.com/ttulka/ddd-example-ecommerce\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy coloring!</p>', 'SOA,Microservices,Software Architecture', 'false', 'false', 1, 2),
(75, 'no-internals-in-configuration-api', 1590226000, 'No Internals in Configuration API', '<p>Configuration is part of the application API, so the same rules apply to it: no leak of implementation detail.</p>', '<p>Unfortunately, I see breaking this rule pretty often. I guess this is because the basic premise is not as obvious as it should be: <strong>configuration is an API</strong>.</p>\r\n\r\n<p>As an example, consider a Java application using an LDAP-based authentication implemented with Spring framework. Typically, one has to provide settings of the connection to the external LDAP server in the runtime environment:</p>\r\n\r\n<pre>\r\nspring.ldap.urls=ldap://myserver:1234\r\nspring.ldap.username=admin\r\nspring.ldap.password=secret\r\n</pre>\r\n\r\n<p>Because the LDAP server is not part of the application, the configuration is expected to be set by the user. This is where apparent internals become externals.<p>\r\n\r\n<p>If the user uses the same configuration properties the implementation details leak into the API. Technically, this will work fine, but the trouble comes later. Consider, for example, that the application gets rid of Spring completely and starts using a new fancy technology instead:</p>\r\n\r\n<pre>\r\nquarkus.security.ldap.dir-context.url=ldap://myserver:1234\r\nquarkus.security.ldap.dir-context.principal=admin\r\nquarkus.security.ldap.dir-context.password=secret\r\n</pre>\r\n\r\n<p>Now what? We could just use the new properties from now on, but this will break the old versions already installed. Breaking the contract is not very nice to the users, <strong>APIs should stay forever</strong>. Introducing a breaking change always requires good communication with customers, brings additional work and has a negative impact on user experience.</p>\r\n\r\n<p>We can implement a migration mechanism setting the new properties based on the old ones. But this will increase the complexity of the system and make the code maintenance more expensive. It’s always better to make things right from the beginning:</p>\r\n\r\n<pre>\r\nmyapp.ldap.url=ldap://myserver:1234\r\nmyapp.ldap.username=admin\r\nmyapp.ldap.password=secret\r\n</pre>\r\n\r\n<p>Here, <strong>no implementation concerns are exposed to the configuration API</strong>, the user doesn’t know (and doesn’t care) what kind of technology or framework is used - those all are just implementation details and must never leak into the API.</p>\r\n\r\n<p>The application prefix <code>myapp</code> will further ensure no future collisions with other internal configurations.</p>\r\n\r\n<p>The internals can be then configured based on the application properties (e.g. with Spring):</p>\r\n\r\n<pre>\r\nspring.ldap.urls=${myapp.ldap.url}\r\nspring.ldap.username=${myapp.ldap.username}\r\nspring.ldap.password=${myapp.ldap.password}\r\n</pre>\r\n\r\n<p>This approach leads to a solid configuration API, with clarity of application-specific property names, cheap maintenance, no potential breaking changes and great user experience.</p>\r\n\r\n<p>Happy configuring!</p>\r\n', 'API,Deployment,Software Architecture', 'false', 'false', 1, 1),
(76, 'too-many-interfaces', 1591074000, 'Too Many Interfaces', '<p>Interfaces are good stuff. Does that mean the more the better?</p>', '<h2>What Is an Interface</h2>\r\n<p>The term interface is overloaded. For the purpose of this text I use the following definition: <strong>An interface is a language construct</strong> typical for strongly typed programming languages like Java or Smalltalk. An interface specifies behavior of objects which implement it.</p>\r\n<p>Interfaces can be useful as a tool to implement good design practices like the Strategy pattern or the Interface segregation principle. On the other hand, <strong>mere use of interfaces doesn’t guarantee a good design</strong>. When overused, it could make things even worse.</p>\r\n\r\n<h2>Fallacies of Interfaces</h2>\r\n<p>Using interfaces does not automatically bring any benefits. Here are typical misconceptions:</p>\r\n<h3>Interfaces Are Abstractions</h3>\r\n<p>An abstraction means a separation of <em>what</em> from <em>how</em>. Abstractions separate a solution from its implementation. The reason is decomposition. Decomposition is about dividing a complex problem into smaller composable sub-problems. As Eric Elliott <a href=\"https://twitter.com/_ericelliott/status/1259561354203140096\" target=\"_blank\">tweeted</a>:</p>\r\n\r\n<blockquote class=\"quote\">The essence of development is composition. The essence of software design is problem decomposition.</blockquote>\r\n\r\n<p>Although often used to implement it, an <strong>interface doesn’t necessarily mean abstraction</strong>; check out the following code:</p>\r\n\r\n<pre class=\"brush: java\">\r\nimport com.example.vegetateframework.orm.Entity;\r\n\r\ninterface ProductRepository {\r\n\r\n  Entity&lt;Product&gt; findProduct(Long id);\r\n}\r\n</pre>\r\n\r\n<p>Is this a good abstraction? I don’t think so. The return type of the method is <code>Entity&lt;Product&gt;</code> which exposes the implementation details and binds the interface to an ORM framework. Another issue is the <code>Long</code> datatype of the product ID. Longs are usually used in the database for record IDs, but those database IDs must not leak into the interface. In my opinion, the name of the interface is bad too, as the term <em>Repository</em> is pure technical.</p>\r\n<p>There is nothing in the language itself to prevent us from writing such interfaces. In general, <a href=\"https://blog.ploeh.dk/2010/12/02/Interfacesarenotabstractions/\" target=\"_blank\">interfaces are not abstractions</a>.</p>\r\n<p>All the other upcoming points are driven by this particular misconception, but it’s still worth mentioning them here explicitly as they are probably the most justified causes for incorrectly using interfaces.</p>\r\n\r\n<h3>Interfaces Are Contracts</h3>\r\n<p>A contract is an unbreakable <strong>agreement between the provider and consumers</strong> about the expected functionality. Usually, describes behavior of a component in a declarative way.</p>\r\n<p>The level of abstraction is important. Consider the following example:</p>\r\n\r\n<pre class=\"brush: java\">\r\ninterface PriceCalculator {\r\n\r\n  Float calculatePrice(Product product);\r\n\r\n  Float applyDiscount(Integer discount, Float basePrice);\r\n}\r\n</pre>\r\n\r\n<p>The second method <code>applyDiscount</code> lives on a lower level of abstraction and shouldn’t be part of the interface. The class which implements this interface will probably have this method as a private member. Consumers of the contract are not interested in such low-level functionality and it’s not a good idea to bind them to it. Therefore, this interface is not a proper contract.</p>\r\n\r\n<h3>Interfaces Are Loose Coupling</h3>\r\n<p>Level of coupling is defined by the number of dependencies. <strong>Adding an additional level of indirection (an interface) doesn\'t make the code more decoupled</strong>.</p>\r\n<p>Compare these two code snippets:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass ShoppingCart {\r\n\r\n  private PriceCalculator priceCalc;\r\n  private Set&lt;CartItem&gt; items;\r\n \r\n  public Double totalAmount() {\r\n    return items.stream()\r\n        .mapToDouble(ci -&gt; ci.quantity() *\r\n            priceCalc.calculatePrice(ci.product())\r\n        ).sum();\r\n  }\r\n  // ...\r\n}\r\n\r\nclass PriceCalculator {\r\n\r\n  public calculatePrice(Product product) {\r\n    return ...;\r\n  }\r\n}\r\n</pre>\r\n\r\n<pre class=\"brush: java\">\r\nclass ShoppingCart {\r\n\r\n  // as above ...\r\n}\r\n\r\ninterface PriceCalculator {\r\n\r\n  calculatePrice(Product product);\r\n}\r\n\r\nclass PriceCalculatorImpl implements PriceCalculator {\r\n\r\n  public calculatePrice(Product product) {\r\n    return ...;\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>In the former snippet, the class <code>ShoppingCart</code> has a dependency to the concrete class <code>PriceCalculator</code>, while in the latter a new layer of indirection, the interface, was added.</p>\r\n<p>Did we make the code more loosely coupled? The <code>ShoppingCart</code> has still one dependency, nothing changed, but now there is a new dependency between the <code>PriceCalculator</code> interface and its implementation <code>PriceCalculatorImpl</code>. That means we’ve just made things worse! Complexity increased and the code became harder to understand. No good.</p>\r\n\r\n<h3>Interfaces Are Reuse</h3>\r\n<p>Planned reuse has a lot to do with the <a href=\"https://verraes.net/2014/08/dry-is-about-knowledge/\" target=\"_blank\">DRY</a> principle. When we find ourselves writing the same code several times, we usually cut the snippet out and put it into a separated module. Then, we reference this module as a dependency.</p>\r\n<p>Another motivation for creating incorrect reuse is speculation about future scenarios. The YAGNI principle addresses this issue, however, it’s still not uncommon to see this principle being violated even by experienced developers. But that\'s a different story.</p>\r\n<p><strong>Excessive aiming for reuse leads often to <a href=\"https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction\" target=\"_blank\">wrong abstractions</a></strong>. Consider the following code:</p>\r\n\r\n<pre class=\"brush: java\">\r\ninterface Product {\r\n\r\n  String name();\r\n  Float price();\r\n}\r\n\r\nclass ProductImpl implements Product {\r\n\r\n  private String name;\r\n  private Float price;\r\n\r\n  ProductImpl(String name, Float price) {\r\n    this.name = name;\r\n    this.price = price;\r\n  }\r\n  public String name() { return name; }\r\n  public Float price() { return price; }\r\n}\r\n</pre>\r\n\r\n<p>Does the interface make the code more reusable? Hardly. What is the benefit of that? I can’t think of any...</p>\r\n\r\n<h3>Interfaces Solve Circular Dependencies</h3>\r\n<p>Carola Lilienthal states in her book Sustainable Software Architecture:</p>\r\n\r\n<blockquote class=\"quote\">If a system has cycles, it is because the tasks of the classes involved are not clearly defined.</blockquote>\r\n\r\n<p>Her observation tells us that the root cause of circular dependencies lies in incorrect design of the domain. It means the <strong>circular dependencies are fundamentally not a technical issue</strong> and therefore cannot be solved easily by mere technical solutions such as interfaces.</p>\r\n<p>Breaking circular dependencies with an interface as middleman may remove only compile circular dependencies in code, but not the circular dependencies in runtime. The bad thing is, when compile cycles disappear, the runtime circular dependencies become more or less invisible, what leads not only to remaining in the system but also to justifying more and more of them as the codebase gets bigger.</p>\r\n<p>Circular dependencies must be solved on the level of overall system design, otherwise a logical monolith (the Big ball of mud) will emerge. <strong>Interfaces often encourage hiding design flaws, such as high coupling and low cohesion, with purely technical shortcuts.</strong></p>\r\n\r\n<h2>What Is a Good Abstraction</h2>\r\n\r\n<blockquote class=\"quote\">Keep things simple by not providing abstractions until the abstractions provide simplicity. ~ Kent Beck</blockquote>\r\n\r\n<p>Every construct in software development must be <a href=\"https://twitter.com/tomas_tulka/status/1264505013746925569\" target=\"_blank\">a solution to a problem</a>.</p>\r\n<p>A good abstraction is a <strong>solution to an actual problem</strong>. An unnecessary abstraction is waste in the best case or, worse, increase of accidental complexity of the system. A good abstraction <strong>reduces complexity and provides simplicity</strong>. An interface with only a single implementation is usually a wrong abstraction and should be eliminated.</p>\r\n<p>A good abstraction is <strong>simple and clear</strong>. All complicated <strong>details are hidden</strong> under a short and <strong>easy-to-use</strong> facade, that provides a clear insight and is intuitive to work with.</p>\r\n<p>A good abstraction is <strong>relevant</strong>. All irrelevant concerns are hidden or separated from the interface. A good abstraction must be <strong>designed with customer needs in mind</strong>. A well-designed interface focuses on ease of use, not ease of implementation.</p>\r\n<p>A good abstraction is <strong>domain-driven</strong>. No technical concern exists in the interface. Interfaces like <code>ProductService</code>, <code>ProductRepository</code> or <code>ProductFactory</code> are not particularly good abstractions.</p>\r\n\r\n<h3>Domain-Driven Abstractions</h3>\r\n<p>Domain-driven abstractions tend to be very <strong>stable</strong>. This idea is based on the presumption that business changes slower than software. It is a great benefit for code maintenance. Consider the following example:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass FindProduct {\r\n\r\n  Product byCode(String code) {\r\n    return ...\r\n  }\r\n}\r\n\r\n@RestController\r\nclass CatalogController {\r\n\r\n  private FindProduct findProduct;\r\n\r\n  @GetMapping\r\n  Product findProduct(String code) {\r\n    return findProduct.byCode(code);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>In the example above, if we ever decide to replace the concrete class <code>FindProduct</code> with an interface, the client code will remain unchanged, because the <strong>abstraction is decoupled from the implementation</strong> structure:</p>\r\n\r\n<pre class=\"brush: java\">\r\ninterface FindProduct {\r\n\r\n  Product byCode(String code);\r\n}\r\n\r\nclass FindProductJdbc implements FindProduct {\r\n\r\n  private JdbcTemplate jdbcTemplate;\r\n\r\n  Product byCode(String code) {\r\n    return jdbcTemplate.query(...);\r\n  }\r\n}\r\n\r\nclass FindProductMongo implements FindProduct {\r\n\r\n  private MongoCollection collection;\r\n\r\n  Product byCode(String code) {\r\n    return collection.find(...);\r\n  }\r\n}\r\n\r\n@RestController\r\nclass CatalogController {\r\n\r\n  // NO CHANGES NEEDED HERE :-)\r\n}\r\n</pre>\r\n\r\n<p>Good abstractions don\'t have to be sought, they emerge on their own when the design is domain-driven.</p>\r\n\r\n<h2>How Many Is Too Much</h2>\r\n<p>To summarize a bit, an interface is <em>too much</em> when it doesn’t serve a good abstraction, particularly:</p>\r\n<ul>\r\n    <li>has only one implementation,</li>\r\n    <li>is coupled to a concrete technology or implementation,</li>\r\n    <li>operates on multiple levels of abstraction,</li>\r\n    <li>contains irrelevant or unclear parts,</li>\r\n    <li>is not domain-driven,</li>\r\n    <li>is unstable.</li>\r\n</ul>\r\n\r\n<h2>But What About…?</h2>\r\n<p>There are several problem scenarios that are traditionally solved using interfaces. A lot of literature, articles and tutorials recommend such. A technical solution is always easier to understand and apply, but I believe it’s a cure for symptoms only, not for the cause.</p>\r\n\r\n<h3>Dependency Injection</h3>\r\n<p>Dependency injection is one of the most important principles in software development. Please, use it!</p>\r\n<p>In general, it controls which <em>object</em> is used. Choosing an <em>implementation</em> for an interface-based dependency is just a special case. The Dependency injection principle is equally useful without any additional level of indirection.</p>\r\n<p>Dependency injection doesn\'t need interfaces as well as a <strong>good abstraction doesn\'t need interfaces</strong>.</p>\r\n\r\n<h3>Mocking</h3>\r\n<p>If you find yourself in a situation you need to introduce an interface to make a module testable, try to step back first and think about the design of the module. Focus on cohesion and <a href=\"https://blogs.itemis.com/en/unit-tests-are-tests-of-modularity\" target=\"_blank\">modularity</a>. Proper decomposition can solve a lot of coupling issues without introducing any additional interface.</p>\r\n<p>When really needed, a concrete class can be mocked using existing tools as well.</p>\r\n\r\n<h2>Conclusion</h2>\r\n<p><em>Program to interfaces</em> is a well known practice, but one shouldn\'t take it literally. Instead, saying \"<strong>program to abstractions</strong>\", focusing more on <em>what</em> rather than <em>how</em>, may clear things up.</p>\r\n\r\n<p>Happy interfacing!</p>', 'Abstraction,OOP,Programming', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(77, 'monolithic-objects', 1601303000, 'Monolithic Objects', '<p>Don\'t model the real world, model your business!</p>', '<p>Even the largest system starts as a small bunch of objects. The design of these objects are mirrored throughout the entire system. That is, it’s very important to model the system correctly from the beginning, from the very first object.</p>\r\n\r\n<p>In this post, we will use the <em>strategic domain-driven design</em> to develop a simple system. We will show typical mistakes and pitfalls to be avoided and improve it step by step on the way.</p>\r\n\r\n<p>We will demonstrate how easy it is to fall into the trap of monolithic thinking. We will see that even the smallest <em>monolithic object</em> has a great impact on the entire system, and that every <em>big ball of mud</em> starts as a small ball of a few muddy objects.</p>\r\n\r\n<h2>Domain, entities, repositories and services</h2>\r\n\r\n<p>Let’s design a simple booking system “Rent a&nbsp;car”. We typically begin with listing types of entities included in the system. We can think of <em>entities</em>:</p>\r\n<ul>\r\n  <li>Customers</li>\r\n  <li>Vehicles</li>\r\n  <li>Bookings</li>\r\n</ul>\r\n<p>The Customer entity has a first and last name, a customer ID, and maybe some contact information such as an e-mail address:</p>\r\n<pre class=\"brush: java\">\r\nclass Customer {\r\n\r\n  Long id;\r\n  String firstName;\r\n  String lastName;\r\n  String email;\r\n}\r\n</pre>\r\n\r\n<p>The Vehicle entity has an ID, a type, a color and an availability flag telling us if the Vehicle is for rent or not (under repair, discarded, etc.):</p>\r\n<pre class=\"brush: java\">\r\nclass Vehicle {\r\n\r\n  Long id;\r\n  String type;\r\n  String color;\r\n  Boolean available;\r\n}\r\n</pre>\r\n\r\n<p>Now, the Booking entity maps a Vehicle with a Customer for some time period:</p>\r\n<pre class=\"brush: java\">\r\nclass Booking {\r\n\r\n  Long id;\r\n  Date from;\r\n  Date until;\r\n  Vehicle vehicle;\r\n  Customer customer;\r\n}\r\n</pre>\r\n\r\n<p>Easy. Having the domain model, we can consider a <a href=\"/what-is-a-repository\"><em>repository</em></a> to each entity:</p>\r\n<pre class=\"brush: java\">\r\ninterface CustomerRepository {\r\n\r\n  Customer findById(Long id);\r\n\r\n  Collection&lt;Customer&gt; findAll();\r\n\r\n  void save(Customer customer);\r\n}\r\n\r\ninterface VehicleRepository {\r\n\r\n  Vehicle findById(Long id);\r\n\r\n  Collection&lt;Vehicle&gt; findAll();\r\n\r\n  void save(Vehicle vehicle);\r\n}\r\n\r\ninterface BookingRepository {\r\n\r\n  Booking findById(Long id);\r\n\r\n  Collection&lt;Booking&gt; findAll();\r\n\r\n  void save(Booking booking);\r\n}\r\n</pre>\r\n\r\n<p>Straight-forward. <em>Services</em> and <em>controllers</em> pretty much copy functionality of the repositories.</p>\r\n\r\n<h2>Code organization</h2>\r\n\r\n<p>For the start, we chose a <em>layered</em> structure of our codebase:</p>\r\n<pre>\r\ncontrollers/\r\n    BookingController.java\r\n    CustomerController.java\r\n    VehicleController.java\r\ndomain/\r\n    Booking.java\r\n    Customer.java\r\n    Vehicle.java\r\nrepositories/\r\n    BookingRepository.java\r\n    CustomerRepository.java\r\n    VehicleRepository.java\r\nservices/\r\n    BookingService.java\r\n    CustomerService.java\r\n    VehicleService.java\r\n</pre>\r\n\r\n<p>Layered architecture partitions code by technical concerns. It’s the easiest way of structuring code, working just fine for the start and for small codebases. Usually, it becomes suboptimal as the system grows.</p>\r\n\r\n<h2>Problem with boundaries</h2>\r\n\r\n<p>Things become interesting when we start thinking about the Booking service, especially about a method for creating a new Booking.</p>\r\n<pre class=\"brush: java\">\r\nclass BookingService {\r\n\r\n  private BookingRepository repo;\r\n\r\n  public Booking findById(Long id) {\r\n    return repo.findById(id);\r\n  }\r\n\r\n  Collection&lt;Booking&gt; findAll() {\r\n    return repo.findAll();\r\n  }\r\n\r\n  void create(???) {\r\n    ???\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Obviously, we need instances of the Customer and Vehicle the Booking is created for. How do we get them? There are several options on offer...</p>\r\n\r\n<h3>Mighty controller</h3>\r\n\r\n<p>The first option would be to load them from the controller and pass them as a parameter to the service:</p>\r\n<pre class=\"brush: java\">\r\n@RestController\r\nclass BookingController {\r\n\r\n  private BookingService bookingService;\r\n  private CustomerService customerService;\r\n  private VehicleService vehicleService;\r\n\r\n  @PostMapping\r\n  void createBooking(Long customerId, Long vehicleId) {\r\n    var customer = customerService.findById(customerId);\r\n    var vehicle = vehicleService.findById(vehicleId);\r\n    bookingService.create(customer, vehicle);\r\n  }\r\n  ...\r\n}\r\n</pre>\r\n\r\n<p>This will work, the service gets what it needs, everything’s fine. The problem with this approach is that it puts too much responsibility on the controller. A controller has a single job: processing user input (request) into output (response). Any orchestration is not a job for a controller and it should not be. Consider another controller of a command-line application, do we want to duplicate this behavior there? Probably not, because it is not a controller-specific job and has no place in the controller code, or at least the resulting fragmentation of domain logic is not ideal.</p>\r\n\r\n<h3>Broken encapsulation</h3>\r\n\r\n<p>Saying that, we can try the second side and use repositories from within the service:</p>\r\n<pre class=\"brush: java\">\r\nclass BookingService {\r\n\r\n  private BookingRepository bookingRepo;\r\n  private CustomerRepository customerRepo;\r\n  private VehicleRepository vehicleRepo;\r\n  ...\r\n\r\n  void create(Long customerId, Long vehicleId);\r\n    var customer = customerRepo.findById(customerId);\r\n    var vehicle = vehicleRepo.findById(vehicleId);\r\n    if (vehicle.available()) {\r\n        bookingRepo.save(new Booking(customer, vehicle));\r\n    }\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Well, this will work, too. The problem here is that a repository is a mere implementation detail of a particular service. Using a repository of another service means breaking the encapsulation of that service. What’s more, a service usually implements some business rule upon the data. Calling the repository directly we skip those rules entirely and can potentially break business invariants, which leads to working with invalid objects. This is definitely not what we want.</p>\r\n\r\n<h3>Coupled services</h3>\r\n\r\n<p>There is one more option: calling services from a service:</p>\r\n<pre class=\"brush: java\">\r\nclass BookingService {\r\n\r\n  private BookingRepository bookingRepo;\r\n\r\n  private CustomerService customerService;\r\n  private VehicleService vehicleService;\r\n  ...\r\n\r\n  void create(Long customerId, Long vehicleId);\r\n    var customer = customerService.findById(customerId);\r\n    var vehicle = vehicleService.findById(vehicleId);\r\n    if (vehicle.available()) {\r\n      bookingRepo.save(new Booking(customer, vehicle));\r\n    }\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>We have fixed all the previous problems, this solution seems to be the best one. At least the best in the current settings. There is still one issue with this approach: it’s desperately <em>monolithic</em>.</p>\r\n\r\n<p><strong>Service should be autonomous</strong>, that is, a service must have all it needs to carry out a particular business capability. Unfortunately, this is not our case: the Booking service needs other services to create a new Booking. This is, due to an incorrect <em>cohesion</em>, the services are <em>tightly coupled</em> to each other, which is always a sign of a monolithic design. With a setting like that, we can’t talk about services anymore.</p>\r\n\r\n<p>You can raise an objection saying that the whole system is a service, but that’s just not true. Because a lot of modeled information is actually not necessary for the Booking. For instance, a customer\'s name or vehicle’s color are super redundant in the booking process, they do belong to the Customer service and Vehicle service, because the application we develop is a system of multiple services rather than a single service. The problem is that our objects are modeled in a logically monolithic way.</p>\r\n\r\n<h3>Are we there yet?</h3>\r\n\r\n<p>An obvious fix would be to remove redundant entities and replace them with mere references:</p>\r\n<pre class=\"brush: java\">\r\nclass Booking {\r\n  ...\r\n  Long vehicleId;\r\n  Long customerId;\r\n}\r\n</pre>\r\n\r\n<p>In a real development, using <em>domain primitives</em> such as <code>VehicleId</code> and <code>CustomerId</code> objects rather than Longs would create much more secure code. For sake of simplicity we stick with <code>Long</code> here.</p>\r\n\r\n<p>We have made the Booking entity independent, unfortunately, this doesn’t help us much further, because the Availability information from the Vehicle is still needed in the Booking service. We’re trapped and there is no way out!</p>\r\n<pre class=\"brush: java\">\r\nvoid create(Long customerId, Long vehicleId);\r\n  var available = ???\r\n  if (available) {\r\n     bookingRepo.save(new Booking(customerId, vehicleId));\r\n  }\r\n}\r\n</pre>\r\n\r\n<h2>Truly domain-driven</h2>\r\n\r\n<p>The tricky part of designing a service is not to fall in the trap of modeling as the real world. <strong>Business is just a specialized and very concrete subset of the real world with special rules and different meanings</strong>.</p>\r\n\r\n<p>Let’s think about the concept of Availability. In the real world, it belongs naturally to the Vehicle entity. In our business domain, however, there is no meaning of Availability in the Vehicle service whatsoever. Availability has a meaning only in the Booking service, this is, it truly belongs to the Booking service:</p>\r\n<pre class=\"brush: java\">\r\nclass Availability {\r\n\r\n  Long vehicleId;\r\n  Boolean available;\r\n}\r\n</pre>\r\n\r\n<h2>Explicit boundaries</h2>\r\n\r\n<p>Now the Booking service has everything it needs to create a new Booking without asking another service for additional information. All services can be now developed and deployed independently. We can make their boundaries explicit by organizing code into domain specific packages:</p>\r\n<pre>\r\nbooking/\r\n    controllers/\r\n        BookingController.java\r\n    domain/\r\n        Availability.java\r\n        Booking.java\r\n    repositories/\r\n        AvailabilityRepository.java\r\n        BookingRepository.java\r\n    services/\r\n        BookingService.java\r\ncustomer/\r\n    controllers/\r\n        CustomerController.java\r\n    domain/\r\n        Customer.java\r\n    repositories/\r\n        CustomerRepository.java\r\n    services/\r\n        CustomerService.java\r\nvehicle/\r\n    controllers/\r\n        VehicleController.java\r\n    domain/\r\n        Vehicle.java\r\n    repositories/\r\n        VehicleRepository.java\r\n    services/\r\n        VehicleService.java\r\n</pre>\r\n\r\n<p>Putting the same domain concepts together made our code much clearer. Technical packages seem to be no more necessary, we can get rid of them completely:</p>\r\n<pre>\r\nbooking/\r\n    Availability.java\r\n    AvailabilityRepository.java\r\n    Booking.java\r\n    BookingController.java\r\n    BookingRepository.java\r\n    BookingService.java\r\ncustomer/\r\n    Customer.java\r\n    CustomerController.java\r\n    CustomerRepository.java\r\n    CustomerService.java\r\nvehicle/\r\n    Vehicle.java\r\n    VehicleController.java\r\n    VehicleRepository.java\r\n    VehicleService.java\r\n</pre>\r\n\r\n<h3>Exposed behavior</h3>\r\n\r\n<p>We have successfully broken our small monolith into decoupled services with explicit boundaries. We can go even further by exposing just specific behavior instead of a single monolithic service API.</p>\r\n\r\n<p>What is it good for? Consider a client interested in only listing actual Bookings. Why should such a client depend on the whole service? Better would be to cut the use-case off the still-pretty-monolithic service and expose it explicitly. Same for all other use-cases:</p>\r\n<pre>\r\nbooking/\r\n    CreateBooking.java\r\n    FindAllBookings.java\r\n    FindBooking.java\r\n    ...\r\n</pre>\r\n\r\n<p>The use-case driven API can be implemented entirely by the Booking service class or by multiple independent classes. Clients are not forced to depend on things they don’t need. This concept is known as the <em>Interface segregation principle</em>.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Business usually differs from the real world. <strong>Business represents just a specific and very opinionated view of the real world</strong>. These differences must be understood and modeled in the code. This is the essence of domain-driven design.</p>\r\n\r\n<p>Typical mistake is to model a real world entirely and try to fit it into the business afterwards. Such an approach often leads to wrong boundaries and tightly coupled code.</p>\r\n\r\n<p>Business-oriented thinking must be applied from the very beginning, from the very first objects. Monolithic systems are made of monolithically designed objects. <strong>The overall system architecture reflects objects it is composed of</strong>.</p>\r\n\r\n<p><strong>Behavior-driven</strong> rather than entities-driven design is a good strategy to avoid typical pitfalls of monolithic objects. All in all, objects are about behavior, not about data.</p>\r\n\r\n<p>Happy objecting!</p>\r\n', 'OOP,SOA,Monolith,Software Architecture', 'false', 'false', 1, 1),
(78, 'java-records-are-not-necessarily-evil', 1591225200, 'Java Records Aren’t Necessarily Evil', '<p>How do Java Records fit to the object-oriented design?</p>', '<p><a href=\"https://blogs.oracle.com/javamagazine/records-come-to-java\" target=\"_blank\">Records</a> are likely the most discussed feature new in Java 14. At the same time, there is a lot of criticism due to their non-object-oriented nature. A typical argument says that records are a concept from procedural programming and have no place in an object-oriented language.</p>\r\n\r\n<p>Do records really encourage procedural rather than object thinking?</p>\r\n\r\n<p>Well, yes and no. I totally agree that records are no object-oriented feature, on the other hand, I believe there are valid use-cases for them even in perfectly object-oriented applications.</p>\r\n\r\n<p>The reason is, there are no true <a href=\"https://blog.ploeh.dk/2011/05/31/AttheBoundaries,ApplicationsareNotObject-Oriented/\" target=\"_blank\">objects at the boundaries</a>. Consider for instance a REST resource, database entity or configuration properties. Those are all mere data structures, but still need to have their representations in Java code.</p>\r\n\r\n<h2>Evolution of Records</h2>\r\n\r\n<p>Of course, we have data structure types such as <code class=\"is kd ke kf kg b\">Map</code> or <code class=\"is kd ke kf kg b\">Set</code> in Java, and they are sufficient for a lot of cases. Sometimes, however, it is beneficial to have a typed data structure — for declarative validations, marshaling or mapping, for example. Traditionally, the <a href=\"https://martinfowler.com/eaaCatalog/dataTransferObject.html\" target=\"_blank\">Data Transfer Objects (DTO)</a> are used in such scenarios:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass AccountData {\r\n \r\n  private String username;\r\n  private String password;\r\n  private String email;\r\n     \r\n  public AccountData(\r\n      String username, String password, String email) {\r\n    this.username = username;\r\n    this.password = password;\r\n    this.email = email;\r\n  }\r\n  public String getUsername() {\r\n    return username;\r\n  }\r\n  public void setUsername(String username) {\r\n    this.username = username;\r\n  }\r\n  public String getPassword() {\r\n    return password;\r\n  }\r\n  public void setPassword(String password) {\r\n    this.password = password;\r\n  }\r\n  public String getEmail() {\r\n    return email;\r\n  }\r\n  public void setEmail(String email) {\r\n    this.email = email;\r\n  } \r\n  @Override\r\n  public boolean equals(Object o) {\r\n    if (this == o) {\r\n      return true;\r\n    }\r\n    if (o == null || getClass() != o.getClass()) {\r\n      return false;\r\n    }\r\n    AccountData that = (AccountData) o;\r\n    return username.equals(that.username) &&\r\n        password.equals(that.password) &&\r\n        email.equals(that.email);\r\n  }\r\n  @Override\r\n  public int hashCode() {\r\n    return Objects.hash(\r\n        username, password, email);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Well, this is really a lot of boilerplate code for such a simple thing. You can hate it, you can love it, but <a href=\"https://projectlombok.org/features/Data\" target=\"_blank\">Lombok</a> is addressing exactly this gap quite elegantly:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Data\r\nclass AccountData {\r\n \r\n  private String username;\r\n  private String password;\r\n  private String email;\r\n}\r\n</pre>\r\n\r\nLombok is based on code-manipulations, which could be seen as inadequate “black magic” a lot of developers don’t want to have in their codebases. Java 14 solves this problem with the records:\r\n\r\n<pre class=\"brush: java\">\r\nrecord AccountData (\r\n    String username, \r\n    String password, \r\n    String email) {}\r\n</pre>\r\n\r\nAny resemblance to Kotlin’s data classes is purely coincidental:\r\n\r\n<pre class=\"brush: java\">\r\ndata class AccountData (\r\n    var username: String, \r\n    var password: String, \r\n    var email: String)\r\n</pre>\r\n\r\n<h2>The Problem with DTOs</h2>\r\n\r\n<p>So, what’s the problem here? Java records seem to introduce DTOs as a language feature. What’s the problem with DTOs? The “O” is. The <strong>Data Transfer Objects are no objects at all!</strong> A DTO is just a typed data structure.</p>\r\n\r\n<p>Code should be as explicit as possible and data structures should not be called objects. This fallacy leads to a bad object design, usually to the infamous <a href=\"https://martinfowler.com/bliki/AnemicDomainModel.html\" href=\"_blank\">Anemic Domain Model</a>.</p>\r\n\r\n<p>On the other hand, records (and even the Lombok’s <code>@Data</code> annotation) have no \"object\" in the name. Records make it explicit, <a href=\"/treat-data-as-data\" href=\"_blank\">data is treated as data</a>.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Java records are a new feature that brings an additional data structure type into the language. In contrast to DTOs, <strong>records make data structures in code explicit</strong>. They are right tools for right purposes. Being used incorrectly they are evil servants like everything else could be, too.</p>\r\n\r\n<p>There are much more dangerous common practices in Java such as null references, static methods and global constants, but that’s a different story...</p>\r\n\r\n<p>Happy recording!</p>\r\n\r\n', 'OOP,Programming,Java', 'false', 'false', 1, 1),
(79, 'the-interface-segregation-principle-with-lambdas', 1602025200, 'The Interface Segregation Principle with Lambdas', '<p>How to implement the ISP using simple functions to reduce coupling and complexity at the same time.</p>', '<p>The <a href=\"https://en.wikipedia.org/wiki/Interface_segregation_principle\" target=\"_blank\">Interface Segregation Principle</a> (ISP) is the “I” in the <a href=\"/solid-principles-in-java-by-example\" target=\"_blank\">SOLID</a> acronym and states that clients should not be forced to depend on methods they don’t use.</p>\r\n\r\n<p>It attempts to decouple clients from unnecessary details.</p>\r\n\r\n<h2>The Problem</h2>\r\n\r\n<p>Consider a typical violation of the ISP (in Java):</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass UserService {\r\n	\r\n  User register(String username) { … }\r\n	\r\n  User find(String username) { … }\r\n\r\n  void lock(User user) { … }\r\n}\r\n\r\nclass UserRegistrationClient {\r\n	\r\n  private UserService userService;\r\n\r\n  /* constructor */\r\n\r\n  void registerUser() {\r\n    String username = …\r\n\r\n    User user = userService\r\n      .register(username);\r\n    …\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>The <code>UserRegistrationClient</code> class doesn’t need anything else than the <code>register</code> method from the <code>UserService</code>, but it does depend on the whole API.</p>\r\n\r\n<p>A solution according the ISP would be to create an explicit contract only for the needs of the particular client:</p>\r\n\r\n<pre class=\"brush: java\">\r\ninterface RegisterUser {\r\n	\r\n  User register(String username);\r\n}\r\n\r\nclass UserService implements RegisterUser {\r\n	\r\n  /* as above */\r\n}\r\n\r\nclass UserRegistrationClient {\r\n	\r\n  private RegisterUser registerUser;\r\n\r\n  /* constructor */\r\n\r\n  void registerUser() {\r\n    String username = …\r\n\r\n    User user = registerUser\r\n      .register(username);\r\n    …\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>This already looks much better. The benefits are pretty obvious: the client code is decoupled from unnecessary details and an explicit contract between the service provider and client is established. This is a great way for the client to express her needs and to ensure it is implemented by the provider.</p>\r\n\r\n<p>However, there are drawbacks, too. <strong>Any new interface brings certain complexity</strong> and following the approach strictly ends up <strong>writing more code</strong>, in extreme cases it could lead to an explosion of interfaces. Another problem appears when we don’t have the provider’s code in the hand.</p>\r\n\r\n<h2>The Solution</h2>\r\n\r\n<p>A solution would be to use lambdas (anonymous functions):</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass UserService {\r\n	\r\n  /* as above */\r\n}\r\n\r\nclass UserRegistrationClient {\r\n	\r\n  private Function&lt;String,User&gt; registerUser;\r\n\r\n  /* constructor */\r\n\r\n  void registerUser() {\r\n    String username = …\r\n\r\n    User user = registerUser\r\n      .apply(username);\r\n    …\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>The functionality is injected into the client using the provider:</p>\r\n\r\n<pre class=\"brush: java\">\r\nnew UserRegistrationClient(\r\n    userService::register\r\n);\r\n</pre>\r\n\r\n<p>Again, this solution doesn’t come without drawbacks: we have lost the explicit and named contract and the code is less type-safety. On the other hand, we have much <strong>less code to maintain </strong>and gained a great <strong>flexibility</strong> as any function can be injected to implement the client’s needs, not just derivations of <code class=\"jg kz la lb kv b\">RegisterUser</code>. And of course, <strong>third-parties are no problem</strong> anymore.</p>\r\n\r\n<p>The technique can be used even in languages without interfaces, such as JavaScript:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nclass UserRegistrationClient {\r\n\r\n  constructor(registerUser) {\r\n    this._registerUser = registerUser;\r\n  }\r\n\r\n  registerUser() {\r\n    let username = …\r\n    this._registerUser(username);\r\n    …\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Rather than:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nclass UserRegistrationClient {\r\n\r\n  constructor(userService) {\r\n    this._userService = userService;\r\n  }\r\n\r\n  registerUser() {\r\n    let username = …\r\n    this._userService\r\n      .register(username);\r\n    …\r\n  }\r\n}\r\n</pre>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Using lambdas to decouple the client code according the ISP works pretty well depending on the specific context, but it doesn’t come without drawbacks.</p>\r\n\r\n<p>Moreover, the technique is not very common in languages where lambdas were introduced later than interfaces, such as Java or C#.</p>', 'Design Patterns,Programming', 'false', 'false', 1, 1),
(80, 'how-cohesion-and-coupling-correlate', 1603148400, 'How Cohesion and Coupling Correlate', '<p>Cohesion done right reduces the coupling and complexity of systems.</p>', '<p>As I was finishing <a href=\"/monolithic-objects\">my previous post</a> about defining service boundaries, I had a very strong feeling that there must be some abstract concept of what I was trying to explain using actual examples...</p>\r\n\r\n<p>Of course, there is! It is the concept of <em>cohesion</em> and <em>coupling</em> that I will discuss in this post.</p>\r\n\r\n<p>I will start with some definitions:</p>\r\n\r\n<div class=\"definition\">\r\n<p><a href=\"https://en.wikipedia.org/wiki/Cohesion_(computer_science)\" target=\"_blank\"><em>Cohesion</em></a> is the degree to which the elements inside a module belong together.</p>\r\n<p><a href=\"https://en.wikipedia.org/wiki/Coupling_(computer_programming)\" target=\"_blank\"><em>Coupling</em></a> is the degree of interdependence between software modules.</p>\r\n</div>\r\n\r\n<p>High cohesion and loose coupling are the most important principles in software engineering. They manifest themselves everywhere from code to team organization.</p>\r\n\r\n<p>Cohesion and coupling are tightly related. Why are they so important? Both help us reduce complexity, the true fun killer of software development.</p>\r\n\r\n<p>To a lot of people, sadly, the concepts sound too academic and are therefore often poorly understood.</p>\r\n\r\n<h2>What is cohesion, anyway?</h2>\r\n\r\n<p>Tough question. The definition is pretty broad and there are several interpretations out there. Not all of them are necessarily wrong, the more valid question is: which one is most beneficial? I use the following definition as I believe it always leads to cohesive components with tight coupling inside and loose coupling outside, which is exactly what we want:</p>\r\n\r\n<div class=\"definition\">\r\n<p><em>The degree of cohesion of a component by a particular key equals the number of elements cohesive by the key within the component divided by the sum of the total number of elements cohesive by the key in the whole system and the number of elements not cohesive by the key inside the component.</em></p>\r\n</div>\r\n\r\n<p>This long definition can be expressed as a simple formula:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-formula.png\" /><br /><i class=\"caption\">The cohesion formula</i></p>\r\n\r\n<p>Where <code>c</code> stands for the component, <code>k</code> stands for the key, and <code>N</code> stands for the number of elements. Obviously, the maximal cohesion of a component is equal to one. This is what we strive for.</p>\r\n\r\n<p>I want to emphasize that <strong>cohesion does not depend on the number of connections</strong> between elements, which is what coupling is all about. Cohesion is more about <em>belonging together</em>. However, cohesive components do tend to have a higher degree of coupling <em>within</em> the component, but that is just a symptom of high cohesion, not the cause.</p>\r\n\r\n<p>The definition above might look complicated, but it is actually quite easy. It can be illustrated by some examples. We measure the degree of cohesion by the <em>violet</em> key for the components bordered with a dashed line in the following systems:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-examples.png\" /><br /><i class=\"caption\">Example measurements of cohesion</i></p>\r\n\r\n<p>Functionality (business) is always the right key to use. Violet and blue can stand for sales and accounting, a product and an invoice, or user registration and ordering.</p>\r\n\r\n<p>My definition may be a bit oversimplified as the boundaries are not always as solid and obvious. This is why business experts must be involved.</p>\r\n\r\n<h2>Myth busted</h2>\r\n\r\n<p>Cohesion and coupling are almost always discussed together as they tightly correlate. The relation is sometimes a source of confusion as well, although its understanding is very useful to gain the utmost for the software system under development.</p>\r\n\r\n<p>A typical myth, I often hear people believe in, puts cohesion and coupling in opposition. Practically, they say that <em>“the higher the cohesion the tighter the coupling”</em>. I will show you how wrong this statement is.</p>\r\n\r\n<p>This is usually illustrated with an example: Consider the highest possible cohesion of the system where every module is represented by a single line of code (or a single function, an object with a single method, etc.). Such a degree of cohesion will inevitably increase the coupling between modules to the maximum.</p>\r\n\r\n<p>As the conclusion is true, there is a small problem in the prerequisite. To discover it, we must recall the definition of cohesion once again. It talks about <em>belonging together</em>, the strength of <em>relationship</em> of elements, and a <em>common purpose</em>.</p>\r\n\r\n<p>What does it mean in practice? In fact, splitting elements that belong together actually makes cohesion lower. So, in the example above, the system really does not have the highest possible cohesion, just the opposite: breaking modules into the smallest possible elements will separate related concepts and will lead to pretty low cohesion.</p>\r\n\r\n<p>The moral here is: <strong>Cohesion is not something you can create automatically</strong>. <strong>Cohesion is discovered in a particular context</strong>. That is why it is so hard for cohesion to be reliably measured. We will discuss this in detail later, stay tuned.</p>\r\n\r\n<h2>Cohesion and coupling</h2>\r\n\r\n<p>Let me show you some pictures. In each figure below, there are the very same elements with the very same dependencies. Those are further differently organized. Related domain concepts are represented with the same color:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-1.png\" /><br /><i class=\"caption\">Low cohesion, tight coupling</i></p>\r\n\r\n<p>Elements in the first picture have no explicit boundaries; they are an example of so-called coincidental cohesion. Such architecture is known as the Big Ball of Mud or the God Object (in OOP code).</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-2.png\" /><br /><i class=\"caption\">High cohesion, tight coupling</i></p>\r\n\r\n<p>The second picture shows a system with three modules and a lot of dependencies between them. Although the modules are highly cohesive, they are cohesive by the <em>wrong key</em>. This happens when code is organized by other than a domain relationship. A typical example is a logical organization of code in the <a href=\"https://en.wikipedia.org/wiki/Multitier_architecture\" href=\"_blank\">Layered Architecture</a>: just imagine modules such as controllers, repositories, services, etc. Have you seen these already somewhere? Hell yeah!</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-3.png\" /><br /><i class=\"caption\">High cohesion, loose coupling</i></p>\r\n\r\n<p>The system in the third picture shows the ideal scenario: correctly organized modules leading to high cohesion and loose coupling. The right key for organization is functionality, in other words, a business domain. <strong>The domain defines abstractions with a stable purpose the cohesion is driven upon</strong>. By the way, that is the main idea of the <a href=\"https://en.wikipedia.org/wiki/Domain-driven_design\" href=\"_blank\">Domain-Driven Design</a>.</p>\r\n\r\n<h3>Focus on cohesion, not coupling</h3>\r\n\r\n<p>We have exhausted all variants except one: a system with low cohesion and loose coupling. Is it even possible to have such architecture? Unfortunately, it is, and it is actually pretty common.</p>\r\n\r\n<p>Systems with low cohesion and loose coupling are the result of incorrect understanding of the domain and applying purely technical approaches to decouple the modules in an arbitrary way. <em>Interfaces everywhere</em> with no abstraction representing a domain purpose are typical for systems built in this way.</p>\r\n\r\n<p><a href=\"/too-many-interfaces\" target=\"_blank\">Misuse of interfaces</a> will not actually reduce coupling anyway, it just moves it into the runtime.</p>\r\n\r\n<p>Striving for loose coupling at any cost can (and will) harm cohesion. As <strong>loose coupling is driven by high cohesion</strong>, we should strive for high cohesion in the first place.</p>\r\n\r\n<h3>Level of abstraction</h3>\r\n\r\n<p>Yes, high cohesion does not only make the system easy to understand and change, it also <strong>reduces the level of coupling</strong>.</p>\r\n\r\n<p>How is this even possible? Common sense says that the dependencies do not disappear simply by reorganizing elements. While this is true for the overall system dependencies, high cohesion does reduce dependencies on a higher level of abstraction.</p>\r\n\r\n<p>That is, although the absolute amount of dependencies remains the same, the coupling is tackled on different levels of abstraction.</p>\r\n\r\n<blockquote class=\"quote\">The whole is greater than the sum of the parts. ~ Aristotle</blockquote>\r\n\r\n<p>Indeed, we can ignore the interdependencies inside modules and thus get a simplified big picture with only three loosely coupled elements:</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/cohesion-4.png\" /><br /><i class=\"caption\">Coupling on the higher level of abstraction is dramatically reduced</i></p>\r\n\r\n<p>Neat. As we see, <strong>high cohesion actually results in loose coupling</strong>!</p>\r\n\r\n<h2>Talk to me in code!</h2>\r\n\r\n<p>Pictures are nice, but as software developers, we trust only code, don’t we? Alright, I have some code for you. Consider a simple class for a Book Store (in JavaScript, whatever):</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nclass BookStore {\r\n  add(book) { … }\r\n  remove(book) { … }\r\n  sale(book) { … }\r\n  receiptFor(book) { … }\r\n}\r\n</pre>\r\n\r\n<p>This class does literally everything. Its cohesion is pretty low and all clients, whatever their needs are, will be coupled to it. It is an example of a God Object. We can do better:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nclass Inventory {\r\n  add(book) { … }\r\n  remove(book) { … }\r\n}\r\n\r\nclass Sales {\r\n  sale(book) { … }\r\n  receiptFor(book) { … }\r\n}\r\n</pre>\r\n\r\n<p>The <code>Inventory</code> class looks fine, but what about <code>Sales</code>? Must sales and accounting really be so tightly related? Maybe it would be better to split the functionalities into more cohesive classes:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nclass Sales {\r\n  sale(book) { … }\r\n}\r\n\r\nclass Accounting {\r\n  receiptFor(book) { … }\r\n}\r\n</pre>\r\n\r\n<p>But what if our Book Store is just a small family business with one clerk handling sales together with accounting on one old cash desk? We have just hit the nail on the head: we cannot really know what the right cohesion key is unless we know the domain really well. True <strong>cohesion is defined by the clients</strong>. High cohesion is achieved when there is no way to split the module any further while still satisfying the needs of the client. By the way, this is exactly what the <a href=\"https://en.wikipedia.org/wiki/Single-responsibility_principle\" href=\"_blank\">Single Responsibility Principle</a> teaches us.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>High cohesion and loose coupling are the main design drivers towards simple system architecture, which is easy to understand, change, and maintain. High cohesion and loose coupling help us reduce accidental complexity and create modules with well-defined boundaries.</p>\r\n\r\n<ul>\r\n  <li>Coupling is about <em>connections</em>, cohesion is about <em>belonging together</em>.</li>\r\n  <li>Cohesion cannot be created automatically; instead it is <em>discovered in a context</em>.</li>\r\n  <li>Cohesion is defined by the <em>clients</em>.</li>\r\n  <li>True cohesion is <em>domain-driven</em>.</li>\r\n  <li>High cohesion results in <em>loose coupling</em>.</li>\r\n</ul>\r\n\r\n<p>High cohesion is to die for. It enables all others, loose coupling included.</p>\r\n\r\n<p>Happy coding!</p>', 'Software Architecture,Design Patterns', 'false', 'false', 1, 1),
(81, 'spring-http-message-converters-customizing', 1557010800, 'Spring HTTP Message Converters Customizing', '<p>A deep look at how Spring\'s HTTP Message Converters work.</p>', '<p>A typical REST endpoint implemented using the Spring framework looks as follows:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@RestController\r\npublic class SampleController {\r\n\r\n  @GetMapping(path = \"/data\", produces = {\r\n      MediaType.APPLICATION_JSON_VALUE,\r\n      MediaType.APPLICATION_XML_VALUE})\r\n  public SampleData sampleData() {\r\n    return new SampleData(\r\n        new String[]{\"sample1\", \"sample2\"});\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Internally, Spring converts the <code>SampleData</code> object using a <code>HttpMessageConverter</code>.</p>\r\n\r\n<p>Spring brings a set of default converters and you can customize your own one. For instance, when the <code>SampleData</code> looks like this:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Data\r\n@AllArgsConstructor\r\npublic class SampleData {\r\n\r\n  private String[] data;\r\n}\r\n</pre>\r\n\r\nThen, the default XML conversion ends up with the following structure:\r\n\r\n<pre class=\"brush: xml\">\r\n&lt;SampleData&gt;\r\n  &lt;data&gt;\r\n    &lt;data&gt;sample1&lt;/data&gt;\r\n    &lt;data&gt;sample2&lt;/data&gt;\r\n  &lt;/data&gt;\r\n&lt;/SampleData&gt;\r\n</pre>\r\n\r\nBut you may prefer a different structure:\r\n\r\n<pre class=\"brush: xml\">\r\n&lt;SampleData&gt;\r\n  &lt;data&gt;sample1&lt;/data&gt;\r\n  &lt;data&gt;sample2&lt;/data&gt;\r\n&lt;/SampleData&gt;\r\n</pre>\r\n\r\n<p>To achieve this with Spring Boot (you need <code>spring-boot-starter-web</code> dependency and <code>@EnableAutoConfiguration</code> annotation on your application configuration), you have just to register a converter bean:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Bean\r\npublic MappingJackson2XmlHttpMessageConverter \r\n      mappingJackson2XmlHttpMessageConverter() {\r\n  return new MappingJackson2XmlHttpMessageConverter(\r\n      new Jackson2ObjectMapperBuilder()\r\n          .defaultUseWrapper(false)\r\n          .createXmlMapper(true)\r\n          .build()\r\n  );\r\n}\r\n</pre>\r\n\r\n<p>We could end here as things are so easy with Spring Boot — the converter is registered and used in the needed order automatically.</p>\r\n\r\n<p>As a note, it is worth mentioning another option to customize message converters with Spring Boot — a <code>WebMvcConfigurer</code> configuration. The interface comes actually from Spring MVC, but its usage from within a Spring Boot application has some specifics:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\nclass Config implements WebMvcConfigurer {\r\n\r\n  @Override\r\n  public void configureMessageConverters(\r\n        List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) {\r\n    converters.add(0, new MappingJackson2XmlHttpMessageConverter(\r\n        new Jackson2ObjectMapperBuilder()\r\n            .defaultUseWrapper(false)\r\n            .createXmlMapper(true)\r\n            .build()\r\n    ));\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Spring Boot web auto-configuration (concretely <code>WebMvcAutoConfiguration</code>) brings a default configurer which means we can register as many custom configurers we want to without danger to overwrite the default configuration.</p>\r\n\r\n<p>That\'s why we can overwrite the <code>configureMessageConvertersmethod</code> without losing the default message converters, but we have to put it at the beginning of the list to take over from the already included defaults (by converting the first applicable converter will be used).</p>\r\n\r\n<h2>Spring Boot Aside</h2>\r\n\r\n<p>If we can\'t use Spring Boot and must stick with Spring framework only, just registering a converter bean is no more enough.</p>\r\n\r\n<p>One option is to use your own <code>WebMvcConfigurer</code> as we already seen above. But the situation without Spring Boot is slightly different.</p>\r\n\r\n<p>Spring Web MVC default web configuration (enabled by annotating a configuration with <code>@EnableWebMvc</code>) collects instances of <code>WebMvcConfigurer</code> into a composite where all the converters live in a list.</p>\r\n\r\n<p>That means, overwriting the <code>configureMessageConverters</code>method would remove the default converters as well which is, usually, undesirable.</p>\r\n\r\n<p>In this case, the method override will do the job. Again, you have to put the custom converter at the beginning of the list:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@EnableWebMvc\r\n@Configuration\r\nclass MessageConvertersConfig implements WebMvcConfigurer {\r\n\r\n  @Override\r\n  public void extendMessageConverters(\r\n        List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) {\r\n    converters.add(0, new MappingJackson2XmlHttpMessageConverter(\r\n        new Jackson2ObjectMapperBuilder()\r\n            .defaultUseWrapper(false)\r\n            .createXmlMapper(true)\r\n            .build()\r\n    ));\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>Another way is to overwrite the entire configuration. This assumes removing <code>@EnableWebMvc</code> from your codebase. The code is similar to the previous one:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\nclass WebConfig extends WebMvcConfigurationSupport {\r\n\r\n  @Override\r\n  protected void extendMessageConverters(\r\n        List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) {\r\n    converters.add(0, new MappingJackson2XmlHttpMessageConverter(\r\n         new Jackson2ObjectMapperBuilder()\r\n            .defaultUseWrapper(false)\r\n            .createXmlMapper(true)\r\n            .build()\r\n    ));\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>As you can see, there is more to be aware of when working without the convenience of using Spring Boot, but it’s still pretty straightforward.</p>\r\n\r\n<p>And that’s it. I hope this helped a bit.</p>\r\n\r\n<p>You can find working examples on <a href=\"https://github.com/ttulka/spring-boot-samples/tree/master/message-converters\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy converting!</p>\r\n', 'Programming,Java,Spring,Spring Boot', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(82, 'good-and-bad-monolith', 1591660800, 'Good and Bad Monolith', '<p>It is unfortunate that monolith has become a dirty word. In fact, a physical monolith is typically the right thing to do. Pure evil is monolithic thinking.</p>', '<p>After several years of the <a href=\"https://netflixtechblog.com/tagged/microservices\" target=\"_blank\">microservice</a> hype, <a href=\"https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/\" target=\"_blank\">it</a> <a href=\"https://twitter.com/kelseyhightower/status/940259898331238402\" target=\"_blank\">now</a> <a href=\"https://www.infoq.com/news/2020/04/microservices-back-again/\" target=\"_blank\">seems</a> monoliths are cool again! Does this mean we have learned a lesson?</p>\r\n\r\n<p>I guess at least we accepted what Neal Ford stated in his book Building Evolutionary Architectures:</p>\r\n\r\n<blockquote class=\"quote\">If you can’t build a monolith, what makes you think microservices are the answer?</blockquote>\r\n\r\n<p>The disturbing question is: Why is monolith synonymous with a bad design for some and yet the right thing to do for others?</p>\r\n\r\n<h2>Two Kinds of Monolith</h2>\r\n\r\n<p>The reason is, <strong>there are two different kinds of monolith</strong>: physical and logical. While one is mostly a good thing, the other is pure evil.</p>\r\n\r\n<h3>Physical Monolith</h3>\r\n\r\n<p>The first kind of monolith is what we usually picture under the word: a physical block of software, typically running as a single process.</p>\r\n\r\n<p>A <em>physically monolithic</em> system is developed and built as a single artifact, deployed all at once and falling down in its entirety. Resources such as a database are often shared, communication is local, interprocess.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/monolith.png\" alt=\"Monolith\" /><br /><i class=\"caption\">Monolith</i></p>\r\n\r\n<p>The opposite is then a <em>distributed system</em> composed of multiple physically independent components each running in its own process. Each component owns its resources and communication is performed in a remote manner.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/distributed-system.png\" alt=\"Distributed system\" /><br /><i class=\"caption\">Distributed system</i></p>\r\n\r\n<p>A <strong>physical monolith is no anti-pattern</strong>, it is a good thing to start with as it is easy to build, deploy, operate, and reason about.</p>\r\n\r\n<p>Physically monolithic applications are pretty performant as there are no additional overheads in communication. Cross-cutting aspects are much simpler because no special platform (such as a service mesh) is needed.</p>\r\n\r\n<p>As the system gets bigger, further partitioning is possible. A popular option is to apply “satellite” architecture where bottleneck services are separated from the monolithic base.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/monolith-with-satelites.png\" alt=\"Monolith with satellites\" /><br /><i class=\"caption\">Monolith with satellites</i></p>\r\n\r\n<h3>Logical Monolith</h3>\r\n\r\n<p>The second kind is a <em>logically monolithic</em> system. Other names used are Big ball of mud, Spaghetti code, etc. Logically monolithic codebases lack boundaries (technologies are not service boundaries!), everything is coupled to everything and no visible architecture is to be found.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/logical-monolith.png\" alt=\"Logical monolith\" /><br /><i class=\"caption\">Logical monolith</i></p>\r\n\r\n<p><strong>Logical monoliths are evil</strong> and dangerous constructs that cause high complexity and tight coupling of building blocks making development expensive and error-prone.</p>\r\n\r\n<p>Logically monolithic software is unmaintainable on a scale and exponentially corrodes.</p>\r\n\r\n<h2>Modular and Distributed Monoliths</h2>\r\n\r\n<p>By doing things right or very wrong, you will end up with one of two types of systems: modular or distributed monolith, respectively.</p>\r\n\r\n<h3>Modular Monolith (Modulith)</h3>\r\n\r\n<p>The opposite of a logical monolith is a <em>modular monolith</em> (or, if you like, <em>modulith</em>). In a modular codebase business capabilities are worked out by services with explicit logical (not necessarily physical) boundaries.</p>\r\n\r\n<p>A <strong>modular monolith is probably the best architectural approach</strong> for most applications. It is easy to extend, maintain, and reason about.</p>\r\n\r\n<p>That means, moduliths are really cool!</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/modulith.png\" alt=\"Modulith\" /><br /><i class=\"caption\">Modular monolith (modulith)</i></p>\r\n\r\n<p>Although the logical and physical natures of monoliths are independent, they often go hand in hand. That is why people easily confuse them with each other.</p>\r\n\r\n<p>As the boundaries in monolithic codebases are typically not physical it is easy to cross them. A monolithic codebase, therefore, requires great discipline.</p>\r\n\r\n<h3>Distributed Monolith</h3>\r\n\r\n<p>A logically, but not physically, monolithic system is called a <em>distributed monolith</em>. Distributed monoliths have all the drawbacks of distributed systems with almost none of the benefits.</p>\r\n\r\n<p>While dealing with the Big ball of mud is a pain, <strong>distributed monoliths are a real disaster</strong>.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/distributed-monolith.png\" alt=\"Distributed monolith\" /><br /><i class=\"caption\">Distributed monolith</i></p>\r\n\r\n<p>Systems often end up as distributed monoliths while adapting the <a href=\"https://twitter.com/tomas_tulka/status/1193911453595643906\" target=\"_blank\">microservices</a> approach incorrectly.</p>\r\n\r\n<h2>From Microservices to Monolith</h2>\r\n\r\n<p>In the last years the microservices movement promised us a lot. The catch is, <strong>microservices focus only on physical</strong> monoliths, not logical ones.</p>\r\n\r\n<p>To understand why we have to take a closer look at what microservices really attempt to solve.</p>\r\n\r\n<p>A <em>microservice</em> is a service with some technical additions (independent development cycle). It is important to note the word “technical” — as the logically monolithic design is obviously a logical, not technical, issue, there is nothing microservices could potentially do for us!</p>\r\n\r\n<p>Microservices propose a solution to tackle physical monoliths only. That is the reason so many attempts to build microservices failed badly, simply because the wrong issue was addressed and the true problem only got bigger (and, even worse, distributed).</p>\r\n\r\n<p>With the logically monolithic design, microservices do not come to the rescue, more the opposite.</p>\r\n\r\n<h2>Services First</h2>\r\n\r\n<p>We can think of microservices as a specific approach to Service-oriented architecture (SOA).</p>\r\n\r\n<p>There are several definitions of SOA, but we will focus mainly on the concept of a <em>service</em>, because it is the most significant. I use this modified service definition from <a href=\"http://udidahan.com/2010/11/15/the-known-unknowns-of-soa/\" target=\"_blank\">Udi Dahan</a>:</p>\r\n\r\n<p class=\"definition\">A service is the autonomous unit of logic for a specific business capability.</p>\r\n\r\n<p>Now, it is obvious why microservices as such cannot really help us with the logically monolithic design: monolithically designed microservices are no services at all, they are mere physical components!</p>\r\n\r\n<p>Business alone has the key to <a href=\"/colored-services\" target=\"_blank\">defining our services</a> correctly. <strong>Only well-designed services can tackle logical monoliths</strong> and profit from the microservices approach.</p>\r\n\r\n<p>It is a hard task, but Domain-driven design can help us a lot!</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>If you struggle with your monolithic system, the problem may likely lie in its logically monolithic design.</p>\r\n\r\n<p>The physical nature of the monolith is usually a secondary problem, easy to solve after the proper service-oriented design has been applied.</p>\r\n\r\n<p>Once the logical monolith is resolved, microservices architecture is just one step further...</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/monolith/types-of-systems.png\" alt=\"Types of systems by physical and logical architecture\" /><br /><i class=\"caption\">Types of systems by physical and logical architecture</i></p>', 'Software Architecture,Microservices,SOA,Monolith,DDD', 'false', 'false', 1, 2),
(83, 'shades-of-tests', 1607674000, 'Shades of Tests', '<p>Practical examples of software testing in Java using Spring Boot Test without mocking frameworks.</p>', '<p class=\"note\"><span class=\"title\">Disclaimer:</span> For purposes of this text, when I’m talking about testing I mean functional testing. Albeit non-functional testing is an important part of software development I will ignore it here entirely.</p>\r\n\r\n<p>There are many shades of testing in software development. It’s handy for every developer to know the differences and trade-offs to make the testing as <em>effective</em> as possible.</p>\r\n\r\n<p>I use the word “effective” on purpose as I see a lot of effort invested in making testing as <em>efficient</em> as possible, but unfortunately in the wrong kind of testing.</p>\r\n\r\n<p>Features-rich tools are nice to write and execute end-to-end tests efficiently, but results will never be as quick, cheap, applicable and stable as simple but numerous unit tests.</p>\r\n\r\n<p>It’s like trying to optimize a bubble-sort algorithm: you can make it quite efficient, but it still will never beat the merge-sort.</p>\r\n\r\n<p>Understanding different kinds of tests and where to use what is crucial for effectivity.</p>\r\n\r\n<h2>The Test Pyramid</h2>\r\n\r\n<p>When talking about testing enterprise software, the (in)famous <a href=\"https://martinfowler.com/bliki/TestPyramid.html\">Test Pyramid</a> always comes up.</p>\r\n\r\n<p>Developers find the Test Pyramid often annoying for two main reasons:</p>\r\n\r\n<ul>\r\n  <li>it doesn’t fit the product nature, or</li>\r\n  <li>it’s not completely understood.</li>\r\n</ul>\r\n\r\n<p>The former could be really justified, especially for GUI software or projects that focus particularly on integration. That’s why I have to emphasize I will talk about enterprise software development exclusively for the rest of this text.</p>\r\n\r\n<p>The latter reason for the dislike is the problem I’d like to address.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/simple-test-pyramid.png\" alt=\"Simple Test Pyramid\" /><br /><i class=\"caption\">Simple Test Pyramid</i></p>\r\n\r\n<p>The common understanding of the Test Pyramid is its volume aspect: the bigger the volume the bigger the amount of tests of that kind. Simply put, the test base should contain a lot of unit tests, some integration tests, and just a few UI tests.</p>\r\n\r\n<p>However, the Pyramid has more to offer. With the increasing volume increase speed and costs as well. While unit tests are quick and cheap, UI tests tend to be very slow and expensive.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/test-pyramid-speed-cost.png\" alt=\"Test Pyramid with speed and cost axis\" /><br /><i class=\"caption\">Test Pyramid with speed and cost axis</i></p>\r\n\r\n<p>The reason is in the complexity of the tests. Integration tests are obviously more complicated than unit tests and so on. More components are involved, more communication must be worked out, the computer(s) must execute more instruction. Tests are more difficult (expensive) to develop, set up, execute.</p>\r\n\r\n<p>These additional aspects explain the economic reasons behind the different recommended amounts. And what’s more, there is another characteristic we can put to the axis: <em>applicability</em>.</p>\r\n\r\n<p>With increasing volume of the Pyramid the applicability of tests declines. It’s obvious that unit tests of the core domain will always apply, but there could be several UIs accessing its functionality. <a href=\"/spring-boot-custom-components\">Components</a> can be integrated into multiple different systems as well.</p>\r\n\r\n<p>With the absence of unit tests, integration tests would need to be duplicated for every system and UI tests for every client. This is expensive, slow, and tedious. Besides, the core domain is usually the most stable piece of software while its clients tend to be pretty volatile. This leads us to another aspect: <em>stability</em>.</p>\r\n\r\n<p>Tests that frequently require changes and adjustments are often abandoned or ignored. Stability of tests goes down with decreasing volume of the Pyramid.</p>\r\n\r\n<p>Last but not least, tests on the top of the Pyramid are often flaky. Flaky tests are especially annoying when the test suite is slow.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/test-pyramid-characteristics.png\" alt=\"Test Pyramid characteristics\" /><br /><i class=\"caption\">Test Pyramid characteristics</i></p>\r\n\r\n<h2>Kinds of Tests</h2>\r\n\r\n<p>The terminology is vague and ambiguous and there are no official definitions of different types of tests. The terms are overloaded and can obtain various meanings among different people.</p>\r\n\r\n<p>I’m not aspiring to standardize the terminology, you are free to use any name you like. However, to discuss the concept it is important to unify the name, at least for purposes of this text.</p>\r\n\r\n<p>We will refine the Test Pyramid into the following levels:</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/extended-test-pyramid.png\" alt=\"Extended Test Pyramid\" /><br /><i class=\"caption\">Extended Test Pyramid</i></p>\r\n\r\n<h3>(Atom) Unit Tests</h3>\r\n\r\n<p>This definitely the most unfortunate name of all. The problem is, a <em>unit</em> doesn’t necessarily be a small piece of software with a single responsibility and a well-defined API for a business capability, as it should in the context of unit testing. A unit means generally any piece of software, in an extreme case the entire system.</p>\r\n\r\n<p>Albeit it’d be really handy to have a more descriptive name for the unit test, unfortunately I’m not aware of any. We can try it with things like element tests or atom tests to emphasize the atomicity of the unit under test, but the term unit tests is strongly adopted and ubiquitous and is here to stay with us forever.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-atom.png\" alt=\"Unit Test\" /><br /><i class=\"caption\">Atom has no dependencies</i></p>\r\n\r\n<p>How does a unit test look like anyway? We don’t really need any special magic here:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass PromoProductTest {\r\n\r\n  @Test\r\n  void promo_product_is_not_deliverable() {\r\n    Product product = new PromoProduct(\r\n        \"Test product\", 123);\r\n\r\n    assertThat(product.deliverable()).isFalse();\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>No dependencies, tested business capabilities against the API, this is a nice unit test.</p>\r\n\r\n<p>Here is the beauty about unit testing: <strong>unit tests require code to be testable</strong>.</p>\r\n\r\n<p>Testable code must be simple, cohesive and loosely coupled. Easy to say, hard to do. That’s usually the reason a codebase has so few unit tests - they are hard to write because the code is not testable.</p>\r\n\r\n<p><strong>Tests are the very first client of the code</strong> and help you to understand the API to be easy to use and understand.</p>\r\n\r\n<p>Another <strong>benefit of good tests is their documentary function</strong>. Tests show the proper usage of code to serve as an exhaustive user reference.</p>\r\n\r\n<p>I often refer to testing as to the “king’s discipline” of software development. Only good developers can do it right.</p>\r\n\r\n<p>True (atom) unit tests require the domain model to be pure. This is not always easy or even possible to achieve.</p>\r\n\r\n<h3>Component Tests</h3>\r\n\r\n<p>When purity is not feasible, test doubles come to help. External dependencies such as databases or filesystems can be stubbed for testing via alternative implementations of resource adapters.</p>\r\n\r\n<p>It’s important to focus on the business functionality. With test-doubles you can easily find yourself writing tests that check only stubbed calls or integration with doubles. In this case, it’s time to move on to component testing.</p>\r\n\r\n<p>Component tests lie somewhere between unit and integration tests. They are no pure unit tests as more than one “atom” is involved.</p>\r\n\r\n<p>However, component tests are no pure integration tests either, as the unit is not completely and correctly integrated - stubbing is often used, only directly needed components are integrated, not the whole system.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-component.png\" alt=\"Component Test\" /><br /><i class=\"caption\">Component’s dependencies are stubbed</i></p>\r\n\r\n<p>Spring Boot offers so-called \"slicing\" where only needed pieces of the system are included in the test:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@DataJpaTest\r\n@Import(JpaProductConfig.class)\r\nclass PromoteTest {\r\n\r\n  @Autowired\r\n  private Promote promote;\r\n\r\n  @Test\r\n  void product_is_promoted() {\r\n    var product = promote.product(new Promote.ToPromote(\r\n        \"Test product\", 123));\r\n\r\n    assertAll(() -> assertThat(product.id()).isNotNull(),\r\n              () -> assertThat(product.deliverable()).isFalse());\r\n  }\r\n}\r\n</pre>\r\n\r\n<h3>Integration Tests</h3>\r\n\r\n<p>Integration tests check the component fully integrated in the system. The system is seen as a black box accessible only via its public APIs.</p>\r\n\r\n<p>The system is usually bootstrapped from within the test, which makes it possible to run the system with special configuration. For example, a test database with a testing dataset could be used.</p>\r\n\r\n<p>Also, the component under the test is typically deployed into the core system without any other components.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-integration.png\" alt=\"Integration Test\" /><br /><i class=\"caption\">Component is integrated in a system slice</i></p>\r\n\r\n<p>Spring Boot brings the <code>@SpringBootTest</code> annotation to support integration testing:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@SpringBootTest(classes = Application.class,\r\n    webEnvironment = RANDOM_PORT)\r\nclass ProductsTestIT {\r\n\r\n  @LocalServerPort\r\n  private int port;\r\n\r\n  @Test\r\n  void product_is_promoted() {\r\n    with()\r\n        .port(port)\r\n        .basePath(\"/products/promoted\")\r\n        .contentType(ContentType.JSON)\r\n        .body(\"{\\\"name\\\":\\\"Test\\\",\"\r\n             + \"\\\"price\\\":123}\")\r\n        .post().\r\n    then()\r\n        .statusCode(201)\r\n        .body(\"id\", is(notNullValue()))\r\n        .body(\"name\", equalTo(\"Test\"))\r\n        .body(\"price\", equalTo(123));\r\n  }\r\n}\r\n</pre>\r\n\r\n<h3>End-to-End Tests</h3>\r\n\r\n<p>End-to-end tests, also known as <em>system tests</em>, runs typically in three phases:</p>\r\n\r\n<ul>\r\n  <li>preparation, system startup;</li>\r\n  <li>testing, tests execution;</li>\r\n  <li>tear down, system shutdown, cleanup.</li>\r\n</ul>\r\n\r\n<p>As the system is started only once, there is no room for adjustments based on special tests’ needs. The system runs as a separate process, deployed in an environment as similar to production as possible.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-e2e.png\" alt=\"E2E Test\" /><br /><i class=\"caption\">Component is integrated in the whole system</i></p>\r\n\r\n<p><a href=\"https://maven.apache.org/surefire/maven-failsafe-plugin/\" target=\"_blank\">Maven Failsafe Plugin</a> address this kind of testing, as well as <a href=\"https://www.testcontainers.org/\" target=\"_blank\">Testcontainers</a>:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Testcontainers\r\nclass ProductsTestE2E {\r\n\r\n  @Container\r\n  public GenericContainer app = new GenericContainer(\r\n      new ImageFromDockerfile()\r\n          .withFileFromPath(\"app.jar\", \r\n              Path.of(\"build/libs/app.jar\"))\r\n          .withDockerfileFromBuilder(b -> b\r\n              .from(\"openjdk:11-jre-slim\")\r\n              .copy(\"app.jar\", \"/app.jar\")\r\n              .entryPoint(\"java\", \"-jar\", \"/app.jar\")\r\n              .build()))\r\n      .withExposedPorts(8080);\r\n\r\n  private int port;\r\n\r\n  @BeforeEach\r\n  public void setUp() {\r\n    port = app.getFirstMappedPort();\r\n  }\r\n\r\n  @Test\r\n  void product_is_promoted() {\r\n    with()\r\n        .port(port)\r\n        .basePath(\"/products/promoted\")\r\n        .contentType(ContentType.JSON)\r\n        .body(\"{\\\"name\\\":\\\"Test\\\",\"\r\n             + \"\\\"price\\\":123}\")\r\n        .post().\r\n    then()\r\n        .statusCode(201)\r\n        .body(\"id\", is(notNullValue()))\r\n        .body(\"name\", equalTo(\"Test\"))\r\n        .body(\"price\", equalTo(123));\r\n  }\r\n}\r\n</pre>\r\n\r\n<h3>UI Tests</h3>\r\n\r\n<p>In the context of this text, UI testing is different from testing a UI.</p>\r\n\r\n<p>UIs themselves should be unit-tested in isolation against a stubbed backend, but this is no full replacement for UI testing a UI integrated in the system.</p>\r\n\r\n<p>UI tests in enterprise software development are end-to-end tests with actions triggered from a GUI (web or desktop) instead of Restful or similar APIs.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-ui.png\" alt=\"UI Test\" /><br /><i class=\"caption\">Component is accessed via UI</i></p>\r\n\r\n<p>UI tests tend to be very slow, unstable and flaky. Unfortunately, it is still not unusual to have a test base containing UI tests mostly or even only.</p>\r\n\r\n<p>The test base should contain UI tests for elementary user interactions with the UI. Especially to check edge conditions of user input, validation and error handling towards the user.</p>\r\n\r\n<p>The amount of UI tests should be kept on a small maintainable amount, everything must be automated.</p>\r\n\r\n<p>There are plenty of tools for automation of UI testing such as <a href=\"https://www.selenium.dev/\" target=\"_blank\">Selenium</a>:</p>\r\n\r\n<pre class=\"brush: java\">\r\nclass ProductsTestUI {\r\n\r\n  private WebDriver webDriver;\r\n\r\n  @BeforeEach\r\n  void setUp() {\r\n    webDriver = new HtmlUnitDriver();\r\n    webDriver.get(\"http://localhost:8080/ui\");\r\n  }\r\n\r\n  @Test\r\n  void product_is_promoted() {\r\n    var form = webDriver.findElement(By.id(\"form-promote\"));\r\n    form.findElement(By.className(\"name\")).sendKeys(\"Test\");\r\n    form.findElement(By.className(\"price\")).sendKeys(\"123\");\r\n    form.findElement(By.className(\"submit\")).click();\r\n\r\n    var list = webDriver.findElement(By.id(\"products-promo\"))\r\n        .findElements(By.className(\"product\"));\r\n\r\n    assertAll(\r\n        () -> assertThat(list).hasSize(1),\r\n\r\n        () -> assertThat(list.get(0).findElement(\r\n            By.className(\"id\")).getText()).isNotNull(),\r\n\r\n        () -> assertThat(list.get(0).findElement(\r\n            By.className(\"name\")).getText()).isEqualTo(\"Test\"),\r\n\r\n        () -> assertThat(list.get(0).findElement(\r\n            By.className(\"price\")).getText()).isEqualTo(\"123\")\r\n    );\r\n  }\r\n}\r\n</pre>\r\n\r\n<h3>UX Tests</h3>\r\n\r\n<p>Albeit there is no excuse to have an automatic test for everything, one kind of testing does require manual labor.</p>\r\n\r\n<p>Look’n’feel and user experience (UX) in general cannot be evaluated by any machine, a human is required here.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/shades-of-tests/unit-ux.png\" alt=\"UX Test\" /><br /><i class=\"caption\">User experience matters</i></p>\r\n\r\n<h2>Source Code</h2>\r\n\r\n<p>The source code for discussed examples can be found on <a href=\"https://github.com/ttulka/blog-code-samples/tree/master/ecommerce-example-testing\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy testing!</p>\r\n', 'Testing,Programming,Java,Spring Boot,Spring', 'false', 'false', 1, 1),
(84, 'you-are-not-gonna-need-microservices', 1595286000, 'You Aren’t Gonna Need Microservices', '<p>Do you really need a cluster of pods, service mesh, and stream-processing platform right now?</p>', '<p><em>You Aren’t Gonna Need It</em> (YAGNI) is one of the most important principles in software development.</p>\r\n\r\n<blockquote class=\"quote\">Perfection is achieved not when there is nothing more to add, but when there is nothing more to take away. ~ Antoine de Saint-Exupery</blockquote>\r\n\r\n<p>You can see YAGNI as a special case of the <em>Keep It Simple Stupid</em> (KISS) principle. YAGNI applies to a higher level as it tells us to cut off any unnecessary part while KISS advises making the rest as simple as possible.</p>\r\n\r\n<blockquote class=\"quote\">Everything should be made as simple as possible, but no simpler. ~ Albert Einstein</blockquote>\r\n\r\n<p>Violation of YAGNI is sometimes called <em>overengineering</em>. A feature for every possible case, functions/methods with a lot of input parameters, multiple if-else branches, rich and detailed interfaces, all those could be a smell of overengineering. It’s the opposite of the famous UNIX-mantra: <em>Do one thing well</em>.</p>\r\n\r\n<p>To avoid this trap, always ask yourself a question: <i>“Is it really needed right now?”</i> If the answer is <i>“Well, maybe”</i> then probably not.</p>\r\n\r\n<p>Often, significant architectural decisions must be made to implement a feature. This is a problem because the earlier a decision is made the less information we have and the more likely a wrong decision is to be made. Therefore, it is a good practice to <strong>defer difficult decisions</strong> until the last possible moment. Ignoring YAGNI leads to a violation of this practice.</p>\r\n\r\n<p>The main point of YAGNI is to <strong>pay the costs first when the feature is actually needed</strong>, and not right now, just in case. YAGNI violations make development always expensive because the complexity behind each feature must be paid, even though the feature is not necessary (yet). In other words, accidental complexity rises. And complexity is expensive. Complexity is no fun.</p>\r\n\r\n<p>The same reasoning can be applied to microservices. But, don’t take me wrong: there are plenty of highly useful benefits that the microservices style can bring you and a lot of <a href=\"https://12factor.net\" target=\"_blank\">great principles</a> you can apply even to your monolith. However, the actual question remains the same: <strong>Do you really need them right now?</strong> Do you really need to scale every service up to a thousand instances? Do you need a Kubernetes cluster, Kafka stream-processing platform, and Istio service mesh? Once again: maybe, but probably not right now.</p>\r\n\r\n<h2>Fallacies of microservices</h2>\r\n\r\n<p>Let\'s take a look at some typical fallacies I\'ve observed so far among teams adopting microservices:</p>\r\n\r\n<h3>Microservices help you find boundaries</h3>\r\n\r\n<p>This is probably the most dangerous misunderstanding of microservices and a very expensive mistake you can make. Remember: <strong>Microservices are about technology, boundaries are about business</strong>. No technology can help you define your boundaries, only proper understanding of your business does.</p>\r\n\r\n<p>Microservices are just one option for how to physically separate components once boundaries are correctly defined. At the same time, they make the boundaries hard to change if they are found to be wrong, which often happens, especially in the early phases of product development.</p>\r\n\r\n<p>Try to define your service boundaries without any concerns of technology, build a team around them, implement a few product iterations. As you gain some confidence, you can turn them into microservices, if it will solve an actual problem.</p>\r\n\r\n<h3>Microservices make your system modular</h3>\r\n\r\n<p>Microservices don\'t make your system modular out of the box. <a href=\"https://blog.ttulka.com/good-and-bad-monolith\" target=\"_blank\">Modularity</a> means <em>logical separation</em> of business concepts. Microservices make this separation visible and difficult to cross over. While this is good, it does not come without drawbacks such as resistance to refactoring the structure of the modules.</p>\r\n\r\n<p>Anyway, there are much cheaper options such as using <a href=\"https://github.com/ttulka/ddd-example-ecommerce\" target=\"_blank\">packages/namespaces</a>, <a href=\"https://github.com/ttulka/ddd-example-ecommerce/tree/modulith\" target=\"_blank\">modules/artifacts</a>, etc. Those solutions provide a good level of physical separation along with ease to change.</p>\r\n\r\n<h3>Microservices make your system performant</h3>\r\n\r\n<p>The possibility to scale every single service independently is one of the most emphasized advantages of microservices. While this is true, it comes with costs. Running every service as a separate process can\'t be done without supporting infrastructure. Things like service mesh, service discovery, or load balancer come to mind first.</p>\r\n\r\n<p>Cloud providers offer a working solution for you, but one has to bear in mind that remote calls are always more expensive than inter-process communication. Do you really need to scale all your services? Maybe one or two most loaded would be enough. Vertical scaling or scaling the whole system are options that usually just work and that are much cheaper than microservices.</p>\r\n\r\n<p>Another aspect is good modularity. When service boundaries are incorrectly defined and multiple remote calls must be done to process a request, the overall performance is likely to decrease when turning such a system into microservices.</p>\r\n\r\n<h3>Microservices are cheaper to operate</h3>\r\n\r\n<p>A typical example of this fallacy is Serverless computing. While there definitely are use-cases where the serverless approach can be beneficial and cheap to develop and operate, the amount of such use-cases is pretty limited. What\'s more, it\'s easy to get an impression that going serverless means just writing code and taking no care of how it\'s being operated. This is usually not true. For example, monitoring is still an important and hard problem no one else will solve for you.</p>\r\n\r\n<p>Serverless is an extreme case. An application running in the cloud in the same way as previously in your own server warehouse is likely to be much more expensive. This is very true, especially for so-called Lift&Shift solutions.</p>\r\n\r\n<p>One idea behind microservices is to move a piece of complexity from development into operations, where it is easier (and cheaper) to tackle. Yes, by doing microservices right, you might save some money on development but your operation costs will probably increase rapidly. This is not necessarily a bad thing as you do save some bucks at the end of the day, it\'s just something to be aware of.</p>\r\n\r\n<h3>Microservices are easier to develop</h3>\r\n\r\n<p>Because a lot of complexity is moved to operations, great Ops skills are required inside the teams when taking the microservices-oriented approach. As the need for communication between Devs and Ops becomes crucible, there\'s no room for traditional separation by technical responsibilities. All members of the team must work together on a whole business feature, preferably colocated in the same room.</p>\r\n\r\n<p>Microservices are a practical approach to Agile development and require a real DevOps mindset, where the whole team is responsible for the product from the first sketches to running the production environment.</p>\r\n\r\n<p>If \"DevOps\" in your organization is done by that guy sitting in the last office on the second floor, then you shouldn\'t even think about microservices.</p>\r\n\r\n<h3>Microservices are small</h3>\r\n\r\n<p>Very popular topic and a big misconception. \"Micro\" does mean small or even smaller, right? Small is good, small is easy. Easy to understand, easy to develop, easy to operate… Well, no. <strong>\"Micro\" is not about size, it is about responsibility.</strong> Anecdotes such as <em>two pizza teams</em> or <em>code that fits on my screen</em> might work in some organizations, however, they are no general rules.</p>\r\n\r\n<p>A service should be minimal in its responsibility and API. A service should implement just a single business capability. Nothing more, nothing less. The contract must be simple and clear. No unnecessary pollution, no implementation details.</p>\r\n\r\n<p>The implementation can be big, even huge at the beginning. The bounded context (from Domain-driven development) defines the biggest possible service. That is the boundary to start with. Other services will emerge by themselves as the domain becomes clearer.</p>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>Always ask yourself: <i>\"Do I really need this right now? Is this the simplest thing that can possibly work?\"</i></p>\r\n\r\n<p>Don\'t go microservices just because they look cool and everybody is talking about them - they are no silver bullet. Doing so would be a violation of the YAGNI principle on the architecture level. That is, don\'t do it.</p>\r\n\r\n<p>All in all, you aren\'t gonna need it!</p>', 'Microservices,Software Architecture,Monolith,Design Patterns,SOA', 'false', 'false', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(85, 'spring-boot-custom-components', 1607413000, 'Spring Boot Custom Components', '<p>Let\'s build domain-oriented custom components with Spring Boot starters, auto-configuration and configuration properties.</p>', '<p>Spring Boot philosophy is to develop applications by composing <strong>independent, modular</strong> and <strong>highly configurable</strong> components. Spring Boot provides smart and simple mechanisms and conventions for building such components, that could be easily applied in custom projects.</p>\r\n\r\n<p>In this post, we will explore those ideas, and show how to create <strong>domain-oriented custom components</strong> the Spring Boot way.</p>\r\n\r\n<h2>What Is Spring Boot</h2>\r\n\r\n<p><a href=\"https://spring.io/projects/spring-boot\" target=\"_blank\">Spring Boot</a> takes an <strong>opinionated view</strong> of the <a href=\"https://spring.io\">Spring platform</a> and third-party libraries. This means, all necessary dependencies are included and pre-configured, and there is none or very little additional configuration most Spring Boot applications need to add.</p>\r\n\r\n<p>Dependencies are consistent with each other and tested in integration.</p>\r\n\r\n<p>For web applications, embedded servers (Tomcat, Jetty, Undertow) are included. Even the traditional deployment into an application container is possible, Spring Boot is self-contained as default, everything is included in the final JAR and no additional infrastructure is needed (except JDK, obviously).</p>\r\n\r\n<p>In addition, Spring Boot comes with a lot of <a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-features.html\" target=\"_blank\">production ready features</a>, such as loggers, metrics, auditing, tracing, and monitoring.</p>\r\n\r\n<p>A simple application skeleton can be easily prototyped by <a href=\"https://start.spring.io\">Spring Initializr</a>.</p>\r\n\r\n<h2>Build Systems</h2>\r\n\r\n<p>Spring Boot offers <a href=\"https://docs.spring.io/spring-boot/docs/current/gradle-plugin/reference/htmlsingle/\" target=\"_blank\">Gradle</a> and <a href=\"https://docs.spring.io/spring-boot/docs/current/maven-plugin/reference/htmlsingle/\" target=\"_blank\">Maven</a> plugins for packaging and running Spring Boot applications. It is also possible to create and publish Docker images via the plugins.</p>\r\n\r\n<h3>Gradle</h3>\r\n\r\n<p>Spring Boot comes with <code>org.springframework.boot</code> and <code>io.spring.dependency-management</code> plugins:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\nplugins {\r\n  id \'org.springframework.boot\' version \'2.4.0\'\r\n}\r\n\r\napply plugin: \'io.spring.dependency-management\'\r\n</pre>\r\n\r\n<p>Alternatively, only the dependency management could be used in isolation:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\nplugins {\r\n  id \'io.spring.dependency-management\' version \'1.0.10.RELEASE\'\r\n}\r\n\r\napply plugin: \'io.spring.dependency-management\'\r\n\r\ndependencyManagement {\r\n  dependencies {\r\n    dependency \'org.springframework:spring-core:5.3.0\'\r\n  }\r\n}\r\n</pre>\r\n\r\n<h3>Maven</h3>\r\n\r\n<p>Typically, your Maven POM file inherits from the <code>spring-boot-starter-parent</code> project:</p>\r\n\r\n<pre class=\"brush: xml\">\r\n&lt;parent&gt;\r\n  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n  &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\r\n  &lt;version&gt;2.4.0&lt;/version&gt;\r\n&lt;/parent&gt;\r\n</pre>\r\n\r\n<p>The parent POM brings the Spring Boot plugin, dependency management and other sensible defaults. All default settings can be overwritten in your POM.</p>\r\n\r\n<p>Alternatively, only dependency management can be used without the parent POM by using an import scoped dependency:</p>\r\n\r\n<pre class=\"brush: xml\">\r\n&lt;dependencyManagement&gt;\r\n  &lt;dependencies&gt;\r\n    &lt;dependency&gt;\r\n      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\r\n      &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;\r\n      &lt;version&gt;2.4.0&lt;/version&gt;\r\n      &lt;type&gt;pom&lt;/type&gt;\r\n      &lt;scope&gt;import&lt;/scope&gt;\r\n    &lt;/dependency&gt;\r\n  &lt;/dependencies&gt;\r\n&lt;/dependencyManagement&gt;\r\n</pre>\r\n\r\n<h2>Features Overview</h2>\r\n\r\n<p>Spring Boot introduces several features new to the Spring ecosystem.</p>\r\n\r\n<h3>Spring Boot Application</h3>\r\n\r\n<p class=\"definition\">The <code>SpringApplication</code> class provides a convenient way to bootstrap a Spring application that is started from a <code>main()</code> method.</p>\r\n\r\n<p>When the annotation <code>@SpringBootApplication</code> is included on the main class, the application is bootstrapped, auto-configuration enabled and Spring components scan is set to the current package as root.</p>\r\n\r\n<p>It is recommended to locate the application class in a root package above other classes. So the package defines the search base for the components scan.</p>\r\n\r\n<pre>\r\norg.example.myshop\r\n+- MyShopApplication\r\n+- catalog\r\n+- delivery\r\n+- order\r\n...\r\n</pre>\r\n\r\n<h3>Starters</h3>\r\n\r\n<p class=\"definition\">Starters are a set of convenient dependency descriptors that you can include in your application.</p>\r\n\r\n<p>Starters bring all necessary dependencies and configurations into the application:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\ndependencies {\r\n  implementation \'org.springframework.boot:spring-boot-starter-web\'\r\n}\r\n</pre>\r\n\r\n<p>All starters have the core starter <code>spring-boot-starter</code> a direct dependency, which includes auto-configuration support, logging and YAML.</p>\r\n\r\n<p>Starters are part of Spring Boot philosophy, a convenient way to include components into your application.</p>\r\n\r\n<h3>Auto-Configuration</h3>\r\n\r\n<p class=\"definition\">Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.</p>\r\n\r\n<p>When auto-configuration is enabled, components found on the classpath are automatically loaded and configured.</p>\r\n\r\n<p>Most standard Spring auto-configurations are highly <a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html\" target=\"_blank\">configurable via properties</a>.</p>\r\n\r\n<h3>Application Properties</h3>\r\n\r\n<p>Spring Boot automatically finds and loads <code>application[-profile].(properties|yaml|yml)</code> from the classpath and other default <a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-external-config-files\" target=\"_blank\">locations</a>.</p>\r\n\r\n<p>Properties from the loaded files are added into Spring environment.</p>\r\n\r\n<p>Application properties form a configuration structure of a Spring Boot application with default values, which are meant to be overwritten in runtime from the environment or other <a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-external-config\" target=\"_blank\">external sources</a>.</p>\r\n\r\n<p>Configuration properties are grouped into trees by the context, typically prefixed by the component name and feature:</p>\r\n\r\n<pre class=\"brush: yaml\">\r\n# application.yml\r\n\r\nspring:\r\n  datasource:\r\n    url: jdbc:postgresql://db:5432/test\r\n    username: admin\r\n    password: secret\r\n</pre>\r\n\r\n<h2>Custom Components</h2>\r\n\r\n<p>Features of Spring Boot can serve as a conventional template for custom components. To do things right, a deeper knowledge of Spring Boot concepts and mechanisms is needed.</p>\r\n\r\n<p>In the following section, we take a look at techniques for creating a modular application with independent domain-driven components. The components can be assembled into a single monolithic application or separate applications as microservices afterwards.</p>\r\n\r\n<p>For each component a Spring Boot starter with auto-configuration will be created.</p>\r\n\r\n<p>Assembling of components into an application is achieved simply by adding the component onto the classpath. Practically, by putting the component starter into the application dependencies.</p>\r\n\r\n<p>Spring Boot <strong>components are logically defined by auto-configurations</strong>, starters are an additional mechanism to bring components into the application conveniently.</p>\r\n\r\n<h3>Code Structure</h3>\r\n\r\n<p>We will use Java packages for structuring code by the domain and Maven modules or Gradle sub-projects for technical cuts.</p>\r\n\r\n<p>Every component is a <strong>self-contained business capability service</strong>, exposing multiple artifacts for API and implementation, formed by physical modules.</p>\r\n\r\n<p>A typical component source code structure looks like:</p>\r\n\r\n<pre>\r\ndelivery/	-- comp. root\r\n+- domain/	-- domain API\r\n+- events/	-- events API\r\n+- jdbc/	-- JDBC impl\r\n+- rest/	-- Restful API\r\n+- spring-boot-starter/ -- starter\r\n+- pom.xml\r\n+- build.gradle\r\n\\- settings.gradle\r\n</pre>\r\n\r\n<p>Alternatively, auto-configurations can live in a separate module, which the starter includes as its dependency.</p>\r\n\r\n<p>In a single-application scenario a separate module for a Spring Boot application is created on the root level:</p>\r\n\r\n<pre>\r\nmyshop/\r\n+- application/\r\n+- catalog/\r\n+- delivery/\r\n+- order/\r\n...\r\n</pre>\r\n\r\n<p>In a microservices scenario, each component has its own application module:</p>\r\n\r\n<pre>\r\nmyshop/\r\n+- catalog/\r\n|  +- application/\r\n|  +- domain/\r\n|  +- jdbc/\r\n|  +- rest/\r\n|  \\- spring-boot-starter/\r\n+- delivery/\r\n|  +- application/\r\n|  +- domain/\r\n|  +- events/\r\n|  +- jdbc/\r\n|  +- rest/\r\n|  \\- spring-boot-starter/\r\n+- order/\r\n|  +- application/\r\n|  +- domain/\r\n|  +- events/\r\n|  +- jdbc/\r\n|  +- rest/\r\n|  \\- spring-boot-starter/\r\n...\r\n</pre>\r\n\r\n<p>For both scenarios, the separation of concerns is the structure driver. Every module has its own unique role in the final application.</p>\r\n\r\n<p>Starters bring Spring Boot auto-configuration, applications bring Spring Boot application main classes, but all other modules has no Spring Boot concerns or dependencies.</p>\r\n\r\n<p>For example, even if the REST module is built upon Spring Web, only the corresponding Spring framework dependency should be included:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\n// rest/build.gradle\r\n\r\nimplementation \'org.springframework:spring-web\'\r\n</pre>\r\n\r\n<p>The Spring Boot Web starter will be included into the starter module only:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\n// spring-boot-starter/build.gradle\r\n\r\nimplementation \'org.springframework.boot:spring-boot-starter-web\'\r\n</pre>\r\n\r\n<h3>Java Packages</h3>\r\n\r\n<p>As we build <strong>domain-oriented</strong> components, packages should be domain-driven as well.</p>\r\n\r\n<p>All the modules of a component should share a root package. The modules could be further structured by the domain feature or a technical aspect. Same features should be included in an identical package.</p>\r\n\r\n<p>This strategy enables information hiding of implementation classes using Java package accessibility. Consider an example, where <code>○</code> and <code>●</code> mean <em>public</em> and <em>package-protected</em>, respectively:</p>\r\n\r\n<pre>\r\ndelivery/\r\n+- domain/\r\n|  \\- src/main/java/\r\n|     \\- org.example.myshop.delivery\r\n|        \\- ○DeliveryService.java\r\n+- events/\r\n|  \\- src/main/java/\r\n|     \\- org.example.myshop.delivery\r\n|        \\- ○DeliveryDispatched.java\r\n+- jdbc/\r\n|  \\- src/main/java/\r\n|     \\- org.example.myshop.delivery.jdbc\r\n|        \\- ●DeliveryServiceJdbc.java\r\n+- rest/\r\n|  \\- src/main/java/\r\n|     \\- org.example.myshop.delivery.rest\r\n|        \\- ●DeliveryRestController.java\r\n\\- spring-boot-starter/\r\n   \\- src/main/java/\r\n      \\- org.example.myshop.delivery\r\n         \\- jdbc\r\n            \\- ●DeliveryJdbcConfig.java\r\n         \\- rest\r\n            \\- ●DeliveryRestConfig.java\r\n</pre>\r\n\r\n<p>Classes <code>DeliveryServiceJdbc</code> and <code>DeliveryRestController</code> are in the same package as <code>DeliveryJdbcConfig</code> and <code>DeliveryRestConfig</code>, respectively. This makes them accessible to the configuration classes, which is the only one place they have to be accessible from, and hidden for the rest of the world.</p>\r\n\r\n<p>This kind of protection based on the basic language features is a great asset to the overall modularity, preventing the temptation to access implementation details of a foreign component, and violate its sovereignty so.</p>\r\n\r\n<h3>Custom Spring Boot Starter</h3>\r\n\r\n<p>A typical Spring Boot starter contains auto-configuration code and declared dependencies, and it&#8217;s extensible via configuration properties in a dedicated namespace (prefix).</p>\r\n\r\n<p>By convention, the name of a starter starts with the component name followed by <code>-spring-boot-starter</code> suffix:</p>\r\n\r\n<pre>\r\norg.example.myshop:delivery-spring-boot-starter\r\n</pre>\r\n\r\n<h4>Auto-Configurations</h4>\r\n<p>The entry point to a Spring Boot starter is the <code>META-INF/spring.factories</code> file. Spring Boot checks for the presence of the file within your published JAR. The file should list component auto-configuration classes under the <code>EnableAutoConfiguration</code> key:</p>\r\n\r\n<pre>\r\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\r\n  org.example.myshop.delivery.DeliveryConfiguration\r\n</pre>\r\n\r\n<p class=\"defintion\">Auto-configurations must be loaded that way only. Make sure that they are defined in a specific package space and that they are never the target of component scanning.</p>\r\n\r\n<p>Under the hood, auto-configuration is implemented with standard <code>@Configuration</code> classes. Multiple configuration classes could be composed via the <code>@Import</code> annotation:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\n@Import(JdbcDeliveryConfiguration.class)\r\nclass DeliveryConfiguration {\r\n  ...\r\n}\r\n</pre>\r\n\r\n<p>The auto-configuration creates and registers all necessary Spring beans for the particular component.</p>\r\n\r\n<p>A component starter is the only place configurations should exist. Other modules serve different purposes.</p>\r\n\r\n<h4>Dependencies</h4>\r\n\r\n<p>A Spring Boot starter contains all dependencies required by the component as whole.</p>\r\n\r\n<p>If, for example, a component has a module with Spring Web restful controllers, the starter should contain the corresponding Spring Boot Starter for web:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\nimplementation \'org.springframework.boot:spring-boot-starter-web\'\r\n</pre>\r\n\r\n<p>The minimal dependency every Spring Boot starter must include is Spring Boot core starter:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\nimplementation \'org.springframework.boot:spring-boot-starter\'\r\n</pre>\r\n\r\n<p>The component starters are then added as dependencies into the application module:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\nimplementation \'org.example.myshop:catalog-spring-boot-starter\'\r\nimplementation \'org.example.myshop:delivery-spring-boot-starter\'\r\nimplementation \'org.example.myshop:order-spring-boot-starter\'\r\n...\r\n</pre>\r\n\r\n<h3>Configuration Properties</h3>\r\n\r\n<p class=\"definition\">Spring Boot provides an alternative method of working with properties that lets strongly typed beans govern and validate the configuration of your application.</p>\r\n\r\n<pre class=\"brush: java\">\r\n@ConfigurationProperties(\r\n    prefix = \"myshop.delivery\")\r\n@Setter\r\n@Getter\r\nclass DeliveryProperties {\r\n\r\n  private String cargoName;\r\n  private String dateFormat;\r\n}\r\n</pre>\r\n\r\n<p>Configuration properties are meant to be a convenient way for initializing auto-configuration:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\n@EnableConfigurationProperties(\r\n    DeliveryProperties.class)\r\nclass DeliveryConfiguration {\r\n\r\n  @Bean\r\n  DeliveryService deliveryService(\r\n      DeliveryProperties properties\r\n  ) {\r\n    return new DeliveryServiceImpl(\r\n      properties.getCargoName(),\r\n      properties.getDateFormat()\r\n    );\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>The Spring Boot application defines the configuration structure with default values:</p>\r\n\r\n<pre class=\"brush: yaml\">\r\n# application.yml\r\n\r\nmyshop:\r\n  delivery:\r\n    cargo-name: PPL\r\n    date-format: yyyy-mm-dd\r\n  order:\r\n    prefix-id: OrderID\r\n</pre>\r\n\r\n<p>Defaults can be overwritten in runtime, for example via environment variables:</p>\r\n\r\n<pre>\r\nMYSHOP_DELIVERY_CARGO_NAME=DHL\r\n</pre>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Spring Boot is a great tool for developing <strong>modular monolithic</strong> and <strong>microservices</strong> applications.</p>\r\n\r\n<p>Auto-configurations provide a convenient mechanism for creating independent components in isolation.</p>\r\n\r\n<p>Spring Boot starters contain configurations and dependencies for components and define configuration structure via configuration properties with dedicated namespaces.</p>\r\n\r\n<p>A Spring Boot application assemblies the components and provides cross-cutting concerns in addition.</p>\r\n\r\n<h2>Example</h2>\r\n\r\n<p>An example code of a rich modular Spring Boot application can be found on <a href=\"https://github.com/ttulka/ddd-example-ecommerce-microservices\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<h2>Links</h2>\r\n\r\n<ul>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html\" target=\"_blank\">Spring Boot Reference Documentation</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/gradle-plugin/reference/htmlsingle\" target=\"_blank\">Spring Boot Gradle Plugin Reference Guide</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/maven-plugin/reference/htmlsingle\" target=\"_blank\">Spring Boot Maven Plugin Documentation</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html\" target=\"_blank\">Getting Started With Spring Boot</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html\" target=\"_blank\">Using Spring Boot</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/howto.html\" target=\"_blank\">Spring Boot How-to Guides</a></li>\r\n  <li><a href=\"https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-developing-auto-configuration\" target=\"_blank\">Creating Your Own Auto-configuration</a></li>\r\n  <li><a href=\"https://start.spring.io\" target=\"_blank\">Spring Initializr</a></li>\r\n</ul>', 'Spring Boot,Spring,Software Architecture', 'false', 'false', 1, 1),
(86, 'devops-ad-absurdum', 1600729200, 'DevOps Ad Absurdum', '<p>DevOps is about communication and responsibility. A great product is the result of great cooperation.</p>', '<p>In my <a href=\"/you-are-not-gonna-need-microservices\" target=\"_blank\">previous post</a> I briefly mentioned the importance of a good DevOps culture in an organization as an enabler to build state-of-the-art software, especially in the cloud environment.</p>\r\n\r\n<p>The role of DevOps is crucial in modern software development and, at the same time, often misunderstood even by professionals. Creatively, new meanings were assigned to the term recently, and things got worse once again.</p>\r\n\r\n<p>Is DevOps completely profaned or is there still a chance? First, let’s summarize what I mean when talking about DevOps.</p>\r\n\r\n<h2>What Is DevOps</h2>\r\n\r\n<p>DevOps is a <strong>mindset</strong>, a <strong>way of working</strong>, not a job title or a department. That is, DevOps is not the guy who\'s taking care of your server or pipeline. In a true DevOps model, <strong>everybody is a DevOps engineer</strong>. Although the name is a bit misleading, not only developers (Dev) and system administrators (Ops) are involved. <strong>All team members are included</strong>: developers, operations, testers, QA, support, infosec, product owner, etc.</p>\r\n\r\n<p>The team as a whole is encouraged to make decisions and overtake <strong>responsibility</strong>. Really, there are no such titles as \"Principal Senior Lead Architect\" in an organization practicing DevOps. All in all, even the most inexperienced developer can sometimes bring the best ideas, can\'t she?</p>\r\n\r\n<blockquote class=\"quote\">An individual who makes a decision becomes more invested in its outcome. ~ S. McChrystal</blockquote>\r\n\r\n<p>DevOps is about <strong>communication</strong>. It attempts to avoid silos inside the organization where people with different technical responsibilities can\'t effectively communicate with each other. Conway\'s law is worth mentioning here.</p>\r\n\r\n<h2>What Is DevOps Not</h2>\r\n\r\n<p>Unfortunately, DevOps quickly became a buzzword. I\'ve seen the true meaning of DevOps being ignored not only among recruiters and managers, but sadly, often also among developers. A typical misconception is to consider DevOps as a job title, to simply put an equal sign between DevOps and sysadmin. At the end of the day, there is no improvement whatsoever, just different job titles. The walls remain.</p>\r\n\r\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/devops-not.png\" alt=\"DevOps before and after\" /><br /><i class=\"caption\">DevOps before and after</i></p>\r\n\r\n<p>As if that wasn\'t enough, lately I\'ve been seeing another phenomenon that attaches a completely new meaning to DevOps. Sometimes, it\'s close to the <em>10x engineer</em>, a mystical person who knows practically everything about everything.</p>\r\n\r\n<h2>DevOps, Full-Stack and 10x Engineer</h2>\r\n\r\n<p>About 10 years ago, I was working as a <em>full-stack developer</em> without even noticing it - the term was simply not established yet. The technology stack was not as rich as it is today. For a lot of things there was just one (right) way to go for and it was still possible, though not easy, to master the entire stack in a particular field (such as web applications).</p>\r\n\r\n<p>Nowadays there are tens to hundreds of options for every little piece of the development. Technologies have been involved a lot and it\'s really difficult just to keep track of them all. In each category there are plenty of libraries, frameworks and programming models to choose from. You may stay aware but having hands-on would probably require skipping sleep whatsoever. Consider just programming languages for backend development: Java, Kotlin, Scala, Clojure, Ruby, C#, F#, Go, Python, Rust, PHP, JavaScript, TypeScript, Erlang, Crystal or C++; frameworks for frontend development: React, Angular, Vue, Svelte, Preact, Ember, Web Components; databases: PostgreSQL, MySQL, MariaDB, Oracle, MongoDB, Elasticsearch, DynamDB, Cosmos DB, Redis, Couchbase, Cassandra, Scylla, Neo4j, ArangoDB, and so on and so forth.</p>\r\n\r\n<p>Can one have practical experience and deep knowledge of all of them? I don\'t really think so.</p>\r\n\r\n<p>Even as \"just\" a frontend developer you can\'t probably excel in all the frameworks and JavaScript libraries available on the market, so how could you as a backend developer? Unless you\'re a 10x engineer, which you are not, because they don\'t exist. However, recruiters and managers still may have all those required skills on their wishlists.</p>\r\n\r\n<p>One way out is to understand a full-stack developer as a developer with <strong>great skills specific to one field and some knowledge of the others</strong>. For example, a backend Java developer who can fix a bug in a React based application, when necessary.</p>\r\n\r\n<p>Similar with DevOps. In the era of cloud computing it might be tempting to let the developers do all the operations work in addition. From setting up a build pipeline to monitor the running system in production. A lot of articles even advise doing so. While it\'s definitely worth it for developers to have some level of awareness about the deployment process and operations, the field is just too big to be fully mastered by developers themselves. With <em>Infrastructure as code</em> setup, a developer can (and should) be involved in designing and programming infrastructure, however, she rarely possesses time and required skill to operate the system completely.</p>\r\n\r\n<p>Operations is a full-time job that requires skilled and experienced professionals in many topics such as system administration, operating systems, hardware provisioning, networking, infosec, monitoring, identity and access management, tools, etc.</p>\r\n\r\n<p><strong>It\'s absurd to think that someone can possibly master all</strong> those together with all required developer skills. Unless you\'re a 10x engineer, which you are not, because they don\'t exist! Unfortunately, this is exactly what the common understanding of a DevOps engineer nowadays is.</p>\r\n\r\n<p>Such a situation, typical for startups with early and wild adopting new trends often without actually understanding them, leads to two possible results: 1. system is poorly operated, 2. one or more developers become full-time system administrators. The latter doesn\'t have to be necessarily a bad thing, if you have enough developers (usually not the case), but can easily result in a lack of people in development and consequently poor quality of the actual software solution.</p>\r\n\r\n<p>One exception could be a small product with a little operating overheads, typical for early phases of new products. As the product evolves, it will be necessary to review this strategy.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p><strong>DevOps is about communication and responsibility</strong> for a product as a whole. It\'s not about a new kind of specialization or almighty superpowers. 10x engineer is a myth and you shouldn\'t waste time looking for her. Instead, aim to have excellent developers, excellent sysadmins, excellent testers, etc. <strong>Remove barriers</strong> between them. <strong>Trust people</strong> to do the best work they can, give them the power to do so, bring them <strong>together</strong> and <strong>build a great team</strong> with a <strong>common goal</strong> - a product that delights users.</p>\r\n\r\n<p>People always tend to excel in one thing while being unhandy in others. This is perfectly natural. Accept that and prosper from it. Different skills are required for successful product development. <strong>A great product is the result of great cooperation.</strong></p>\r\n\r\n<p>All-in-one DevOps unicorns don\'t really exist.</p>', 'DevOps,Team,Agile', 'false', 'false', 1, 2),
(87, 'developer-on-a-job-interview', 1606086000, 'Developer on a Job Interview', '<p>What really matters when looking for a new job as a software developer.</p>', '<p>In the last few months, I was looking for a new job as a software developer.</p>\r\n\r\n<p>I attended various interviews, talked to a lot of diverse people, had to solve different tasks, answered plenty of questions, and asked at least as many.</p>\r\n\r\n<p>In this post, I will share with you what I have learned, and tell you some useful tips for doing well at your interview to get the job of your dreams.</p>\r\n\r\n<p>I won’t give you an exhaustive list of technical questions, brainteasers and riddles. Instead, I’ll tell you how to make the most sense out of them.</p>\r\n\r\n<a name=\"software-today\" href=\"#software-today\" class=\"anchor\"></a>\r\n<h2>\r\n  Software Today\r\n</h2>\r\n\r\n<p>Software development nowadays is <strong>less about technology and more about people</strong>. Good communication skills, knowledge sharing, and team spirit are essential qualities of software professionals in modern organizations. Software becomes more and more a people problem.</p>\r\n\r\n<p>Good code is driven by simple, easy-to-understand design with human factors in mind. Perfectly <strong>working code is no more enough</strong>, as far as it is incomprehensible to other teammates. Great programming skills are not sufficient as far as you’re unable to <strong>communicate your work</strong> clearly.</p>\r\n\r\n<p>This fact has been slowly understood by software companies and the interview process has been adapted accordingly. That is, a flawlessly coded quick-sort algorithm is no guarantee for getting a job offer anymore, and it is actually good.</p>\r\n\r\n<p>You spend a great piece of life at work. Not only is it no fun to spend this time with asocial, uncommunicative, or even rude, geeks; it is also pretty unproductive at the same time. Software development became too extensive to fit into a single head entirely, and there are <a href=\"/devops-ad-absurdum\">no unicorns</a>. <strong>Teamwork is a fundamental prerequisite for a successful (software) product</strong>. From experience, just one unsuitable person on a team could cause immeasurable damage to the outcome. I bet you know exactly what I\'m talking about.</p>\r\n\r\n<p>The extra effort at the interview to ensure you’re a good match for the team means the team is a good fit for you, too.</p>\r\n\r\n<p>Soft skills became an important part of your professional profile. Some of them are even more significant than the hard skills you put first in your resume. In fact, <strong>any technology can be learned, but it’s really difficult to unlearn being a jerk</strong>. Deep knowledge of a particular programming language is not as valuable if you can’t use your potential as a team member to create a great product together. <strong>Crucial is the ability and will to learn, communicate, and exchange knowledge</strong> with others.</p>\r\n\r\n<p>Reputable organizations today are not looking for lone wolves or cowboy developers. Rather, team players and excellent communicators are appreciated.</p>\r\n\r\n<a name=\"tell-me-about-yourself\" href=\"#tell-me-about-yourself\" class=\"anchor\"></a>\r\n<h2>\r\n  Tell me about yourself\r\n</h2>\r\n\r\n<p>A few honest sentences about yourself, your background, <strong>personal values</strong>, and what you are looking for, a nice photo, hobbies and interests are more valuable than an overwhelming list of technologies and programming languages you touched since your studies.</p>\r\n\r\n<p>Of course, hard skills are still very important, but your professional profile should tell your potential employer <em>who</em> you are. Forget things like UML or XML, no one is interested, strike off all exotic languages you used to be programming in as a kid, you barely remember them, focus on what’s important and <strong>relevant to the position</strong> you currently apply for.</p>\r\n\r\n<p>Structure your profile hierarchically, start with concepts and then go deeper. Have you designed distributed systems or developed machine learning algorithms? Start with telling that. Continue by diving into concrete technologies, languages, methods. Mention them in the list of working experiences, too. <strong>Be consistent and integrous</strong>. Focus, structure, remove fluff.</p>\r\n\r\n<p>Your professional profile is a sample of your work. Being done poorly tells your potential employer you’re probably no good worker. Your professional profile must be professional.</p>\r\n\r\n<p>Sometimes an obligatory question comes up:</p>\r\n\r\n<p><em>Characterize yourself using only three words.</em></p>\r\n\r\n<p>Albeit such a question would fit more to an interview for a job in the marketing department, it’s a good idea to be able to characterize yourself with a few significant properties or values. <em>Open, creative and focusing</em> could be a good try. You can explain those in detail afterward, ideally with some practical examples.</p>\r\n\r\n<p>I personally characterize myself with the help of three values I identify with: <em>people, quality and learning</em>. People stands for teamwork, knowledge exchange, trust, and intensive transparent communication not only within the team, but also between teams; quality stands for simplicity, maintainability, product-thinking, and focus on purpose; and finally learning stands for continuous improvement, and exploring new things.</p>\r\n\r\n<p>Also, it might be nice to have a life motto that drives your personal characteristics on a very abstract level. Like this one by Albert Einstein: <em>“Everything should be made as simple as possible, but no simpler.”</em></p>\r\n\r\n<a name=\"personal-integrity\" href=\"#personal-integrity\" class=\"anchor\"></a>\r\n<h2>\r\n  Personal integrity\r\n</h2>\r\n\r\n<p>It’s easy to make things up. There’s mostly no way how the interviewers could possibly check if all that you say is true. Well, there is a way. We can call it an <em>integrity check</em>.</p>\r\n\r\n<p>First, <strong>there’s no point in lying</strong>. Don’t do that, I mean it. If you make a different person from yourself, that person would be expected to remain as such afterward. But one can’t lie forever. Soon or later, the truth comes up and the consequences are drastic. Losing the job and all the opportunities that you had to cancel due to this one and that could’ve matched your dreams much better. Broken trust, shame, and a black hole in your resume. Definitely not worth it. What’s more, telling the truth is much easier as you don’t have to memorize anything.</p>\r\n\r\n<p>Even if everything you say is true, you have to be ready to prove it. You have to watch the integrity of your words. For each and every statement there must be a concrete example from your experience.</p>\r\n\r\n<p><em>You say, you’re creative? Tell me about your best ideas in your previous work. You say, you’re a leader? Tell me a story where you take over a control. Are you good at conflicts resolving? Tell me about a conflict in your team and how you managed to resolve it. You like learning new things? Tell me about a piece of technology you read about recently.</em></p>\r\n\r\n<p>Questions about resolving conflicts and disagreements in the team or on the organization level are very popular among interviewers. They help them to test your integrity as a team player. A true <strong>team player welcomes diversity and disagreements</strong>. Different opinions are opportunities for exchange, to learn something new, or to refine your own thoughts. Conflicts should never become purely emotional and must be resolved rationally through discourse with the whole team.</p>\r\n\r\n<p>As the ability and will to learn is one of the most important characteristics of a good developer, you may expect a lot of questions touching on this aspect:</p>\r\n\r\n<p><em>How do you learn new things? What was the last lecture you attended and what did you learn? How would you explain a particular technology to a younger coworker?</em></p>\r\n\r\n<p>Another interesting sort of questions focuses on legacy software. While, as the old joke says, a new JavaScript framework is born every day, a successful software product could live over ten years. This means most stable <strong>companies deal with legacy systems of some kind</strong>. A question about how you get on with this is definitely about to come.</p>\r\n\r\n<p>While it is good to be keen on shiny new stuff, it’s necessary not to forget about the purpose. You should not adopt a new technology just because of the technology itself, but because <strong>it solves a problem you actually have</strong>. <a href=\"/you-are-not-gonna-need-microservices\">Microservices</a> or machine learning are cool, but not a silver bullet. You should aim to use good and <strong>appropriate technology that fits your context</strong>, and communicate it so towards your interviewers.</p>\r\n\r\n<p><strong>Be honest with your weaknesses</strong> as well. Don’t be afraid to admit that you don’t know everything. It’s perfectly fine to be aware of room for improvements and way better than being caught lying. Much more important is to show the will to learn and to get continuously better.</p>\r\n\r\n<a name=\"what-how-and-why\" href=\"#what-how-and-why\" class=\"anchor\"></a>\r\n<h2>\r\n  What, how and why\r\n</h2>\r\n\r\n<p><em>What</em> and <em>how</em> questions are familiar to every developer. There are plenty of interview questions over the Internet everyone can just print and learn at bedtime as a poem:</p>\r\n\r\n<p><em>How does merge-sort work? What transaction isolation levels do you know? How to check two strings for equality? What does <a href=\"/solid-principles-in-java-by-example\">SOLID</a> mean?</em></p>\r\n\r\n<p>I bet you have already been asked all of those questions and you definitely know the correct answers by heart. As this is good, it’s not enough.</p>\r\n\r\n<p>Good companies will test your <strong>understanding and solving skills rather than encyclopedic knowledge</strong> of definitions and algorithms. That is, you should also expect why questions, such as:</p>\r\n\r\n<p><em>Why is bubble sort ineffective and why is it not possible to fix it? In which cases could bubble-sort do make sense? Why is the Repeatable reads isolation level not sufficient to avoid phantom reads? Why do we not use the Serializable level everywhere? Why is the == operator not adequate for checking strings equality in Java? Why does the Dependency inversion principle lead to loosely decoupled code?</em></p>\r\n\r\n<p>Knowing what and how is necessary, but a good engineer must know why, too.</p>\r\n\r\n<a name=\"analyze-be-creative\" href=\"#analyze-be-creative\" class=\"anchor\"></a>\r\n<h2>\r\n  Analyze, be creative\r\n</h2>\r\n\r\n<p>A lot of questions are not meant to be answered directly or even correctly. Many of them have an <strong>open ending or no solution</strong> at all. Interviewers are interested more in your thought process and solving skills than in a precise answer.</p>\r\n\r\n<p>There is no shame in taking some time to think. Nobody expects you to come up with a brilliant solution instantly. Take a deep breath, <strong>analyze the question, be creative, and share your thoughts</strong>.</p>\r\n\r\n<p><em>What profession would you do if you were not a developer? What is your favorite phone app? What color would you be?</em></p>\r\n\r\n<p>There is no correct answer to the above questions. They all test your creativity and ability to think out of the box. Analyze the hypothetical situation first:</p>\r\n\r\n<p>Maybe you always wanted to be a musician and now you have an opportunity to sell your dream. But if you always wanted to be a programmer it’s unlikely that you have a list of alternative professions in mind. Why would you be someone else? Maybe because there is no such job whatsoever! So, what would you likely be in medieval years? Or in a parallel universe where physical laws are not constant and humans communicate via music. Show that <strong>you’re not boring</strong>. Nobody wants to work with a tedious person.</p>\r\n\r\n<p>Some questions are put unclearly on purpose and <strong>interviewers expect you to ask additional questions</strong> to make the problem clear. Take the interviewers into account as equal partners, try to work it out together, collaborate:</p>\r\n\r\n<p>What does “favorite” actually mean? Most beloved or mostly used? What would be the app I would keep if I could keep only one? You can talk about the newest game you had really fun playing, or be practical saying that your favorite app is the internet browser because you can do practically everything with it.</p>\r\n\r\n<p>There is no need to study psychology and emotional values hidden under different colors. Your interviewers did certainly not. If you want, say you would like to be blue as it is your favorite color remaining you of the sky. Or maybe yellow like the dress of your child on its first school day. You may also say that you’d like to be white because white is used only little and by only true artists, which means you would participate in as many great paintings as possible. Options are infinite.</p>\r\n\r\n<a name=\"youre-supposed-to-be-happy-too\" href=\"#youre-supposed-to-be-happy-too\" class=\"anchor\"></a>\r\n<h2>\r\n  You’re supposed to be happy too\r\n</h2>\r\n\r\n<p>The task of an interviewer is to find out if you’re a good fit for the team and the organization she represents. Your task is to figure out <strong>if the team and the organization is a good fit for you</strong>.</p>\r\n\r\n<p>This aspect of the interview is often underrated which can easily lead to disappointment, or even anxiety and burnout. <strong>You spend at work way too much time to be unhappy there</strong>. If nothing else, please, remember this.</p>\r\n\r\n<p>Make sure that the job fits your expectations and there is consistency in what the interviewer offers. <strong>Ask questions</strong>. A lot of them. <strong>Show your interest</strong> in the organization, the team, and the product. Ask about company values and check if they match yours. Ask about challenges and how they solve them. Check answers for openness and consistency. I used to ask this very same question to every interviewer:</p>\r\n\r\n<p><em>What is the worst thing about working for the company?</em></p>\r\n\r\n<p>Some companies would promise the earth but the reality turns out to be very different. Do they have concrete examples to share with you? How does the organization support your ambitions and needs? Do they offer lectures and training? Will you have time to experiment and try new things out?</p>\r\n\r\n<p>Interviewers often over-use buzzwords they barely understand. Try to address this as well:</p>\r\n\r\n<p><em>What do you understand under Agile? What does DevOps mean to you? How do you do Scrum?</em></p>\r\n\r\n<p>It’s always a good idea to ask about your potential role and organizational structure. An unclear and inconsistent understanding of the role responsibilities, high and opaque hierarchy, or cumbersome processes are signs of a dysfunctional organization.</p>\r\n\r\n<p>To show interest brings points for you as well.</p>\r\n\r\n<a name=\"how-to-do-your-homework\" href=\"#how-to-do-your-homework\" class=\"anchor\"></a>\r\n<h2>\r\n  How to do your homework\r\n</h2>\r\n\r\n<p>Last but not least is a technical task, usually given as homework with limited time in the first phases of the interview process.</p>\r\n\r\n<p>This kind of examination became very popular the last time and replaced typical multiple-choice tests almost completely.</p>\r\n\r\n<p>The point is not only to see your skills in action, it’s also a good check of your thinking in an unusual situation.</p>\r\n\r\n<p>What is so different about such homework? The conditions are pretty unnatural. It’s no real problem you have to solve. You’re missing context and typically a lot of other information. You might ask additional questions, but <strong>you’re also supposed to make assumptions</strong> by yourself.</p>\r\n\r\n<p>There are several rules I’d recommend:</p>\r\n\r\n<p>First, if you’ve got some code to extend, study it. <strong>Never do any changes</strong>, except you have found a bug or been told otherwise.</p>\r\n\r\n<p>Don’t over-engineer. This is time to shine, but it doesn’t mean you should use every piece of technology you know. Rather, make your solution extensible, so it’s open for further improvements. You don’t have to use each design pattern from the GoF catalog, it’s much better to use only one, but properly. <strong>Use best practices, but keep it simple</strong>. Think in terms of MVP (Minimal Viable Product): What’s the minimal solution it could possibly work?</p>\r\n\r\n<p>Always <strong>write tests</strong>. Not only tests increase your assurance that the code is doing well, but they will also make you more productive and help you get things done quickly. Every change, every requirement and condition must be found in the test suite.</p>\r\n\r\n<p><strong>Use static code analysis tools</strong>. In this case, the tool that comes along with your IDE will be sufficient. This will be probably the first thing your reviewers will do. Don’t let them find any code smells. It’s easy money.</p>\r\n\r\n<p>Consider options, make notes. Your solution will be discussed later, so it’s important to know why you did as you did. There could be more than one right solution, but it’s important to explain why you have chosen this over that. Everything is about trade-offs, you should <strong>understand the benefits</strong> of your solution and <strong>be aware of its drawbacks</strong>.</p>\r\n\r\n<a name=\"summary\" href=\"#summary\" class=\"anchor\"></a>\r\n<h2>\r\n  Summary\r\n</h2>\r\n\r\n<p>Let’s summarize the most important points to be aware of when preparing for an interview as a software developer:</p>\r\n\r\n<ul>\r\n<li>Software today is less about technology and more about people.</li>\r\n<li>Working code is no more enough, you have to communicate your work.</li>\r\n<li>Teamwork is a fundamental prerequisite for success.</li>\r\n<li>Ability and will to learn is crucial.</li>\r\n<li>Your personal profile is at least as important as your hard skills.</li>\r\n<li>Be honest, consistent and integrous. Don’t lie.</li>\r\n<li>A good engineer must know why and understand trade-offs.</li>\r\n<li>Analyze, be creative, and share your thoughts.</li>\r\n<li>Ask questions, show interest.</li>\r\n<li>When working on a task, use best practices, but keep it simple.</li>\r\n<li>The job must fit you.</li>\r\n</ul>\r\n\r\n<p>I wish you good luck with the interview, getting your dream job, and having a lot of fun developing software!</p>\r\n', 'Career,Team,Agile', 'false', 'true', 1, 2);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(88, 'keep-options-open', 1608996000, 'Keep Options Open', '<p>Keeping options open is one of the most useful principles of Clean Architecture. Let’s dive deeper with code examples in Java and Spring framework.</p>', '<p>I bet you’ve read Robert Martin’s Clean Architecture. Sure you have. And if you haven’t, you definitely should. It’s a gentle book about basic architecture principles and best practices, every developer must know by heart.</p>\r\n\r\n<p>Some chapters might be a bit outdated and beautifully impractical, but the book does give you a nice overview of how to build a good software architecture, in its purest ideal.</p>\r\n\r\n<h2>Software architecture</h2>\r\n\r\n<p>Software architecture is about trade-offs. It\'s up to you to find a balance between all pros and cons that meets your current situation and the context. Needless to say, the tension will change as the product changes and grows over time. A good architecture minimizes the effort to make the changes happen. <strong>A good architecture can evolve</strong>.</p>\r\n\r\n<p>In this post, I will leave all the <a href=\"/solid-principles-in-java-by-example\">SOLID</a> stuff, stability metrics, and dependency rules aside. Rather, I focus on one aspect only: <em>keep options open</em>.</p>\r\n\r\n<p>Keeping options open is an important architectural rule that deserves our utmost attention. We will discuss a few good practices and put them into context with some code examples.</p>\r\n\r\n<h2>Defer hard decisions</h2>\r\n\r\n<p>It seems that a software architect must foresee the evolution of the software product to sketch a good architecture up-front. But is it even possible? Can engineers see the future?</p>\r\n\r\n<p>The world of business is a wild place that changes rapidly in unexpected ways. The requirements on architecture change as the business changes. That’s why every attempt to design the whole system from the beginning fails (remember waterfall). </p>\r\n\r\n<p>Of course, some up-front design must be done, but it is not much more than intelligent guessing.</p>\r\n\r\n<p>The Agile movement teaches us to react to frequent feedback and do small incremental steps that are easy to revert. <strong>Software architecture is an ongoing activity</strong> that must be continuously checked whether it works as expected.</p>\r\n\r\n<p>Making decisions immediately would be counterproductive, as the necessary information is not available yet. Thus, it is wise to <strong>defer the decisions until the last possible moment</strong>.</p>\r\n\r\n<p>Of course, it is not possible to defer all decisions. The architecture would become very expensive. It’s the job of the software architect to find a sweet spot in this tense. Details that have nothing to do with the business could and should be deferred because a good software architecture doesn’t depend on them.</p>\r\n\r\n<p>Once again, software architecture is about trade-offs. The <em>You aren’t gonna need it</em> principle is to keep in mind.</p>\r\n\r\n<p class=\"note\"><span class=\"title\">Note:</span> I’m not talking about the “software architect” as about a job title. A software architect is a role that should be distributed in the team, based on the current setup. Every developer should be a software architect at some level.</p>\r\n\r\n<p>Leaving the options open will give us time to retrieve all the necessary information to make the best possible decision when it’s really needed. This approach will result in a flexible and clean architecture, open to extension, and easy to maintain.</p>\r\n\r\n<p>One useful technique, mentioned in the Clean Architecture book, is to do all the work and then to skip the last step.</p>\r\n\r\n<h2>Skip the last step</h2>\r\n\r\n<p>Making architecture clean requires explicit defining of the boundaries between modules. Boundaries are the architectural decision that is hard to make because the boundaries often change as you gain more insight into the product.</p>\r\n\r\n<p>Fixing boundaries right in the beginning will make it expensive to change later. Therefore, it is a good idea to sketch lines without fixing them until it’s necessary.</p>\r\n\r\n<p>That’s why I don’t recommend the microservices-first approach. Microservices make boundaries fixed, as the code is usually completely physically separated (in different repositories). Such a separation is hard to refactor later, which often leads to keeping wrong boundaries on the place. And, voilà, a <a href=\"/good-and-bad-monolith\">distributed monolith</a> was born!</p>\r\n\r\n<p>Clean Architecture advises to do all work of separation of boundaries and then leave them live together in a single modular monolithic application, in a single-repository codebase.</p>\r\n\r\n<p>This approach gives us good boundaries for a truly clean architecture, but it is still easy to refactor (with a little help of your IDE).</p>\r\n\r\n<p>Now, we have a single-process program with clean boundaries and modular codebase, but no administration costs that are typical for <a href=\"/you-are-not-gonna-need-microservices\">distributed systems</a>. The best of both worlds!</p>\r\n\r\n<p>When the physical separation is needed, there is just the last step to be done.</p>\r\n\r\n<h2>Example</h2>\r\n\r\n<p>As an example, consider an eCommerce system with several services: <em>Sales</em>, <em>Billing</em>, <em>Shipping</em>, <em>Warehouse</em>, etc.</p>\r\n\r\n<p>In the beginning, the business is small (hundreds of users a day). There is no need to set up any complicated infrastructure. The priority is to bring the product to its customers.</p>\r\n\r\n<p>A monolith is your best bet for the start.</p>\r\n\r\n<p>As the business grows and the number of users increases to hundreds of thousands, we can think of separating the most critical services into individual processes.</p>\r\n\r\n<p>Microservices or a hybrid approach will solve your scalability issues.</p>\r\n\r\n<h3>Boundaries</h3>\r\n\r\n<p>In Java, packages are the most simple mechanism we can implement our boundaries with.</p>\r\n\r\n<pre>\r\norg.ecommerce \r\n  sales\r\n  billing\r\n  shipping\r\n  warehouse\r\n  …\r\n</pre>\r\n\r\n<p>A step further would be to build services as separate artifacts (Maven modules or Gradle sub-projects). Doing so, we make our dependencies explicit. This prevents developers from accidentally crossing boundaries. That is, to cross a boundary, an explicit dependency to that service artifact must be included in the module. This will make a developer think if she does the right thing or not.</p>\r\n\r\n<pre>\r\necommerce/\r\n  sales/\r\n    src/main/java/\r\n      org.ecommerce.sales\r\n      …\r\n  billing/\r\n    src/main/java/\r\n      org.ecommerce.billing\r\n      …\r\n  shipping/\r\n    src/main/java/\r\n      org.ecommerce.shipping\r\n      …\r\n  warehouse/\r\n    src/main/java/\r\n      org.ecommerce.warehouse\r\n      … \r\n  …\r\n</pre>\r\n\r\n<p>Now, how difficult could it be to move one module to a separate repository? Not at all!</p>\r\n\r\n<h3>Linking together</h3>\r\n\r\n<p>A service should be autonomous. We can use Spring Boot to implement every service as a self-contained <a href=\"/spring-boot-custom-components\">custom component</a>.</p>\r\n\r\n<p>The application (<em>Main</em> component) will assemble all services simply by referencing them as dependencies:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\ndependencies {\r\n  implementation project(\':sales-spring-boot-starter\')  \r\n  implementation project(\':billing-spring-boot-starter\')  \r\n  implementation project(\':shipping-spring-boot-starter\')  \r\n  implementation project(\':warehouse-spring-boot-starter\')  \r\n  … \r\n}\r\n</pre>\r\n\r\n<p>When adopting microservices, an application module must be created per service. No change is required in the services code. The Open-Closed Principle in action.</p>\r\n\r\n<p>When things change later, the system can be deployed as a monolith again.</p>\r\n\r\n<h3>Communication</h3>\r\n\r\n<p>Components in a monolithic application typically use inter-process communication. It is quick, cheap, and as robust as the application itself. The simplest form is just an ordinary method call.</p>\r\n\r\n<p>This approach has several disadvantages: It’s blocking and couples the caller directly to the callie.</p>\r\n\r\n<p>The idea of messaging addresses this issue.</p>\r\n\r\n<p>But, do we need to spin up a message broker to do messaging in a single-process application? No, even sending messages via a network is usually unnecessary.</p>\r\n\r\n<p>Spring framework offers a neat feature: Application Events. Messaging in Spring is a capability provided by the Application Context and doesn’t need any networking or other special setup. We can easily use it for communication across service boundaries.</p>\r\n\r\n<p>Now, the question is, how to make this solution open for communication between services? Well, let’s introduce an interface:</p>\r\n\r\n<pre class=\"brush: java\">\r\npublic interface Publisher {\r\n\r\n  void publish(Object msg);\r\n}\r\n</pre>\r\n\r\n<p>For our monolithic application we simply provide an implementation based on Spring Application Events:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\nclass MessagingConfig {\r\n\r\n  @Bean\r\n  Publisher publisher(\r\n      ApplicationEventPublisher pub) {\r\n    return pub::publishEvent;\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>For microservices, we create a different configuration based on an external message broker (such as RabbitMQ):</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Configuration\r\nclass RabbitMqConfig {\r\n\r\n  @Primary\r\n  @Bean\r\n  Publisher rabbitPublisher(\r\n      RabbitTemplate rabbit) {\r\n    return m -> rabbit.convertAndSend(\r\n        TOPIC, ROUTING, m);\r\n  }\r\n  …\r\n}\r\n</pre>\r\n\r\n<p>Using the <code>@Primary</code> annotation we overwrite the default configuration. It’s a good practice to use Spring profiles to control the setup of the components:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Profile(\"rabbitmq\")\r\n@Configuration\r\nclass RabbitMqConfig {\r\n  …\r\n}\r\n</pre>\r\n\r\n<p>With a setup like that, we can run the monolithic application with the default Spring profile:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\ngradle :application:bootRun\r\n</pre>\r\n\r\n<p>Or run every service independently so they communicate remotely over a RabbitMQ message broker:</p>\r\n\r\n<pre class=\"brush: groovy\">\r\ngradle :sales:application:bootRun \\\r\n  --args=\'--spring.profiles.active=rabbitmq\'\r\n</pre>\r\n\r\n<p>The source code could be found on <a href=\"/ddd-example-ecommerce-microservices\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<h2>Conclusion</h2>\r\n\r\n<p>Don’t decide about details that don’t matter right now. Delaying such decisions will give you the necessary information needed to make <em>good</em> decisions.</p>\r\n\r\n<p>As Robert Martin states:</p>\r\n\r\n<blockquote class=\"quote\">\r\n	A good architect maximizes the number of decisions not made.\r\n</blockquote>\r\n\r\n<p>Like any other architectural rule, keeping options open mustn\'t be taken dogmatically. Time, budget, team size, and skills should be taken into account. Otherwise, the architecture could easily become too expensive.</p>\r\n\r\n<p class=\"center\"><img src=\"https://raw.githubusercontent.com/ttulka/blog-assets/master/keep-options-open.jpg\" alt=\"Keep your options open\" /><br /><i class=\"caption\">Keep your options open</i></p>\r\n\r\n\r\n', 'Software Architecture,Design Patterns,Deployment', 'false', 'false', 1, 1),
(89, 'documentation-boosts-development', 1608661000, 'Documentation Boosts Development', '<p>How writing documentation can improve implementation and overall quality of software.</p>', '<p>I know that you\'re thinking: writing documentation is boring. One gets goosebumps just thinking about it. In no time, developers come up with excuses: our code is self-documented, documentation is in the code, the code itself is documentation, and similar.</p>\r\n\r\n<p>Well, there is nothing wrong with having clean and self-documented code. The problem is, not everyone has access to and can read code (think of your project manager or people from the QA department).</p>\r\n\r\n<p>In fact, there are several kinds of documentation. The focus differs in various types of audience and levels of detail.</p>\r\n\r\n<p>I won’t give you an exhaustive manual for writing technical documentation here. First, I’ll start with a few important bullet points good documentation should meet. Then, I’ll tell you how documentation is useful for developers and how it can improve the quality of software.</p>\r\n\r\n<p>I mean it, documentation is neither useless nor a necessary evil. <strong>Documentation is a fully-fledged development activity that improves software quality</strong>. When done regularly, it can be easy and even fun to write.</p>\r\n\r\n<p>The best time to write documentation is while you\'re writing code. You are your very first audience.</p>\r\n\r\n<h2>Kinds of Documentation</h2>\r\n\r\n<p>Code is the lowest-level documentation. Things like good names, focused single-purpose functions, well-structured modules, and domain-oriented abstractions are the main characteristics of clean self-documented code.</p>\r\n\r\n<p>Comments as explicit documentation in code (such as JavaDoc) are extensions to self-documented code. They explain the purpose and usage of code, and <em>why</em> things have been done in this way. This kind of documentation doesn’t require direct access to the code, but it still focuses primarily only on people who can read code (developers).</p>\r\n\r\n<p>APIs are usually documented in a machine-friendly format (such as JSON, or XML). There are useful standards (such as <a href=\"https://www.openapis.org/\" target=\"_blank\">OpenAPI</a>) and tools that can generate human-friendly documents directly from code. The target audience is technical people (developers and architects).</p>\r\n\r\n<p>Tests are another kind of very useful documentation. Tests are the first client code to the APIs. Tests can be read by QA or even non-technicians such as product managers (remember <a href=\"https://en.wikipedia.org/wiki/Behavior-driven_development\" target=\"_blank\">BDD</a>).</p>\r\n\r\n<p>A good practice is to put a README into every module with a separate API. A short description of the purpose, basic usage, and references to further readings must be included at least. Version-control systems like GitHub or BitBucket search for an occurrence of a README file to show it as a show-window. READMEs are a great way to help developers get in quickly.</p>\r\n\r\n<p>Architectural documents are the most abstract form of technical documentation. They provide the big picture, describe important concepts and significant decisions made. Sadly, those documents often don’t reflect the current state or are missing completely which is unfortunate in many aspects. If that is your case, it\'s time to change it.</p>\r\n\r\n<h2>Importance of Documentation</h2>\r\n\r\n<p>In general, absent or sloppy documentation makes onboarding difficult as there is no obvious place to start. Exploring huge codebases randomly without any clue is tedious and inefficient. Indeed, onboarding is a quality check of your software solution including documentation.</p>\r\n\r\n<p>Similar to further development: Undocumented decisions and practices are forgotten, the wheel is invented again and again, and development becomes expensive very quickly.</p>\r\n\r\n<p>Knowledge stored only in the heads of engineers tends to rot and disappear over time. <i>Why did we do things this way? What is this thing doing? How does it do it?</i> The documentation should give you answers. <strong>Documentation helps you understand and remember</strong>.</p>\r\n\r\n<p>It’s not new that articulating a problem helps to understand it. This is what the popular technique <em>Rubber duck debugging</em> is based on. Similar, when thinking about a solution. Have you ever been sure you understood something well but found gaps when trying to explain it to another person (or a duck)? Our brains can implicitly fill in blank spots in understanding but those will come up when being articulated. Some of them might turn out to be important although they seemed subtle and insignificant before.</p>\r\n\r\n<p><strong>You can’t write documentation if you don’t fully understand the solution</strong>. This is a powerful realization, indeed. Many times, I have found a flaw in the implementation simply by documenting it.</p>\r\n\r\n<p>Not only bugs can come out. If documentation turns out to be too complicated the solution is likely troublesome as well. Documentation can show up that API is not user-friendly, configuration too cumbersome, setup too tangled. <strong>Documentation provides valuable feedback</strong> that drives development towards better quality.</p>\r\n\r\n<p>Documentation is a great input for a discussion and a resource for collaboration. Having some documentation, for example in the form of diagrams, would be extremely useful before the first line of code is written. A design flaw found early saves days or even weeks of programming later. Well documented decisions help speed up the actual implementation.</p>\r\n\r\n<p>Last but not least, documentation is the key to efficient reviewing. Documentation as part of the pull-request helps the reviewer get an insight into the solution That can shorten the review time significantly. Moreover, documentation is probably the first thing an external consultant will ask you for.</p>\r\n\r\n<p><strong>A feature is not done when yet not documented!</strong></p>\r\n\r\n<h2>How to Write Documentation</h2>\r\n\r\n<p>There are several templates and styles for writing technical documentation (such as <a href=\"https://arc42.org\" target=\"_blank\">arc42</a> and <a href=\"https://c4model.com\" target=\"_blank\">C4</a>). Choose what fits you best. Important is to have your audience in focus. Write to help, be minimalistic and clear, show use-cases and examples.</p>\r\n\r\n<p>Doesn’t matter how good the documentation is when no one can find it. Create a centralized access point, but keep documentation close to the code. For example, a directory doc in the root of every repository works well for me.</p>\r\n\r\n<p>We can go even further and treat all documentation as code. This enables handy possibilities such as generating documents in various formats, tracking changes in a version-control system, etc. <a href=\"https://asciidoc.org\" target=\"_blank\">AsciiDoc</a> and <a href=\"https://plantuml.com\" target=\"_blank\">PlantUML</a> are amazing tools for writing documentation source code.</p>\r\n\r\n<p>Manually written documentation should find a sweet spot of how deep into details it should go. Too abstract and broad documentation is not very useful, while too detailed documentation is painful to maintain and often left outdated.</p>\r\n\r\n<p><strong>Outdated documentation is worse than no documentation</strong> as it is a source of confusion and no actual help. Thus, keep your documentation free from any volatile elements and implementation details. Generate as much as possible.</p>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>The role of documentation in software development is often undervalued. However, there is great value documentation can provide when done properly.</p>\r\n\r\n<ul>\r\n  <li>Incorporate documentation in the development process.</li>\r\n  <li>Code is just a part of software documentation.</li>\r\n  <li>Documentation helps by understanding and remembering.</li>\r\n  <li>Documentation makes onboarding easier.</li>\r\n  <li>Documentation provides valuable feedback.</li>\r\n  <li>Documentation helps discover design flaws and implementation bugs.</li>\r\n  <li>Documentation serves as a collaboration resource.</li>\r\n  <li>Documentation is the key to efficient reviewing.</li>\r\n  <li>Bad documentation is worse than no documentation.</li>\r\n</ul>\r\n\r\n<p>Engineers should see documentation as a friend, not a foe. Writing documentation conscientiously is a great way to improve development towards better quality.</p>\r\n\r\n<p>Don\'t leave writing documentation at the last minute. Start now. A bit every day continuously.</p>\r\n\r\n<p>Happy documenting!</p>', 'Documentation', 'false', 'false', 1, 2),
(90, 'learning-webassembly-series', 1609757021, 'Learning WebAssembly Series', '<p>A series of learning texts covering the first steps with WebAssembly for complete beginners.</p>', '<p>Some time ago I started learning WebAssembly as an absolute beginner. It has been an exciting but not so simple journey.</p> \r\n\r\n<p>I decided to publish my continuous and probably neverending notices to make your learning path a little easier.</p>\r\n\r\n<p>Here is the actual list of finished and planned posts. I will constantly update and extend it.</p>\r\n\r\n<dl>\r\n  <dt><a href=\"/learning-webassembly-1-hello-world-of-wasm\">Hello, World of Wasm!</a></dt>\r\n  <dd>Let’s try WebAssembly for the first time. We will create a simple program in the Wat text format, compile it into Wasm binary, and finally execute it in a browser and as a server application.</dd>\r\n  <dt><a href=\"/learning-webassembly-2-wasm-binary-format\">Wasm Binary Format</a></dt>\r\n  <dd>We will explore the structure of the Wasm binary format byte by byte.</dd>\r\n  <dt><a href=\"/learning-webassembly-3-wat-programming-basics\">Wat Programming Basics</a></dt>\r\n  <dd>Finally some real programming! We will learn the very basics of the Wat text format, how to work with functions and variables, write conditions, and loops.</dd>\r\n  <dt><a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\">Wasm Memory and Working with Strings</a></dt>\r\n  <dd>The concept of memory objects is very important know-how in Wasm. We will use it to deal with strings.</dd>\r\n  <dt><a href=\"/learning-webassembly-5-running-wasm-in-the-browser\">Running Wasm in the Browser</a></dt>\r\n  <dd>WebAssembly is part of the web platform, thus it is important to understand interactions between Wasm and JavaScript. We will learn about shared memory and global variables.</dd>\r\n  <dt><a href=\"/learning-webassembly-6-running-wasm-in-nodejs\">Running Wasm in Node.js</a></dt>\r\n  <dd>Browser is not the only environment that Wasm can run in. We will also learn how to execute Wasm in Node.js, the popular backend platform, as well.</dd>\r\n  <dt><a href=\"/learning-webassembly-7-introducing-wasi\">Introducing WASI</a></dt>\r\n  <dd>WebAssembly System Interface (WASI) provides access to several operating-system-like features from Wasm. We will learn how to write programs that use WASI.</dd>\r\n  <dt><a href=\"/learning-webassembly-8-compiling-into-wasm\">Compiling into Wasm</a></dt>\r\n  <dd>There are plenty of languages that can be compiled into Wasm. We will take a look at some of them.</dd>\r\n  <dt><a href=\"/learning-webassembly-9-assemblyscript-basics\">AssemblyScript Basics</a></dt>\r\n  <dd>AssemblyScript syntax is very close to JavaScript and as such is an ideal candidate to write Wasm in. We will explore the basics of AssemblyScript and create a few neat programs.</dd>\r\n  <dt><a href=\"/learning-webassembly-10-image-processing-in-assemblyscript\">Image Processing in AssemblyScript</a></dt>\r\n  <dd>WebAssembly is a great fit for computation-intensive tasks like generating and processing graphics. We will experiment in this field a little.</dd>\r\n</dl>\r\n\r\n<p>You are welcome to join me on the exciting journey of learning WebAssembly!</p>', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(91, 'learning-webassembly-1-hello-world-of-wasm', 1609757120, 'Learning WebAssembly #1: Hello, World of Wasm!', '<p>Getting started with WebAssembly: writing and executing a simple program in Wat.</p>', '<p>This is the first part of a <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a> about programming in WebAssembly (Wasm) for beginners. The goal is to give you the first impression of what Wasm is and how it can be beneficial for your own projects. The further the series goes the deeper we will delve into the topic.</p>\r\n\r\n<p>I suppose you have already heard about WebAssembly; how performant, lightweight, secure, and overall <a href=\"https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/\" target=\"_blank\">promising</a> it is. Otherwise, you probably would not be reading this text right now, would you? So, I will just skip all that fluff and go directly to the more interesting stuff: like how to <em>write</em> Wasm code and how to actually <em>use</em> it.</p>\r\n\r\n<h2>Wasm and Wat</h2>\r\n\r\n<p><em>Wasm</em> is a binary format (files with the <code>.wasm</code> extension). It also has a textual representation called <em>Wat</em> (files with the <code>.wat</code> extension). Wat could easily be generated from the binary format, for example via developer tools in the browser.</p>\r\n\r\n<p>Albeit Wat is a text format readable by humans, it is still pretty low-level — programming complex systems in Wat would quickly become a nightmare. But no worries, Wasm can be compiled from <a href=\"https://github.com/appcypher/awesome-wasm-langs\" target=\"_blank\">many</a> <a href=\"https://webassembly.org/getting-started/developers-guide/\" target=\"_blank\">languages</a> like Rust, C/C++, Go, or Kotlin. Just choose your favourite one!</p>\r\n\r\n<h2>Hello, WebAssembly!</h2>\r\n\r\n<p>To understand the very basics we will start at quite a low-level. Well, not on the binary level, all in all, it was never meant to be written or read by humans. We will create a simple <i>Hello world</i> program in Wat, the WebAssembly text format.</p>\r\n\r\n<p>Copy and save the following code into a file (hello.wat):</p>\r\n\r\n<pre>\r\n(module\r\n  (func (export \"main\") \r\n        (result i32)\r\n    i32.const 42 \r\n    return))\r\n</pre>\r\n\r\n<p>Wait a minute, there is no “Hello” in the code above! This is the first catch: WebAssembly does not have a value <a href=\"https://webassembly.github.io/spec/core/syntax/types.html\" target=\"_blank\">type</a> that would represent <em>strings</em>. We have only <a href=\"https://webassembly.github.io/spec/core/syntax/values.html\" target=\"_blank\">numeric types</a> in our base equipment. For now, 42, <a href=\"https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)\" target=\"_blank\">the answer</a> to the ultimate question of life, the universe, and everything, will serve as our “hello, world” phrase.</p>\r\n\r\n<p>To test the code, we can use the <a href=\"https://github.com/webassembly/wabt\" target=\"_blank\">WebAssembly Binary Toolkit</a>. First, we must compile the Wat text format into the Wasm binary format (hello.wasm):</p>\r\n\r\n<pre>\r\n$ wat2wasm hello.wat -o hello.wasm\r\n</pre>\r\n\r\n<p>We can run it with an interpreter from the same toolkit:</p>\r\n\r\n<pre>\r\n$ wasm-interp --run-all-exports hello.wasm\r\n</pre>\r\n\r\n<p>The following output should appear:</p>\r\n\r\n<pre>\r\nmain() =&gt; i32:<b>42</b>\r\n</pre>\r\n\r\n<p>We can run the same code in the browser, too:</p>\r\n\r\n<pre class=\"brush: html\">\r\n&lt;html&gt;\r\n&lt;body&gt;\r\n&lt;script&gt;\r\n  WebAssembly\r\n    .instantiateStreaming(fetch(&apos;hello.wasm&apos;))\r\n    .then(({instance}) =&gt; \r\n        console.log(instance.exports.main())\r\n    );\r\n&lt;/script&gt;\r\n&lt;/body&gt;\r\n&lt;/html&gt;\r\n</pre>\r\n\r\n<p class=\"warning\">For the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\" target=\"_blank\">Fetch API</a> to work the HTML page must be served via HTTP(s). Tools like <a href=\"https://github.com/ttulka/dir2http\" target=\"_blank\">dir2http</a> do the job.</p>\r\n\r\n<p>After opening the page in a browser we should see the answer in the dev console:</p>\r\n\r\n<pre>\r\n&raquo; 42\r\n</pre>\r\n\r\n<p>As just shown, the identical WebAssembly program can run both inside and outside a browser. This shows the great potential of WebAssembly to become a universal programming platform for both web clients and server applications.\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>All right, we have written our first program in Wasm and now we know the answer to the ultimate question. What\'s next?</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-2-wasm-binary-format\">second part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will go deeper and explain byte by byte what actually happens under the hood.</p>\r\n\r\n<p>Stay tuned!</p>', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(92, 'learning-webassembly-2-wasm-binary-format', 1609757220, 'Learning WebAssembly #2: Wasm Binary Format', '<p>Discovering the basic representation of WebAssembly: binary and text formats.</p>', '<p>In the <a href=\"/learning-webassembly-1-hello-world-of-wasm\" target=\"_blank\">first part</a> of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we created a simple Wasm program and executed it in a browser and as a server application. In this part, we will take a closer look at the generated Wasm binary code.</p>\r\n\r\n<p>WebAssembly (<em>Wasm</em>) is a binary instruction format. <em>Wat</em> is a textual representation of Wasm to be read and edited by humans.</p>\r\n\r\n<p>The following text focuses exclusively on the binary format. If you are only interested in the Wat textual format, feel free to skip directly to the <a href=\"/learning-webassembly-3-wat-programming-basics\">third part</a>.</p>\r\n\r\n<h2>Binary Wasm</h2>\r\n\r\n<p>In the <a href=\"/learning-webassembly-1-hello-world-of-wasm\" target=\"_blank\">first part</a> of this series, we saw the following Wat code:</p>\r\n\r\n<pre>\r\n(module\r\n  (func (export \"main\") \r\n        (result i32)\r\n    i32.const 42 \r\n    return))\r\n</pre>\r\n\r\n<p>Now, we are about to explore the same WebAssembly program in its binary representation.</p>\r\n\r\n<p>After been compiled into the Wasm format, the program bytes read as follows:</p>\r\n\r\n<pre>\r\n00000000: 0061 736d\r\n00000004: 0100 0000\r\n00000008: 0105 0160\r\n0000000c: 0001 7f03\r\n00000010: 0201 0007\r\n00000014: 0801 046d\r\n00000018: 6169 6e00\r\n0000001c: 000a 0701\r\n00000020: 0500 412a\r\n00000024: 0f0b\r\n</pre>\r\n\r\n<p>It is the <a href=\"https://webassembly.github.io/spec/core/binary\" target=\"_blank\">binary format</a> of WebAssembly. We will explore the code byte by byte.</p>\r\n\r\n<p>The first four bytes represent the Wasm binary magic number <code>\\0asm</code>; the next four bytes represent the Wasm binary version in a 32-bit format:</p>\r\n\r\n<pre>\r\n; Wasm magic number (\\0asm) \r\n0000000: 0061 736d\r\n\r\n; version\r\n0000004: 0100 0000\r\n</pre>\r\n\r\n<p>In Wasm, modules are organized into <em>sections</em> (type section, function section, export section, etc.). The first byte of each section represents the section ID (1 for the section “type”) and section size (5 following bytes):</p>\r\n\r\n<pre>\r\n; section \"type\" (ID 1)\r\n0000008: 01\r\n\r\n; section size (5 bytes)\r\n0000009: 05\r\n</pre>\r\n\r\n<p>The <em>type section</em> contains function signatures. Our example has one function with zero parameters and one return result of type <code>i32</code> (32-bit integer):</p>\r\n\r\n<pre>\r\n; number of types (1)\r\n0000000a: 01\r\n\r\n; func\r\n0000000b: 60\r\n\r\n; number of parameters (0)\r\n0000000c: 00\r\n\r\n; number of results (1)\r\n0000000d: 01\r\n\r\n; result type i32\r\n0000000c: 7f\r\n</pre>\r\n\r\n<p>After five bytes (the section size) a new section begins. In our example, ID 3 stands for a <em>function section</em>. The section stores indexes of the function signature:</p>\r\n\r\n<pre>\r\n; section \"function\" (ID 3)\r\n0000000f: 03\r\n\r\n; section size (2 bytes)\r\n00000010: 02\r\n\r\n; number of functions (1)\r\n00000011: 01 \r\n\r\n; index of the function (0)\r\n00000012: 00  \r\n</pre>\r\n\r\n<p>Next, the <em>export section</em> (ID 7) follows. The section defines the export name with the index to our function:</p>\r\n\r\n<pre>\r\n; section \"export\" (ID 7)\r\n0000013: 07\r\n\r\n; section size (8 bytes)\r\n0000014: 08\r\n\r\n; number of exports (1)\r\n0000015: 01\r\n\r\n; length of the export name (4 bytes)\r\n0000016: 04\r\n\r\n; export name (\"main\")\r\n0000017: 6d61 696e\r\n\r\n; export kind (0 for function)\r\n000001b: 00\r\n\r\n; index of the exported function (0)\r\n000001c: 00\r\n</pre>\r\n\r\n<p>The next section is a <em>code section</em> (ID 10) that represents the actual code of the function:</p>\r\n\r\n<pre>\r\n; section \"code\" (ID 10)\r\n000001d: 0a\r\n\r\n; section size (7 bytes)\r\n000001e: 07\r\n\r\n; number of functions (1)\r\n000001f: 01\r\n\r\n; function body size (5 bytes)\r\n0000020: 05\r\n\r\n; number of local declarations (0) \r\n0000021: 00\r\n</pre>\r\n\r\n<p>In our example, a numeric constant <code>42</code> (the answer) is pushed onto the stack and returned as the result:</p>\r\n\r\n<pre>\r\n; instruction i32.const \r\n0000022: 41\r\n\r\n; i32 literal (42)\r\n0000023: 2a\r\n\r\n; return\r\n0000024: 0f\r\n</pre>\r\n\r\n<p>The very last byte is simply the end of the function code:</p>\r\n\r\n<pre>\r\n; end of the function code\r\n0000025: 0b\r\n</pre>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>We have seen that the Wasm binary code is divided into a vector of sections. Our simple function is distributed into four sections: <em>type</em>, <em>function</em>, <em>export</em>, and <em>code</em>.</p>\r\n\r\n<p>The whole program as pseudocode would read as follows:</p>\r\n\r\n<pre>\r\nWasm magic number\r\nversion\r\n\r\nsection \"type\"\r\n  func\r\n  result type: i32\r\n\r\nsection \"function\"\r\n  index of the function: 0\r\n\r\nsection \"export\"\r\n  export name: \"main\"\r\n  export kind: function\r\n  index of the function: 0\r\n\r\nsection \"code\"\r\n  i32.const \r\n  i32 literal: 42\r\n  return\r\n</pre>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>Our short excursion into the binary world of WebAssembly is over. I hope that it was just long enough to make a good impression on you and interesting enough for you not to get sick of the bytes.</p>\r\n\r\n<p>For detailed information you might refer to the <a href=\"https://www.w3.org/TR/wasm-core-1\" target=\"_blank\">WebAssembly Core Specification</a>.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-3-wat-programming-basics\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will delve deeper into the basics of Wat programming.</p>\r\n\r\n<p>Stay tuned!</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(93, 'learning-webassembly-3-wat-programming-basics', 1609757320, 'Learning WebAssembly #3: Wat Programming Basics', '<p>Learning basic building blocks of programming in Wat.</p>', '<p>In the <a href=\"/learning-webassembly-1-hello-world-of-wasm\">first part</a> of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we saw a little <i>Hello world</i> program written in Wat. Albeit it was simple enough, it can be even simpler.</p>\r\n\r\n<p class=\"note\">To compile Wat code into binary Wasm, you can use the <a href=\"https://github.com/webassembly/wabt\" target=\"_blank\">WebAssembly Binary Toolkit</a>: <code>wat2wasm test.wat -o test.wasm</code></p>\r\n\r\n<p>The following code is literally the simplest valid Wat program:</p>\r\n\r\n<pre>\r\n(module)\r\n</pre>\r\n\r\n<p>Nice, but not particularly useful. It does show us, however, one important concept of programming in Wat: <a href=\"https://en.wikipedia.org/wiki/S-expression\" target=\"_blank\">S-expression</a>.</p>\r\n\r\n<h2>S-expressions</h2>\r\n\r\n<p><em>S-expressions</em> (aka <em>symbolic expressions</em>) are a simple textual notation for representing trees. Basically, it is about nesting parenthesized lists. If you have ever seen the syntax of <a href=\"https://en.wikipedia.org/wiki/Lisp_(programming_language)\" target=\"_blank\">Lisp</a> you already know S-expressions.</p>\r\n\r\n<p>The module above is an example of a simple tree written as an S-expression. Well, it is a bit degenerated tree; it is only a standalone root. A slightly better example follows:</p>\r\n\r\n<pre>\r\n(module (memory 1) (func))\r\n</pre>\r\n\r\n<p>Here, we have a simple tree with one root module and two child nodes <code>memory</code> and <code>func</code>. The node <code>memory</code> has one attribute <code>1</code>.</p>\r\n\r\n<p>Every Wat program is practically a big S-expression with the root <code>module</code>.</p>\r\n\r\n<h2>Stack Machine</h2>\r\n\r\n<p>Wasm execution is defined in terms of a <em>stack machine</em>. Each instruction pushes and/or pops a certain value to/from a stack.</p>\r\n\r\n<p>We can demonstrate this idea with our favourite example:</p>\r\n\r\n<pre>\r\n(module\r\n  (func (export \"main\") \r\n        (result i32)\r\n    i32.const 42 \r\n    return))\r\n</pre>\r\n\r\n<p>The code above defines a single function that is exported under the name <code>main</code>. The export name is fully arbitrary, it would work as <code>hello</code> or <code>Answer_to_the_Ultimate_Question</code> as well. The <a href=\"https://webassembly.github.io/spec/core/syntax/types.html#syntax-resulttype\" target=\"_blank\">result type</a> is of 32-bit integer.</p>\r\n\r\n<p>The body of the function has exactly two instructions: <code>i32.const</code> and <code>return</code>. The former pushes the literal <code>42</code> to the stack; the latter pops the value from the stack and returns as the function result.</p>\r\n\r\n<dl class=\"definition\">\r\n  <dt><code>i32.const x</code></dt>\r\n  <dd>Pushes the 32-bit value <code>x</code> to the stack.</dd>\r\n  <dt><code>return</code></dt>\r\n  <dd>Exits the current function and returns the value(s) on the top of the stack.</dd>\r\n</dl>\r\n\r\n<p>We can achieve the same result by nesting S-expressions:</p>\r\n\r\n<pre>\r\n(module\r\n  (func (export \"main\") \r\n        (result i32)\r\n    (return \r\n      (i32.const 42))))\r\n</pre>\r\n\r\n<p>The stack-based programs are, however, much closer to how the program really executes, so, further on, we will stick with the stack-based programming model.</p>\r\n\r\n<p class=\"note\">BTW, the <code>return</code> instruction could be omitted as the function returns the value(s) on the top of the stack automatically.</p>\r\n\r\n<p>A more advanced example would be a function for adding two numbers:</p>\r\n\r\n<pre>\r\n(module\r\n  (func (export \"sum\") \r\n        (param $a i32)\r\n        (param $b i32)\r\n        (result i32)\r\n    local.get $a\r\n    local.get $b\r\n    i32.add\r\n    return))\r\n</pre>\r\n\r\n<p>Here, the exported function <code>sum</code> takes two parameters named <code>$a</code> and <code>$b</code> and returns a 32-bit integer result. Parameters are practically local variables with values initialized by the caller.</p>\r\n\r\n<p>The instruction <code>local.get</code> pushes a variable to the stack, the instruction <code>i32.add</code> adds the two topmost values and pushes the result to the stack.</p>\r\n\r\n<dl class=\"definition\">\r\n  <dt><code>local.get $x</code></dt>\r\n  <dd>Pushes the value of the variable <code>$x</code> to the stack.</dd>\r\n  <dt><code>i32.add</code></dt>\r\n  <dd>Pops the two topmost values on the stack, adds them, and pushes the result to the stack.</dd>\r\n</dl>\r\n\r\n<p>Enough the theory. Now, we will finally jump into programming.</p>\r\n\r\n<h2>Control Flow</h2>\r\n\r\n<p>Let’s take a look at <a href=\"https://webassembly.github.io/spec/core/syntax/instructions.html#control-instructions\" target=\"_blank\">control instructions</a> in Wat: <em>conditions</em> and <em>loops</em>.</p>\r\n\r\n<p>We will demonstrate these with a basic factorial algorithm. First, we must recall the algorithm:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nfunction factorial(n) {\r\n  if (n <= 2) return n;\r\n  let fact = 1;\r\n  for (let i = 2; i <= n; i++) {\r\n      fact = fact * i;\r\n  }\r\n  return fact;\r\n}\r\n</pre>\r\n\r\n<p>Now, we program the same algorithm as a Wat function. The signature is straight-forward; one parameter, two local variables:</p>\r\n\r\n<pre>\r\n(func (export \"fac_loop\")\r\n      (param $n i32)\r\n      (result i32)\r\n      (local $i i32)\r\n      (local $fac i32)\r\n  ...)\r\n</pre>\r\n\r\n<p>For the first condition, we use <code>if-else-end</code> control instructions:</p>\r\n\r\n<pre>\r\n(func (export \"fac_loop\")\r\n      (param $n i32)\r\n      (result i32)\r\n      (local $i i32)\r\n      (local $fac i32)\r\n  i32.const 2\r\n  local.get $n\r\n  i32.ge_s\r\n  <b>if (result i32)</b>\r\n    local.get $n\r\n    return\r\n  <b>else</b>\r\n    ...\r\n  <b>end</b>)\r\n</pre>\r\n\r\n<dl class=\"definition\">\r\n  <dt><code>i32.ge_s</code></dt>\r\n  <dd>Pops the two topmost values on the stack as signed 32-bit integers, compares them to be greater or equals, and pushes the result to the stack.</dd>\r\n</dl>\r\n\r\n<p>We simply push <code>$n</code> and <code>2</code> values to the stack and check them on being greater or equal. The factorial calculation happens in the else block:</p>\r\n\r\n<pre>\r\n(func (export \"fac_loop\")\r\n      (param $n i32)\r\n      (result i32)\r\n      (local $i i32)\r\n      (local $fac i32)\r\n  i32.const 2\r\n  local.get $n\r\n  i32.ge_s\r\n  if (result i32)\r\n    local.get $n\r\n    return\r\n  else\r\n    i32.const 1\r\n    local.set $fac\r\n    i32.const 2\r\n    local.set $i\r\n\r\n    <b>loop</b>\r\n      local.get $i\r\n      local.get $fac\r\n      i32.mul\r\n      local.set $fac\r\n\r\n      local.get $i\r\n      i32.const 1\r\n      i32.add\r\n      local.set $i\r\n      \r\n      local.get $n\r\n      local.get $i\r\n      i32.ge_s\r\n      <b>br_if 0</b>\r\n    end\r\n\r\n    local.get $fac\r\n    return\r\n  end)\r\n</pre>\r\n\r\n<p>The <code>loop</code> instruction is controlled by the <code>br_if</code> instruction. If the condition passes, the loop continues.</p>\r\n\r\n<dl class=\"definition\">\r\n  <dt><code>local.set $x</code></dt>\r\n  <dd>Pops the value on the top of the stack and assigns it to the variable <code>$x</code>.</dd>\r\n  <dt><code>i32.mul</code></dt>\r\n  <dd>Pops the two topmost values on the stack, multiplies them, and pushes the result to the stack.</dd>\r\n</dl>\r\n\r\n<p>The parameter of <code>br_if</code> is an index of the loop. Alternatively, an explicit label can be used:</p>\r\n\r\n<pre>\r\nloop $myloop\r\n  ...\r\n  br_if $myloop\r\nend\r\n</pre>\r\n\r\n<h2>Calling Functions</h2>\r\n\r\n<p>Decomposition is a handy tool every programmer should employ. Using the recursive factorial algorithm as an example we will show how to call functions inside a function in Wat.</p>\r\n\r\n<p>The algorithm reads as follows:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nfunction factorial_rec(n) {\r\n  if (n <= 2) return n;\r\n  return n * factorial_rec(n - 1);\r\n}\r\n</pre>\r\n\r\n<p>The signature and first condition should be no surprise:</p>\r\n\r\n<pre>\r\n(func <b>$fac_rec</b>\r\n      (export \"fac_rec\")\r\n      (param $n i32)\r\n      (result i32)\r\n  i32.const 2\r\n  local.get $n\r\n  i32.ge_s\r\n  if (result i32)\r\n    local.get $n     \r\n  else\r\n    ...\r\n  end\r\n  return)\r\n</pre>\r\n\r\n<p>An important difference is the function name <code>$fac_rec</code>. This makes it easier (in comparison to using indexes) to call the function within the module:</p>\r\n\r\n<pre>\r\n(func $fac_rec\r\n      (export \"fac_rec\")\r\n      (param $n i32)\r\n      (result i32)\r\n  i32.const 2\r\n  local.get $n\r\n  i32.ge_s\r\n  if (result i32)\r\n    local.get $n     \r\n  else\r\n    local.get $n\r\n    i32.const 1\r\n    i32.sub\r\n    <b>call $fac_rec</b>\r\n    local.get $n\r\n    i32.mul\r\n  end\r\n  return)\r\n</pre>\r\n\r\n<p>The instruction <code>call</code> calls the referenced function with the parameter from the stack. The result of the call is pushed to the stack and multiplied by the value of the variable <code>$n</code>.</p>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>This time, we have focused on writing functions in Wat. We have shown function signatures, variables, and control flow instructions.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will explore the memory concept of Wasm and how to use it for working with complex data types such as strings.</p>\r\n\r\n<p>Stay tuned!</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(94, 'learning-webassembly-4-wasm-memory-and-working-with-strings', 1609757400, 'Learning WebAssembly #4: Wasm Memory and Working with Strings', '<p>Dealing with strings and other complex data types via Wasm memory mechanism.</p>', '<p>In the previous parts of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we were manipulating scalar numeric data exclusively. More complex data types such as strings do not have their representation in Wasm itself and must be modeled via Wasm <a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Using_the_JavaScript_API#Memory\" target=\"_blank\">memory</a> mechanism.</p>\r\n\r\n<h2>Wasm Memory</h2>\r\n\r\n<p><em>Memory</em> in Wasm is a large array of bytes that can grow over time. One has direct access to the raw bytes of the memory. A memory object can be provided by initialization or created automatically.</p>\r\n\r\n<p>The memory object can be shared and parties (like Wasm and JS) can pass values back and forth.</p>\r\n\r\n<p>Because Wasm memory is practically just a JavaScript object, it itself is tracked by the garbage collector. This prevents the system from running out of memory.</p>\r\n\r\n<p>The memory object is isolated from other memory objects owned by other modules.</p>\r\n\r\n<h2>Imported Memory from JavaScript</h2>\r\n\r\n<p>We can create a Memory object in JavaScript and have the WebAssembly module import the memory:</p>\r\n\r\n<pre class=\"brush: wat\">\r\n(module\r\n  (<b>import \"js\" \"mem\" (memory 1)</b>)\r\n  (<b>data (i32.const 0) \"Hello\"</b>)\r\n  \r\n  (func (export \"memtest\") \r\n        (result i32)\r\n    i32.const 5  ;; data length\r\n    return))\r\n</pre>\r\n\r\n<p>The module expects a memory object imported under the name <code>js.mem</code>. Data <code>Hello</code> is stored to the memory object on the position <code>0</code>.</p>\r\n\r\n<p>Then, JavaScript code creates a memory object and reads the data of the returned length:</p>\r\n\r\n<pre>\r\nvar mem = new <b>WebAssembly.Memory</b>({initial:1});\r\n\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'memory.wasm\'), { \r\n    js: { mem } \r\n  })\r\n  .then(({instance}) =&gt; {\r\n    var length = instance.exports.memtest();\r\n    var bytes = new <b>Uint8Array(mem.buffer, 0, length)</b>;\r\n    var string = new TextDecoder(\'utf8\').decode(bytes);\r\n\r\n    console.log(string);  // \"Hello\"\r\n  });\r\n</pre>\r\n\r\n<p>Raw data is read from the memory object in <code>Uint8Array</code> representation and decoded into UTF8 afterward.</p>\r\n\r\n<p>The expected result is printed to the dev console:</p>\r\n\r\n<pre>\r\n&raquo; Hello\r\n</pre>\r\n\r\n<h2>Exported Memory from Wasm</h2>\r\n\r\n<p>Albeit importing memory from JavaScript is more typical, we can also have the WebAssembly module create the memory and export it to JavaScript:</p>\r\n\r\n<pre class=\"brush: wat\">\r\n(module\r\n  (<b>memory $m 1</b>)\r\n  (<b>export \"mem\" (memory $m)</b>)\r\n  (data (i32.const 0) \"Hello\")\r\n  \r\n  (func (export \"dataLength\") \r\n        (result i32)\r\n    i32.const 5  ;; data length\r\n    return))\r\n</pre>\r\n\r\n<p>Now, the memory object is exported and accessible in JavaScript by the export name:</p>\r\n\r\n<pre>\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'memory2.wasm\'))\r\n  .then(({instance}) =&gt; {\r\n    var <b>memory = instance.exports.mem</b>;\r\n\r\n    var length = instance.exports.memtest();\r\n    var bytes = new Uint8Array(memory.buffer, 0, length);\r\n    var string = new TextDecoder(\'utf8\').decode(bytes);\r\n\r\n    console.log(string);  // \"Hello\"\r\n  });\r\n</pre>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>We have learned how to read a string from the memory object shared between Wasm and JavaScript code.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-5-running-wasm-in-the-browser\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will take a deeper look at the execution of Wasm in a browser and interaction with JavaScript.</p>\r\n\r\n<p>Stay tuned!</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(95, 'learning-webassembly-5-running-wasm-in-the-browser', 1609757500, 'Learning WebAssembly #5: Running Wasm in the Browser', '<p>Executing Wasm code in a browser via WebAssembly JavaScript API.</p>', '<p>In the previous parts of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we already executed Wasm modules in a browser. In this part, we will continue in explaining the <a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Using_the_JavaScript_API\" target=\"_blank\">WebAssembly JavaScript API</a> that makes all this possible.</p>\r\n\r\n<h2>Wasm Module Initializing</h2>\r\n\r\n<p>The easiest way to load and execute Wasm code in a browser is the <code>WebAssembly.instantiateStreaming</code> function:</p>\r\n\r\n<pre>\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'some.wasm\'))\r\n  .then(obj =&gt; {\r\n    ...\r\n  });\r\n</pre>\r\n\r\n<p>The initialized object has two attributes: <code>instance</code> and <code>module</code>. The <code>module</code> object contains stateless Wasm code that can be efficiently shared and initialized multiple times. The <code>instance</code> object is stateful, executable instance of a Wasm module.</p>\r\n\r\n<p class=\"warning\">To make the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\" target=\"_blank\">Fetch API</a> work you must serve the web page via HTTP(S).</p>\r\n\r\n<h2>Calling Wasm Functions</h2>\r\n\r\n<p>The interesting attribute of the <code>instance</code> object is the <code>exports</code> object which contains all exported functions from the Wasm module:</p>\r\n\r\n<pre>\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'some.wasm\'))\r\n  .then(obj =&gt; {\r\n    obj.instance.exports.myfunc1();\r\n    obj.instance.exports.myfunc2();\r\n  });\r\n</pre>\r\n\r\n<p>The JavaScript code can synchronously call the <em>exports</em>, which are exposed as normal JavaScript functions; with parameters and result values:</p>\r\n\r\n<pre>\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'some.wasm\'))\r\n  .then(obj =&gt; {\r\n    obj.instance.exports.myfunc1(1, 2, 3);\r\n    var res = obj.instance.exports.myfunc2();\r\n  });\r\n</pre>\r\n\r\n<h2>Importing JavaScript Functions</h2>\r\n\r\n<p>JavaScript functions can also be synchronously called by Wasm code by passing in as <em>imports</em> to a Wasm module instance:</p>\r\n\r\n<pre>\r\n(module\r\n  (import \"js\" \"log\" \r\n    (func $log (param i32)))\r\n  \r\n  (func (export \"logIt\")\r\n    i32.const 42\r\n    call $log))\r\n</pre>\r\n\r\n<p>Now, we call the exported function with the imported <code>console.log</code>:</p>\r\n\r\n<pre>\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'log.wasm\'), {\r\n    js: { log: console.log } \r\n  })\r\n  .then(({instance}) =&gt; {      \r\n    instance.exports.logIt();\r\n  });\r\n</pre>\r\n\r\n<p>We can see the result in the dev console:</p>\r\n\r\n<pre>\r\n&raquo; 42\r\n</pre>\r\n\r\n<h2>Advanced Logging Example</h2>\r\n\r\n<p>In the <a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\" target=\"_blank\">previous part</a>, we have worked with Wasm memory objects. Now, we put it all together and create proper logging from a Wasm module.</p>\r\n\r\n<p>We have to import a function taking two parameters: the offset and length of the data in the memory:</p>\r\n\r\n<pre>\r\n(module\r\n  (import \"js\" \"log\" \r\n    (func $log (param i32 i32)))\r\n\r\n  (import \"js\" \"mem\" (memory 1))\r\n  (data (i32.const 0) \"Hello\")\r\n\r\n  (func (export \"logIt\")\r\n    i32.const 0  ;; offset to log\r\n    i32.const 5  ;; length to log\r\n    call $log))\r\n</pre>\r\n\r\n<p>In JavaScript, the imported function takes a memory chunk defined by the offset and length parameters, decodes it as a string, and prints it to the console:</p>\r\n\r\n<pre>\r\nvar mem = new WebAssembly.Memory({initial:1});\r\n\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'mem.wasm\'), { \r\n    js: { \r\n      mem,\r\n      log: <b>(offset, length) =&gt;\r\n        logMemory(mem, offset, length)</b>\r\n    } \r\n  })\r\n  .then(({instance}) =&gt; {      \r\n    instance.exports.logIt();\r\n  });\r\n\r\nfunction <b>logMemory</b>(memory, offset, length) {\r\n  var bytes = new Uint8Array(memory.buffer, offset, length);\r\n  var str = new TextDecoder(\'utf8\').decode(bytes);\r\n  \r\n  console.log(str);\r\n}\r\n</pre>\r\n\r\n<p>We can see the result in the dev console:</p>\r\n\r\n<pre>\r\n&raquo; Hello\r\n</pre>\r\n\r\n<h2>Global Variables</h2>\r\n\r\n<p>We can create global variables accessible from both JavaScript and Wasm, importable/exportable across one or more module instances.</p>\r\n\r\n<p>First, we must define a variable in the global section:</p>\r\n\r\n<pre>\r\n(module\r\n  (global $g (import \"js\" \"glob\") (mut i32))\r\n  ...\r\n)\r\n</pre>\r\n\r\n<p>We call it simply <code>$g</code> and define it as mutable, of type 32-bit integer. Optionally, we will import the global instance from JavaScript code, so we can modify it directly in JavaScript.</p>\r\n\r\n<div class=\"note\">\r\n<p>When the instance is not imported, a default value must be provided:</p>\r\n<pre>\r\n(global $g1 (mut i32) (i32.const 42))\r\n</pre>\r\n</div>\r\n\r\n<p>Then, we add a getter and a setter for the global variable:</p>\r\n\r\n<pre>\r\n(module\r\n  (global $g (import \"js\" \"glob\") (mut i32))\r\n  \r\n  (func (export \"getGlobal\") \r\n        (result i32)\r\n    <b>global.get $g</b>)\r\n\r\n  (func (export \"setGlobal\") \r\n        (param $value i32)\r\n    local.get $value\r\n    <b>global.set $g</b>)\r\n)\r\n</pre>\r\n\r\n<p>Now, we can access and modify the same instance of the global variable in both Wasm and JavaScript:</p>\r\n\r\n<pre>\r\n// mutable global variable, default value: 2\r\nvar glob = new <b>WebAssembly.Global</b>({\r\n             value: \"i32\", mutable: true}, 2);\r\n\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'glob.wasm\'), { \r\n    js: { glob } \r\n  })\r\n  .then(({instance}) =&gt; {\r\n    console.log(instance.exports.getGlobal());\r\n    // prints \'2\'\r\n\r\n    <b>glob.value =</b> 3;\r\n    console.log(instance.exports.getGlobal());\r\n    // prints \'3\'\r\n\r\n    instance.exports.setGlobal2(4);    \r\n    console.log(instance.exports.getGlobal());\r\n    // prints \'4\'\r\n  });\r\n</pre>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>In the <a href=\"/learning-webassembly-6-running-wasm-in-nodejs\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will see how to run Wasm instances in Node.js.</p>\r\n\r\n<p>Stay tuned!</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(96, 'learning-webassembly-6-running-wasm-in-nodejs', 1609928000, 'Learning WebAssembly #6: Running Wasm in Node.js', '<p>Running Wasm code in Node.js, the popular backend platform.</p>', '<p>In the <a href=\"/learning-webassembly-5-running-wasm-in-the-browser\" target=\"_blank\">previous part</a> of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we executed Wasm modules in a browser. In this part, we will achieve the same in <a href=\"https://nodejs.org\" target=\"_blank\">Node.js</a>, a popular JavaScript backend platform.</p>\r\n\r\n<p>Starting with version 12, Node.js provides an implementation of the <a href=\"https://nodejs.org/api/wasi.html\" target=\"_blank\">WASI API</a>, WebAssembly System Interface. WASI gives sandboxed WebAssembly applications access to the underlying operating system features. In the following text, we will focus only on basic Wasm, WASI will be discussed in a dedicated future post.</p>\r\n\r\n<h2>Wasm Module Initializing</h2>\r\n\r\n<p>Practically, Node.js provides a global WebAssembly object, which we already know from <a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Using_the_JavaScript_API\" target=\"_blank\">WebAssembly JavaScript API</a>.</p>\r\n\r\n<p>There are some little differences though. For example, the convenience static method <code>instantiateStreaming</code> is missing whatsoever. The reason is quite clear: the method accepts an interface from the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\" target=\"_blank\">Fetch API</a>, which is not provided by Node.js out of the box.</p>\r\n\r\n<p>Instead, we can use the <code>instantiate</code> method which accepts an <code>ArrayBuffer</code>:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar buf = fs.readFileSync(\'./hello.wasm\');\r\nWebAssembly.instantiate(buf)...\r\n</pre>\r\n\r\n<p>The rest is the same as when <a href=\"/learning-webassembly-5-running-wasm-in-the-browser\" target=\"_blank\">running in a browser</a>:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nWebAssembly\r\n  .instantiate(fs.readFileSync(\'./hello.wasm\'))\r\n  .then(obj => console.log(obj.instance.exports.hello()));\r\n</pre>\r\n\r\n<p>It works on my machine:</p>\r\n\r\n<pre>\r\n$ node --version\r\nv14.15.3\r\n\r\n$ node hello.js \r\n42\r\n</pre>\r\n\r\n<p>Similar for dealing with the memory or global variables. There is no change needed in the actual code except the initialization of a Wasm instance.</p>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>We have seen how easy is to work with WebAssembly in Node.js. Except for a little difference in the initialization, all that we have learned so far applies here as well. It’s very good news!</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-7-introducing-wasi\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we will see how to access operating system features from Wasm with WebAssembly System Interface (WASI).</p>\r\n\r\n<p>Stay tuned!</p>', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(97, 'learning-webassembly-7-introducing-wasi', 1610217000, 'Learning WebAssembly #7: Introducing WASI', '<p>Accessing operating system features from Wasm with examples in Wat.</p>', '<p>In the <a href=\"/learning-webassembly-6-running-wasm-in-nodejs\" target=\"_blank\">previous part</a> of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we executed Wasm modules in Node.js, a popular JavaScript backend platform. In this part, we will see how to make system calls from Wasm.</p>\r\n\r\n<p><a href=\"https://wasi.dev\" target=\"_blank\">WebAssembly System Interface</a> (WASI) is a family of APIs designed as a new standard engine-independent non-web system-oriented interface for WebAssembly. It enables working with files, networking, and other operating system features directly from Wasm.</p>\r\n\r\n<p>WASI focuses highly on <strong>security</strong> and <strong>portability</strong>. In fact, these principles are baked explicitly into the WASI APIs.</p>\r\n\r\n<h2>Sandboxing</h2>\r\n\r\n<p>WebAssembly is <em>sandboxed</em>. This means that Wasm code can’t talk directly to the operating system. In order for Wasm to do anything with system resources, the host (browser or another runtime like Node.js or Wasmtime) must <a href=\"/learning-webassembly-5-running-wasm-in-the-browser\" target=\"_blank\">import functions</a> in the sandbox that the code can use.</p>\r\n\r\n<p>In this way, the host can limit what a program can do on a program-by-program basis.</p>\r\n\r\n<p>Albeit sandboxing makes systems more secure, the host can still import capabilities that had better not be there. WASI gives us a way to extend this security even further.</p>\r\n\r\n<h2>Security</h2>\r\n\r\n<p>With WASI, it is possible to attach permissions to different resources on a module-by-module basis. By default, a module has no access to any resource.</p>\r\n\r\n<p>When a module needs to access a file in a specific directory, the permission must be explicitly passed to the module:</p>\r\n\r\n<pre>\r\nvar wasi = new WASI({\r\n  preopens: {\r\n    \'/sandbox\': \'/real/path/that/wasm/can/access\'\r\n  }\r\n});\r\n</pre>\r\n\r\n<p>This brings Wasm closer to the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_privilege\" target=\"_blank\">principle of least privilege</a>, where a module can only access the exact resources it needs to do its job.</p>\r\n\r\n<h2>Portability</h2>\r\n\r\n<p>Wasm is a <em>portable</em> binary format. This means Wasm code could be compiled once and run across a whole bunch of different machines.</p>\r\n\r\n<p>The same Wasm module could be executed in a <a href=\"https://wasi.dev/polyfill\" target=\"_blank\">browser</a> and, for example, <a href=\"https://nodejs.org/api/wasi.html\" target=\"_blank\">Node.js</a> runtime.</p>\r\n\r\n<h2>WASI in Node.js</h2>\r\n\r\n<p>Starting with version 12, Node.js provides an implementation of WASI:</p>\r\n\r\n<pre>\r\nconst fs = require(\'fs\');\r\nconst { WASI } = require(\'wasi\');\r\n\r\nconst wasi = new <b>WASI</b>({\r\n  args: process.argv,\r\n  env: process.env,\r\n  preopens: {\r\n    \'/sandbox\': \'/some/real/path\'\r\n  }\r\n});\r\n\r\nconst importObject = { \r\n  <b>wasi_unstable: wasi.wasiImport</b>\r\n};\r\n\r\n(async () => {\r\n  const wasm = await WebAssembly\r\n    .compile(fs.readFileSync(\'hello.wasm\'));\r\n\r\n  const instance = await WebAssembly\r\n    .instantiate(wasm, importObject);\r\n\r\n  <b>wasi.start</b>(instance);\r\n})();\r\n</pre>\r\n\r\n<p>The following Wat code shows using WASI to print a string to the standard output:</p>\r\n\r\n<pre>\r\n(module\r\n  (import \"wasi_unstable\" \"<b>fd_write</b>\" \r\n    (func $fd_write (param i32 i32 i32 i32) \r\n                    (result i32)))\r\n\r\n  (memory 1)\r\n  (export \"memory\" (memory 0))\r\n  (data (i32.const 8) \"hello\\n\")\r\n\r\n  (func $main (export \"_start\")\r\n  \r\n    ;; io vector within memory\r\n    (i32.store (i32.const 0) (i32.const 8))\r\n    (i32.store (i32.const 4) (i32.const 6))\r\n\r\n    (call <b>$fd_write</b>\r\n        (i32.const 1)  ;; file_descriptor\r\n        (i32.const 0)  ;; *iovs\r\n        (i32.const 1)  ;; iovs_len\r\n        (i32.const 14) ;; nwritten\r\n    )\r\n    drop ;; drop the result from the stack\r\n  )\r\n)\r\n</pre>\r\n\r\n<p>As WASI is an experimental feature, the <code>--experimental-wasi-unstable-preview1</code> CLI argument is needed for this example to run:</p>\r\n\r\n<pre>\r\n$ node --version\r\nv14.15.3\r\n\r\n$ node --experimental-wasi-unstable-preview1 wasi-node.js\r\nhello\r\n</pre>\r\n\r\n<p>Let\'s take a closer look at the code above.</p>\r\n\r\n<p>The <code>fd_write</code> function is part of the WASI API and it is imported by the runtime. It takes a file descriptor, a pointer to a list of IO vectors and its length, and an index of a place in <a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\" target=\"_blank\">memory</a> to store the number of bytes written as parameters and returns the number of bytes written. The IO vector is the most interesting part.</p>\r\n\r\n<p>An <em>IO vector</em> is a structure to describe a piece of data in memory. It consists of two 32-bit integers: the index of the memory chunk where the data starts, and the length of the data in bytes:</p>\r\n\r\n<pre>\r\n;; data starts on 8th byte in memory\r\n(i32.store (i32.const 0) (i32.const 8))\r\n\r\n;; data \"hello\\n\" has length of 6 bytes\r\n(i32.store (i32.const 4) (i32.const 6))\r\n</pre>\r\n\r\n<p>Our vector is stored in 8 bytes of memory starting on index 0.</p>\r\n\r\n<p class=\"note\">Why are two 32-bit integers stored in 8 bytes? Easy numbers: one byte has 8 bits, a 32-bit type needs 32 / 8 = 4 bytes; two 32-bit types need 4 * 2 = 8 bytes.</p>\r\n\r\n<p>As parameters of the <code>fd_write</code> function, we put <code>1</code> for the stdout file descriptor, <code>0</code> as the memory index of the list IO vectors, <code>1</code> as the length of the list (we have only one string to print / one IO vector in the list), and finally, an empty place in memory to store the number of bytes written (data in the linear memory ends on the 14th byte — 8 + 6 = 14).</p>\r\n\r\n<p>And of course, we can run the same code with another WASI runtime like <a href=\"https://github.com/bytecodealliance/wasmtime\" target=\"_blank\">Wasmtime</a> as well:</p>\r\n\r\n<pre>\r\n$ wasmtime run hello.wasm\r\nhello\r\n</pre>\r\n\r\n<h2>WASI Read and Write</h2>\r\n\r\n<p>To demonstrate both read and write WASI capabilities, we write a slightly more advanced <em>Echo</em> program that reads input from stdout and prints it to stdout afterward:</p>\r\n\r\n<pre>\r\n(module\r\n  (import \"wasi_unstable\" \"<b>fd_read</b>\" \r\n    (func $fd_read \r\n      (param i32 i32 i32 i32) \r\n      (result i32)))\r\n  (import \"wasi_unstable\" \"<b>fd_write</b>\" \r\n    (func $fd_write \r\n      (param i32 i32 i32 i32) \r\n      (result i32)))\r\n\r\n  (memory 1)\r\n  (export \"memory\" (memory 0))\r\n\r\n  (func $main (export \"_start\")\r\n\r\n    ;; buffer of 100 chars to read into\r\n    (i32.store (i32.const 4) (i32.const 12))\r\n    (i32.store (i32.const 8) (i32.const 100))\r\n\r\n    (call <b>$fd_read</b>\r\n      (i32.const 0) ;; 0 for stdin\r\n      (i32.const 4) ;; *iovs\r\n      (i32.const 1) ;; iovs_len\r\n      (i32.const 8) ;; nread\r\n    )\r\n    drop\r\n\r\n    (call <b>$fd_write</b>\r\n      (i32.const 1) ;; 1 for stdout\r\n      (i32.const 4) ;; *iovs \r\n      (i32.const 1) ;; iovs_len\r\n      (i32.const 0) ;; nwritten\r\n    )\r\n    drop\r\n  )\r\n)\r\n</pre>\r\n\r\n<p>The <code>fd_read</code> function has a familiar signature; it takes a file descriptor, a list of IO vectors to read into, the list length, and a place in memory for the number of bytes read.</p>\r\n\r\n<p>This time, we will allocate memory of 100 bytes:</p>\r\n\r\n<pre>\r\n(i32.store (i32.const 4) (i32.const 12))\r\n(i32.store (i32.const <b>8</b>) (i32.const <b>100</b>))\r\n</pre>\r\n\r\n<p>We update the length of data (on the memory index 8) based on the number of bytes read, so the same data length will be printed afterward:</p>\r\n\r\n<pre>\r\n(call $fd_read\r\n  (i32.const 0) ;; 0 for stdin\r\n  (i32.const 4) ;; *iovs\r\n  (i32.const 1) ;; iovs_len\r\n  <b>(i32.const 8) ;; nread</b>\r\n)\r\n</pre>\r\n\r\n<p>Without this trick, the whole buffer of 100 bytes would be written to stdout (with trailing null bytes).</p>\r\n\r\n<p>It works as expected:</p>\r\n\r\n<pre>\r\n$ wasmtime run echo.wasm\r\n<i>hello, wasi!</i>\r\nhello, wasi!\r\n</pre>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>This time, we have shown how to work with system resources from Wasm using the WASI API.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-8-compiling-into-wasm\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a> we will leave Wat programming for a while and take a look at how to compile Wasm modules from different programming languages, such as C, Kotlin, and AssemblyScript.</p>\r\n\r\n<p>Stay tuned!</p>', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(98, 'learning-webassembly-8-compiling-into-wasm', 1610909000, 'Learning WebAssembly #8: Compiling into Wasm', '<p>There are plenty of languages Wasm can be compiled from. C, Kotlin, and AssemblyScript are only a few of them.</p>', '<p>In the <a href=\"/learning-webassembly-7-introducing-wasi\">previous part</a> of this <a href=\"/learning-webassembly-series\">series</a>, we introduced WASI, a family of APIs to access system resources. In this part, we will move away from low-level programming in Wat and consider compilation into Wasm from more high-level programming languages like C, Kotlin, or AssemblyScript.</p>\r\n\r\n<p>I deliberately did not choose Rust as, together with C/C++, it is a historically obvious choose and one can find plenty of articles about Rust and WebAssembly on the Internet.</p>\r\n\r\n<p>I choose C with respect to Emscripten, the first tool for producing WebAssembly, Kotlin for my Java background, and finally AssemblyScript as an obvious choice to develop modern Wasm programs.</p>\r\n\r\n<h2>C and Emscripten</h2>\r\n\r\n<p><a href=\"https://emscripten.org\" target=\"_blank\">Emscripten</a> is a complete Open Source compiler toolchain to WebAssembly. It emulates a particular OS system interface, POSIX, on the web. This means that you can use functions from the C standard library (<em>libc</em>).</p>\r\n\r\n<p>Emscripten achieves this by providing its own implementation of libc. The implementation has two parts: the Wasm module and JavaScript glue. The JavaScript part is called in the browser or other JavaScript runtime like Node.js, which then talks to the operating system.</p>\r\n\r\n<p>Unfortunately, the JavaScript glue interface was not designed to be a standard, which is a problem <a href=\"/learning-webassembly-7-introducing-wasi\">WASI</a> is trying to solve.</p>\r\n\r\n<p>Under the hood, Emscripten complies not only C/C++ code, but any LLVM-based language, like Scala, Crystal, Haskell, Julia, Swift, or Ruby.</p>\r\n\r\n<p>All right, let’s write a <i>Hello world</i> in C:</p>\r\n\r\n<pre>\r\n#include &lt;stdio.h&gt;\r\n\r\nint main() {\r\n  printf(\"Hello, world!\\n\");\r\n  return 0;\r\n}\r\n</pre>\r\n\r\n<p>To compile this C code into Wasm, you can either install Emscripten or simply use the official <a href=\"https://hub.docker.com/r/emscripten/emsdk\" target=\"_blank\">Docker image</a>:</p>\r\n\r\n<pre>\r\n$ docker run --rm -v $(pwd):/src\r\n  \\ emscripten/emsdk \r\n  \\ emcc hello.c -o hello.js\r\n</pre>\r\n\r\n<p>This will generate two files: hello.wasm and hello.js. You can run it in <a href=\"/learning-webassembly-6-running-wasm-in-nodejs\">Node.js</a>:</p>\r\n\r\n<pre>\r\n$ node hello.js\r\nHello, world!\r\n</pre>\r\n\r\n<p>Emscripten can also create an HTML page to run the Wasm module in:</p>\r\n\r\n<pre>\r\n$ docker run --rm -v $(pwd):/src\r\n  \\ emscripten/emsdk \r\n  \\ emcc hello.c -o hello.<b>html</b>\r\n</pre>\r\n\r\n<p>Now, you can open hello.html in a browser.</p>\r\n\r\n<p class=\"warning\">To make the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\" target=\"_blank\">Fetch API</a> work you must serve the web page via HTTP(S).</p>\r\n\r\n<p>Emscripten is the oldest compiler tools for Wasm. Nowadays, there many other options you can choose from. However, if you are interested in compiling into Wasm from an LLVM language, Emscripten is a natural choice.</p>\r\n\r\n<h2>Kotlin</h2>\r\n\r\n<p><a href=\"https://kotlinlang.org\" target=\"_blank\">Kotlin</a> is a modern practical language running on JVM. Kotlin can be also compiled into native code, JavaScript, and, obviously, Wasm.</p>\r\n\r\n<p>As already mentioned, Kotlin is based on LLVM and LLVM supports WebAssembly, ergo we can generate Wasm files from Kotlin source code.</p>\r\n\r\n<pre>\r\nfun main(args: Array&lt;String&gt;) {\r\n    println(\"Hello, world!\")\r\n}\r\n</pre>\r\n\r\n<p>You have to download (or build) the <a href=\"https://github.com/JetBrains/kotlin-native\" target=\"_blank\">Kotlin/Native compiler</a> and set Wasm as the compilation target:</p>\r\n\r\n<pre>\r\n$ konanc -target wasm32 hello.kt -o hello\r\n</pre>\r\n\r\n<p>This generates two files: hello.wasm and hello.wasm.js. The last step is to create an HTML page:</p>\r\n\r\n<pre>\r\n&lt;html&gt;\r\n  &lt;body&gt;\r\n    &lt;script wasm=&quot;./hello.wasm&quot; \r\n            src=&quot;./hello.wasm.js&quot;&gt;\r\n    &lt;/script&gt;\r\n  &lt;/body&gt;\r\n&lt;/html&gt;\r\n</pre>\r\n\r\n<p>Serve the page via HTTP and you should see the result in the dev console:</p>\r\n\r\n<pre>\r\nHello, world!\r\n</pre>\r\n\r\n<p>In the time of writing, the Wasm support in Kotlin may not be as mature as in other tools. However, if you are a developer coming from a JVM environment, you should definitely keep an eye on Kotlin.</p>\r\n\r\n<h2>AssemblyScript</h2>\r\n\r\n<p><a href=\"https://www.assemblyscript.org/\" target=\"_blank\">AssemblyScript</a> is a free and open source TypeScript-like language made for Wasm. It targets WebAssembly\'s feature set specifically, giving developers low-level control over their code.</p>\r\n\r\n<p>In AssemblyScript, you can access Wasm features directly via low-level built-ins, or you can use a JavaScript-like standard library, making it suitable for non-browser use cases.</p>\r\n\r\n<p>We will discuss these features in detail in the following article. For now, let’s create a <i>Hello world</i> program in AssemblyScript:</p>\r\n\r\n<pre>\r\n// assembly/index.ts\r\n\r\ndeclare function log(s: string): void;\r\n\r\nexport function main(): void {\r\n  log(\"Hello, world!\");\r\n}\r\n</pre>\r\n\r\n<p>Load and execute it from JavaScript code:</p>\r\n\r\n<pre>\r\n// index.js\r\n\r\nconst fs = require(\"fs\");\r\nconst loader = require(\"@assemblyscript/loader\");\r\n\r\nconst wasm = loader.instantiateSync(\r\n  fs.readFileSync(__dirname + \"/build/optimized.wasm\"), { \r\n    \"index\": {\r\n      log: function(pts) {\r\n        console.log(wasm.exports.__getString(pts));\r\n      } \r\n    } \r\n  }\r\n);\r\n\r\nwasm.exports.main();  // prints \"Hello, world!\"\r\n</pre>\r\n\r\n<p>AssemblyScript an obvious and natural choice for developers that come from the JavaScript environment and want to have low-level control over Wasm features.</p>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>This time, we have seen three different approaches on how to compile into WebAssembly.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-9-assemblyscript-basics\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a> we will focus on AssemblyScript exclusively.</p>\r\n\r\n<p>Stay tuned!</p>', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(99, 'learning-webassembly-9-assemblyscript-basics', 1611506000, 'Learning WebAssembly #9: AssemblyScript Basics', '<p>AssemblyScript is a free and open source TypeScript-like language that gives developers low-level control over Wasm features.</p>', '<p>We already met AssemblyScript in the <a href=\"/learning-webassembly-8-compiling-into-wasm\">previous part</a> of this <a href=\"/learning-webassembly-series\">series</a>. We created a simple program and got an impression of what programming in AssemblyScript feels like. In this part, we will go deeper into the concepts and basics of the language.</p>\r\n\r\n<h2>Gettings Started</h2>\r\n\r\n<p><a href=\"https://www.assemblyscript.org\" target=\"_blank\">AssemblyScript</a> compiles a strict variant of <a href=\"https://www.typescriptlang.org\" target=\"_blank\">TypeScript</a> to WebAssembly using <a href=\"https://github.com/WebAssembly/binaryen\" target=\"_blank\">Binaryen</a> compiler and toolchain infrastructure library.</p>\r\n\r\n<p>We will use <a href=\"https://www.npmjs.com\" target=\"_blank\">npm</a> to initialize our first AssemblyScript project:</p>\r\n\r\n<pre>\r\n$ npm init -y\r\n</pre>\r\n\r\n<p>AssemblyScript provides a very lightweight and efficient <a href=\"https://www.assemblyscript.org/loader.html\" target=\"_blank\">loader</a> of Wasm modules. Besides loading Wasm modules and exposing them via WebAssembly API, the loader is also providing utilities to allocate and read strings, arrays, and classes.</p>\r\n\r\n<pre>\r\n$ npm install --save @assemblyscript/loader\r\n</pre>\r\n\r\n<p>To compile AssemblyScript into Wasm we need to install the <code>asc</code> <a href=\"https://www.assemblyscript.org/compiler.html\" target=\"_blank\">compiler</a> as a dev dependency:</p>\r\n\r\n<pre>\r\n$ npm install --save-dev assemblyscript\r\n</pre>\r\n\r\n<p>Once installed, the compiler provides a handy scaffolding utility to quickly set up a new project:</p>\r\n\r\n<pre>\r\n$ npx asinit .\r\n</pre>\r\n\r\n<p>We will be modifying two files from the generated project:</p>\r\n\r\n<dl>\r\n  <dt><em>index.js</em></dt>\r\n  <dd>The main file for loading and working with Wasm modules.</dd>\r\n  <dt><em>assembly/index.ts</em></dt>\r\n  <dd>AssemblyScript source file to be compiled into Wasm.</dd>\r\n  <dt><em>tests/index.js</em></dt>\r\n  <dd>Test suite for the main file.</dd>\r\n</dl>\r\n\r\n<h2>Exports and Imports</h2>\r\n\r\n<p>Let us create our first AssemblyScript function in <em>assembly/index.ts</em>:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nexport function main(): void {\r\n  print(42);\r\n}\r\n</pre>\r\n\r\n<p>Unfortunately, we don’t get the function <code>print</code> for free. Leaving <a href=\"/learning-webassembly-7-introducing-wasi\">WASI</a> aside, interactions between Wasm modules and operating system must be provided by the glue code, JavaScript in our case. We have to declare the function signature first:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\ndeclare function print(n: i32): void;\r\n</pre>\r\n\r\n<p>Then, we can import a custom implementation in the main JavaScript file <em>index.js</em>:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar imports = {\r\n  \"index\": {\r\n    print: console.log\r\n  }\r\n};\r\n</pre>\r\n\r\n<p>The <code>imports</code> object is used by the initialization of the Wasm module by the loader:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar fs = require(\"fs\");\r\nvar loader = require(\"@assemblyscript/loader\");\r\n\r\nvar imports = {\r\n  \"index\": {\r\n    print: console.log\r\n  }\r\n};\r\n\r\nvar wasm = loader.instantiateSync(\r\n  fs.readFileSync(\"./build/optimized.wasm\"), \r\n  imports);\r\n\r\nwasm.exports.main();\r\n</pre>\r\n\r\n<p>Before we can run this code, first, we must compile our Wasm module:</p>\r\n\r\n<pre>\r\n$ npm run asbuild\r\n</pre>\r\n\r\n<p>This convenient script runs the asc compiler and generates untouched debug and optimized release Wasm files into the <em>build</em> directory. Now, we can simply run it:</p>\r\n\r\n<pre>\r\n$ node index.js\r\n42\r\n</pre>\r\n\r\n<p>Eureka, we have just created our first working Wasm program in AssemblyScript!</p>\r\n\r\n<h2>Strings, Strings, Strings!</h2>\r\n\r\n<p>Don’t get too excited, but there is one thing I have to share with you: AssemblyScript extends the set of Wasm standard data types for <em><a href=\"https://www.assemblyscript.org/stdlib/string.html\" target=\"_blank\">strings</a></em>.</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nexport function hello(\r\n    name: string): string {\r\n\r\n  return \"Hello, \" + name + \"!\";\r\n}\r\n</pre>\r\n\r\n<p>We can import and export strings from JavaScript with a little help from the loader:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar { hello, \r\n  __newString,\r\n  __getString, \r\n  __retain,\r\n  __release } = wasm.exports;\r\n</pre>\r\n\r\n<p>The loaded module contains functions for allocating and retaining strings in and from the Wasm memory:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar pti = __retain(__newString(\"Tomas\"));\r\n\r\nvar pto = hello(pti);\r\n\r\nvar str = __getString(pto);\r\n</pre>\r\n\r\n<p>The memory should be released after the string has been retained:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\n__release(pti);\r\n__release(pto);\r\n</pre>\r\n\r\n<p>Finally, we have an ordinary JavaScript string object:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconsole.log(str);\r\n</pre>\r\n\r\n<p>It works as expected:</p>\r\n\r\n<pre>\r\n$ node index.js\r\nHello, Tomas!\r\n</pre>\r\n\r\n<h2>Memory</h2>\r\n\r\n<p>Wasm uses linear memory stored in a specific memory offset isolated from other programs.</p>\r\n\r\n<p><a href=\"https://www.assemblyscript.org/memory.html\" target=\"_blank\">In AssemblyScript</a>, there is static memory for static data known at the compilation time and dynamic memory (heap) managed by the runtime. Programs access chunks of memory via pointers. Dynamic memory is tracked by the runtime’s garbage collector and reused when not needed by the program anymore.</p>\r\n\r\n<p>We have already seen allocating memory via <code>__newString</code> loader function. <code>__newString</code> is a convenience variant of the more general function <code>__new</code> for allocating managed objects.</p>\r\n\r\n<p>We can achieve the same functionality of <code>__newString</code> with some boiler-plate code:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar { hello, memory,\r\n  __new,\r\n  __retain, \r\n  __release } = wasm.exports;\r\n\r\nvar input = \"Tomas\";\r\nvar length = input.length;\r\n\r\n// allocate memory (usize, String (id=1))\r\nvar pt = __new(length << 1, 1);\r\n\r\n// load bytes into memory\r\nvar bytes = new Uint16Array(memory.buffer);\r\nfor (var i = 0, p = pt >>> 1; i < length; ++i) \r\n  bytes[p + i] = input.charCodeAt(i);\r\n\r\n// retain reference to object\r\nvar pti = __retain(pt);\r\n\r\n// call wasm\r\nhello(pti);\r\n\r\n// release memory\r\n__release(pti);\r\n</pre>\r\n\r\n<p>As we can see, strings in AssemblyScript are nothing more than arrays of integers stored in memory. The language brings some convenient syntax and utilities, however, internally, it does work with the <a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\" target=\"_blank\">same old</a> Wasm mechanism.</p>\r\n\r\n<h2>Arrays</h2>\r\n\r\n<p>AssemblyScript provides convenient functions not only for strings, but we can also work with all kinds of <em><a href=\"https://www.assemblyscript.org/stdlib/array.html\" target=\"_blank\">arrays</a></em>.</p>\r\n\r\n<p>As an example, consider a function that takes an array of 32-bit integers and returns a new array where all elements are multiplied by a scalar. In AssemblyScript, it would look like this:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nexport function multiply(\r\n    matrix: Int32Array, x: i32): Int32Array {\r\n\r\n  var arr = new Int32Array(matrix.length);\r\n  for (var i = 0; i < matrix.length; i++) {\r\n    arr[i] = matrix[i] * x;\r\n  }\r\n  return arr;\r\n}\r\n</pre>\r\n\r\n<p>To work with arrays properly, we need a unique class identifier for <code>Int32Array</code>. We can export it from AssemblyScript:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nexport const Int32Array_ID =\r\n    idof&lt;Int32Array&gt;();\r\n</pre>\r\n\r\n<p>In JavaScript, we can create an input array and read the result with appropriate utility functions:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nvar { multiply, Int32Array_ID,\r\n  __newArray, __getArray, \r\n  __retain,\r\n  __release } = wasm.exports;\r\n\r\n// input array\r\nvar input = [1, 2, 3];\r\nvar arri = __retain(__newArray(\r\n    Int32Array_ID, input));\r\n\r\n// call wasm, output array\r\nvar arro = __getArray(\r\n    multiply(arri, 2));\r\n\r\n// release memory\r\n__release(arri);\r\n__release(arro);\r\n\r\nconsole.log(arro);\r\n</pre>\r\n\r\n<p>The result should be no surprise:</p>\r\n\r\n<pre>\r\n$ node index.js\r\n[ 2, 4, 6 ]\r\n</pre>\r\n\r\n<h2>Testing AssemblyScript</h2>\r\n\r\n<p>After initializing a new AssemblyScript project, you can find a basic test in <em>test/index.js</em>. Basically, we can test everything which we export from the main JavaScript file:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\n// index.js\r\n\r\n// ...\r\n\r\nmodule.exports = wasm.exports;\r\n</pre>\r\n\r\n<pre class=\"brush: javascript\">\r\n// test/index.js\r\n\r\nvar assert = require(\"assert\");\r\nvar wasm = require(\"..\");\r\n\r\nvar pti = wasm.__retain(\r\n    wasm.__newString(\"Test\"));\r\nvar pto = hello(pti);\r\nvar str = wasm.__getString(pto);\r\nwasm.__release(pti);\r\nwasm.__release(pto);\r\n\r\nassert.equal(str, \"Hello, Test!\");\r\n</pre>\r\n\r\n<p>Testing as such looks pretty tedious. We can hide the JavaScript glue behind a facade function and test that instead of the “naked” Wasm function:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\n// index.js\r\n\r\n// ...\r\n\r\nfunction sayHello(name) {\r\n  var pti = __retain(__newString(name));\r\n  var pto = hello(pti);\r\n  var str = __getString(pto);\r\n  __release(pti);\r\n  __release(pto);\r\n\r\n  return str;\r\n}\r\n\r\nmodule.exports = { sayHello };\r\n</pre>\r\n\r\n<pre class=\"brush: javascript\">\r\n// test/index.js\r\n\r\nconst assert = require(\"assert\");\r\nconst index = require(\"..\");\r\n\r\nassert.equal(\r\n    index.sayHello(\"Test\"), \r\n    \"Hello, Test!\"\r\n);\r\n</pre>\r\n\r\n<p>Run it simply with npm:</p>\r\n\r\n<pre>\r\n$ npm test\r\n</pre>\r\n\r\n<h2>Further Steps</h2>\r\n\r\n<p>We have learned the basics of programming in AssemblyScript, including utilities for working with memory, strings and arrays, and we have written our first unit test.</p>\r\n\r\n<p>In the <a href=\"/learning-webassembly-10-image-processing-in-assemblyscript\">next part</a> of this <a href=\"/learning-webassembly-series\">series</a> we will go deeper into programming in AssemblyScript and learn how to generate some graphics.</p>\r\n\r\n<p>Stay tuned!</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1),
(100, 'learning-webassembly-10-image-processing-in-assemblyscript', 1611995000, 'Learning WebAssembly #10: Image Processing in AssemblyScript', '<p>WebAssembly is a great fit for image processing. We will manipulate image data with a simple Wasm function written in AssemblyScript and run it in the web browser.</p>', '<p>In the <a href=\"/learning-webassembly-9-assemblyscript-basics\" target=\"_blank\">previous part</a> of this <a href=\"/learning-webassembly-series\" target=\"_blank\">series</a>, we already learned how to write Wasm modules in AssemblyScript. In this part, we will use this knowledge in a practical scenario: image manipulations with WebAssembly.</p>\r\n\r\n<p>We will demonstrate a typical use-case by a simple function for converting an image to <em>grayscale</em>.</p>\r\n\r\n<p>Albeit the calculation of the gray color is not very demanding, it clearly demonstrates the real-world usage of WebAssembly on the web: <strong>computation-intensive tasks</strong>.</p>\r\n\r\n<p>You can find the full discussed source code on <a href=\"https://github.com/ttulka/assemblyscript-samples/tree/main/grayscale\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<h2>Browser Runtime</h2>\r\n\r\n<p>Unlike our previous experiments with AssemblyScript, this time we will run our Wasm module in a browser. You might first recall how to use a <a href=\"/learning-webassembly-5-running-wasm-in-the-browser\" target=\"_blank\">web browser as a Wasm runtime</a>, but the code is quite straight-forward:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'grayscale.wasm\'), {})\r\n  .then(({ instance }) => {\r\n    ...\r\n  }\r\n</pre>\r\n\r\n<p class=\"warning\">To make the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\" target=\"_blank\">Fetch API</a> work you must serve the web page via HTTP(S).</p>\r\n\r\n<p>The second argument of the <code>instantiateStreaming</code> is an object with Wasm imports. All environment imports for Wasm modules, which were compiled from AssemblyScript, are included in an <code>env</code> object:</p>\r\n\r\n<p>When running in the browser, we must import an error-callback function <code>abort</code>:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'grayscale.wasm\'), {\r\n    env: {\r\n      abort: (_msg, _file, line, column) => \r\n        console.error(`Error at ${line}:${column}`)\r\n    }  \r\n  })\r\n</pre>\r\n\r\n<p class=\"note\">In Node.js those are already provided by the AssemblyScript loader out of the box.</p>\r\n\r\n<h2>Canvas</h2>\r\n\r\n<p>To show the image and its transformation in a browser, we will use the HTML <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/canvas\" target=\"_blank\">canvas element</a>:</p>\r\n\r\n<pre class=\"brush: html\">\r\n&lt;canvas id=&quot;canvas&quot; width=&quot;500&quot; height=&quot;500&quot;&gt;&lt;/canvas&gt;\r\n</pre>\r\n\r\n<p>And its 2D context:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst canvas = document.getElementById(\'canvas\');\r\nconst ctx = canvas.getContext(\'2d\');\r\nconst [width, height] = [canvas.width, canvas.height];\r\n</pre>\r\n\r\n<p>First, we will draw the original image on the canvas:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst img = new Image();\r\nimg.src = \'./my-pretty-picture.png\';\r\nimg.crossOrigin = \'anonymous\';\r\nimg.onload = () =&gt; \r\n  ctx.drawImage(img, 0, 0, width, height);\r\n</pre>\r\n\r\n<p>Then, we will get the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ImageData\" target=\"_blank\">image data</a> that we will work with:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst imageData = ctx.getImageData(0, 0, width, height);\r\nconst data = imageData.data;\r\n</pre>\r\n\r\n<p>The image <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/ImageData/data\" target=\"_blank\">data</a> is represented as a <code>Uint8ClampedArray</code> a one-dimensional array in the RGBA order, with integer values between 0 and 255 (inclusive).</p>\r\n\r\n<h2>Memory</h2>\r\n\r\n<p>Because we will work extensively with memory, we should <a href=\"/learning-webassembly-4-wasm-memory-and-working-with-strings\" target=\"_blank\">recall the fundamentals</a>.</p>\r\n\r\n<p>We can either create a memory instance from JavaScript and import it into the Wasm module or let the module initialize memory and export its instance into JavaScript.</p>\r\n\r\n<p>One benefit of the former approach is the possibility of initializing memory to the needed size.</p>\r\n\r\n<p>When exporting memory from Wasm, the initial size is one <em>page</em> (64 KB). To increase its size, we have to call <code>memory.grow()</code> programmatically, which could be impractical, at least in cases where the size is known in advance.</p>\r\n\r\n<p>Our image might be bigger than 64 KB, so we had better create a big-enough memory instance:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst arraySize = (width * height * 4) &gt;&gt;&gt; 0;\r\nconst nPages = ((arraySize + 0xffff) &amp; ~0xffff) &gt;&gt;&gt; 16;\r\nconst memory = new WebAssembly.Memory({ initial: nPages });\r\n</pre>\r\n\r\n<p>Here comes the catch. To be able to import memory into a Wasm module, we have to compile our AssemblyScript code with the <code>--importMemory</code> flag:</p>\r\n\r\n<pre>\r\n$ npm run asbuild:optimized -- --importMemory\r\n</pre>\r\n\r\n<p>This will generate the following line in the compiled Wasm module:</p>\r\n\r\n<pre>\r\n(import \"env\" \"memory\" (memory $0 1))\r\n</pre>\r\n\r\n<p>Now, we can import our memory object from JavaScript into Wasm:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nWebAssembly\r\n  .instantiateStreaming(fetch(\'grayscale.wasm\'), {\r\n    env:{ memory }\r\n  })\r\n</pre>\r\n\r\n<p>The next step is to initialize the memory with the image data:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst bytes = new Uint8ClampedArray(memory.buffer);\r\n      \r\nfor (let i = 0; i < data.length; i++)\r\n  bytes[i] = data[i];\r\n</pre>\r\n\r\n<p>As you might have noticed, we use the same <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray\" target=\"_blank\">typed array</a> — <code>Uint8ClampedArray</code> — as image data to format and access memory bytes.</p>\r\n\r\n<p>When the memory bytes are filled, we can execute our Wasm function:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\ninstance.exports\r\n  .convertToGrayscale(width, height);\r\n</pre>\r\n\r\n<p>And store the result from memory back into the image:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nfor (let i = 0; i < bytes.length; i++) \r\n  data[i] = bytes[i];\r\n\r\nctx.putImageData(imageData, 0, 0);\r\n</pre>\r\n\r\n<h2>Image Manipulation</h2>\r\n\r\n<p>Now, when we have all we need to run an image-manipulation function in a browser, we shall actually write one. As previously mentioned, we will create a function that converts an image to grayscale:</p>\r\n\r\n<pre class=\"brush: typescript\">\r\nexport function convertToGrayscale(\r\n    width: i32, height: i32): void {\r\n\r\n  const len = width * height * 4;\r\n\r\n  for (let i = 0; i &lt; len; i += 4 /*rgba*/) {\r\n    const r = load&lt;u8&gt;(i);\r\n    const g = load&lt;u8&gt;(i + 1);\r\n    const b = load&lt;u8&gt;(i + 2);\r\n    \r\n    const gray = u8(\r\n      r * 0.2126 + g * 0.7152 + b * 0.0722);\r\n\r\n    store&lt;u8&gt;(i,     gray);\r\n    store&lt;u8&gt;(i + 1, gray);\r\n    store&lt;u8&gt;(i + 2, gray);\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>As the image data contains linear RGBA quadruples of 8-bit signed integers (clamped to the range of values from 0 to 255), we iterate the array of data in 4-incremental steps.</p>\r\n\r\n<p>We load the RGB values from memory, compute the gray color, and store the color values back into the memory.</p>\r\n\r\n<h2>Demo Time</h2>\r\n\r\n<p>That’s it! Now we have all the know-how needed to develop awesome image processing Wasm functions in AssemblyScript.</p>\r\n\r\n<p>Here are some more examples:</p>\r\n\r\n<p>\r\n  <canvas id=\"canvas\" width=\"500\" height=\"500\" \r\n  	style=\"border: 3px solid var(--primary, darkblue); max-width: 100%; box-sizing: border-box;\"></canvas>\r\n</p>\r\n\r\n<ul>\r\n  <li><a href=\"#\" class=\"action original\">original</a></li>\r\n  <li><a href=\"#\" class=\"action grayscale\">grayscale</a></li>\r\n  <li><a href=\"#\" class=\"action basicMonochrome\">monochrome</a></li>\r\n  <li><a href=\"#\" class=\"action randomMonochrome\">randomized monochrome</a></li>\r\n  <li><a href=\"#\" class=\"action invert\">color inversion</a></li>\r\n</ul>\r\n\r\n<script src=\"/storage/imaging.js\"></script>\r\n\r\n<p>You can find the demo source code on <a href=\"https://github.com/ttulka/assemblyscript-samples/tree/main/imaging\" target=\"_blank\">my GitHub</a>.</p>\r\n', 'WebAssembly,Programming', 'false', 'false', 1, 1);
INSERT INTO `Post` (`id`, `url`, `createdAt`, `title`, `summary`, `body`, `tags`, `isMenu`, `isDraft`, `authorId`, `categoryId`) VALUES
(101, 'react-custom-context-hook', 1612465000, 'React Custom Context Hook', '<p>Updatable React context Hook with custom logic in TypeScript.</p>', '<p>React Hooks brings status management to functions and enables us to get rid of JavaScript classes whatsoever. Now, both components and Hooks are mere functions.</p>\r\n\r\n<p>Hooks are a good idea. So good that it was worth mentioning in the <a href=\"https://www.thoughtworks.com/radar/languages-and-frameworks/react-hooks\" target=\"_blank\">Technology Radar</a>.</p>\r\n\r\n<p>As React is an unopinionated framework, it is hard to find a consistent set of good practices and integrous architectural principles.</p>\r\n\r\n<p>The same is true for Hooks. Hooks can be used in different ways, based on the use-case or personal taste.</p>\r\n\r\n<p>The Context Hook is one of the less common but still very useful built-in Hooks.</p>\r\n\r\n<h2>Context Hook</h2>\r\n\r\n<p>Using context makes it easy to share data through components without passing it as properties.</p>\r\n\r\n<p>Practically, it addresses awkward situations like this:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst App = () =&gt; {\r\n  const theme = &quot;dark&quot;;\r\n  return (\r\n    &lt;Menu theme={theme} /&gt;\r\n    &lt;Content theme={theme} /&gt;\r\n    &lt;Footer theme={theme}  /&gt;\r\n  );\r\n}\r\n\r\nconst Menu = ({theme}}) =&gt; {\r\n  return (\r\n    &lt;div&gt;\r\n      &lt;Button theme={theme} label=&quot;Home&quot; /&gt;\r\n      &lt;Button theme={theme} label=&quot;About&quot; /&gt;\r\n    &lt;/div&gt;\r\n  );\r\n}\r\n\r\nconst Button = ({theme, label}) =&gt; {\r\n  return (\r\n    &lt;a className={theme}&gt;\r\n      {label}\r\n    &lt;/a&gt;\r\n  )\r\n}\r\n\r\n...\r\n</pre>\r\n\r\n<p>The build-in Hook <code>useContext</code> accepts a context object and returns the current context value:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst value = useContext(MyContext);\r\n</pre>\r\n\r\n<p>A component calling <code>useContext</code> will always re-render when the context value changes.</p>\r\n\r\n<p>The solution for the problem above looks like follows:</p>\r\n\r\n<pre class=\"brush: javascript\">\r\nconst ThemeContext = createContext(&quot;light&quot;);\r\n\r\nconst App = () =&gt; {\r\n  return (\r\n    &lt;ThemeContext.Provider value=&quot;dark&quot;&gt;\r\n      &lt;Menu /&gt;\r\n      &lt;Content /&gt;\r\n      &lt;Footer /&gt;\r\n    &lt;/ThemeContext.Provider&gt;\r\n  );\r\n}\r\n\r\nconst Menu = () =&gt; {\r\n  return (\r\n    &lt;div&gt;\r\n      &lt;Button label=&quot;Home&quot; /&gt;\r\n      &lt;Button label=&quot;About&quot; /&gt;\r\n    &lt;/div&gt;\r\n  );\r\n}\r\n\r\nconst Button = ({label}) =&gt; {\r\n  const theme = useContext(ThemeContext);\r\n  return (\r\n    &lt;a className={theme}&gt;\r\n      {label}\r\n    &lt;/a&gt;\r\n  )\r\n}\r\n\r\n...\r\n</pre>\r\n\r\n<p>To provide functionality for changing the current value of the context, we must use it together with other Hooks. Combining multiple Hooks to work out a common goal is a case for a <em>custom Hook</em>.</p>\r\n\r\n<h2>Custom Context Hook</h2>\r\n\r\n<p>A custom Hook is a function whose name starts with <code>use</code> and that may call other Hooks.</p>\r\n\r\n<p>Our Hook will create a context for holding a value of the current theme (light or dark). The user can change the theme anytime. The current value will be stored in <code>localStorage</code> to be used on the next visit.</p>\r\n\r\n<p>We will create our <code>useTheme</code> TypeScript-first:</p>\r\n\r\n<pre class=\"brush: typescript\">\r\n// useTheme.tsx\r\n\r\nimport {createContext, useContext, useState} from &quot;react&quot;;\r\n\r\n// context object structure\r\ntype ContextType = {\r\n    theme: string;\r\n    updateTheme: (theme: string) =&gt; void;\r\n};\r\n\r\n// create an empty context\r\nconst ThemeContext = createContext&lt;ContextType&gt;({\r\n    theme: &quot;&quot;,\r\n    updateTheme: () =&gt; {}\r\n});\r\n\r\n// context provider container\r\nexport const ThemeProvider = (prop: \r\n    {value?: string, children: JSX.Element | JSX.Element[]}) =&gt; {\r\n\r\n  const [theme, setTheme] = useState&lt;string&gt;(\r\n    localStorage.getItem(&quot;theme&quot;) || prop.value || &quot;light&quot;);\r\n\r\n  const updateTheme = (theme: string) =&gt; {\r\n    setTheme(theme);\r\n    localStorage.setItem(&quot;theme&quot;, theme);\r\n  };\r\n\r\n  return (\r\n    &lt;ThemeContext.Provider value={{theme, updateTheme}}&gt;\r\n      {prop.children}\r\n    &lt;/ThemeContext.Provider&gt;\r\n  );\r\n};\r\n\r\n// custom context hook\r\nconst useTheme = () =&gt; useContext(ThemeContext) as ContextType;\r\n\r\nexport default useTheme;\r\n</pre>\r\n\r\n<p>Our context object holds the <em>current value</em> and <em>update function</em>. To take the complicated initialization away from the user code, we provide a convenient <code>ThemeProvider</code> function. This provider container initializes the context with a state and the update function. The state is initialized with the value from <code>localStorage</code> or a user-provided default value respectively. The update function actualizes the state and storage when the context value has changed.</p>\r\n\r\n<p>Next, we will use our Hook in the components. The whole component tree must be closed in the context provider:</p>\r\n\r\n<pre class=\"brush: typescript\">\r\n// App.tsx\r\n\r\nimport Menu from &quot;./Menu&quot;;\r\nimport Content from &quot;./Content&quot;;\r\nimport Footer from &quot;./Footer&quot;;\r\nimport {ThemeProvider} from &quot;./useTheme&quot;;\r\n\r\nconst App = () =&gt; {\r\n  return (\r\n    &lt;ThemeProvider value=&quot;dark&quot;&gt;\r\n      &lt;Menu /&gt;\r\n      &lt;Content /&gt;\r\n      &lt;Footer /&gt;\r\n    &lt;/ThemeProvider&gt;\r\n  );\r\n}\r\n\r\nexport default App;\r\n</pre>\r\n\r\n<p>Now, all children of the provider have access to its context.</p>\r\n\r\n<p>We will use the update function to change the theme based on the user’s preferences:</p>\r\n\r\n<pre class=\"brush: typescript\">\r\n// Menu.tsx\r\n\r\nimport Button from &quot;./Button&quot;;\r\nimport useTheme from &quot;./useTheme&quot;;\r\n\r\nconst Menu = () =&gt; {\r\n  // update function from the context\r\n  const {updateTheme} = useTheme();\r\n\r\n  const lighten = () =&gt; updateTheme(&quot;light&quot;);\r\n  const darken = () =&gt; updateTheme(&quot;dark&quot;);\r\n\r\n  return (\r\n    &lt;div&gt;\r\n      &lt;nav&gt;\r\n        &lt;a onClick={lighten}&gt;&#x1F31D;&lt;/a&gt;\r\n        &lt;a onClick={darken}&gt;&#x1F31A;&lt;/a&gt;\r\n      &lt;/nav&gt;\r\n      &lt;Button label=&quot;Home&quot; /&gt;\r\n      &lt;Button label=&quot;About&quot; /&gt;\r\n    &lt;/div&gt;\r\n  );\r\n}\r\n\r\nexport default Menu;\r\n</pre>\r\n\r\n<p>Finally, the value is used to render components with the corresponding theme:</p>\r\n\r\n<pre class=\"brush: typescript\">\r\n// Button.tsx\r\n\r\nimport useTheme from &quot;./useTheme&quot;;\r\nimport &quot;./Button.css&quot;;\r\n\r\nconst Button = (prop: {label: string}) =&gt; {\r\n  // receive the current context value\r\n  const {theme} = useTheme();\r\n  return (\r\n    &lt;button className={theme}&gt;\r\n      {prop.label}\r\n    &lt;/button&gt;\r\n  )\r\n}\r\n\r\nexport default Button;\r\n</pre>\r\n\r\n<h2>Source Code</h2>\r\n\r\n<p>A demo application with mentioned code is on <a href=\"https://github.com/ttulka/react-samples/tree/main/custom-context-hook-theme\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy reacting!</p>\r\n', 'React,Frontend,TypeScript,Programming', 'false', 'false', 1, 1),
(102, 'how-to-test-date-and-time-in-spring-boot', 1612617000, 'How to Test Date and Time in Spring Boot', '<p>Testing temporal events in Spring Boot Java applications with JUnit 5 and Spring Boot Test.</p>', '<p>Last week, on February 1st, our build pipeline noticed a failing test. It showed up that this test, written several weeks ago, in January, passed indeed only in January.</p>\r\n\r\n<p>January is the only month that string-concatenated with <code>\"1\"</code> gives you a valid month — <code>\"11\"</code>, November. Starting with 1st February, the test made no more sense as <code>\"21\"</code> is obviously no valid month.</p>\r\n\r\n<p>Albeit it was quite easy to fix this “everwinter” bug, the situation demonstrates well how tricky testing time could be. If your tests ever failed when close to midnight, in a leap year, or on a machine in a different time zone, then you know what I am talking about.</p>\r\n\r\n<p><em>Test in isolation</em>. I guess you have already heard this statement. It is often understood as isolating units under test from each other. This explanation caused the explosion of mock-based testing. I believe it completely misses the point.</p>\r\n\r\n<p>The <strong>tests</strong> should be <strong>isolated from each other</strong>, so they are <strong>independent of execution</strong> order and side-effects. Tests should also be <strong>isolated from environment</strong> specifics, <strong>independent of volatile</strong> and unstable data.</p>\r\n\r\n<p>What can be more volatile than ever-in-flux, elusive time? We need to find a way how to control time in our tests.</p>\r\n\r\n<h2>Testing Date and Time</h2>\r\n\r\n<p>Consider a simple example of placing a new Order:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Service\r\nclass OrderService {\r\n\r\n  public Order place(String content) {\r\n    return Order.builder()\r\n      .id(UUID.randomUUID())\r\n      .createdAt(ZonedDateTime.now())\r\n      .content(content)\r\n      .build();\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>How can we test that the Order was created at the current time?</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Test\r\nvoid order_is_placed_at_current_time() {\r\n  var order = orderService.place(\"Test\");\r\n\r\n  assertThat(order.getCreatedAt())\r\n    .isEqualTo(ZonedDateTime.now());\r\n}\r\n</pre>\r\n\r\n<p>Nope, this won’t work. There are tools to help us with this tedious task:</p>\r\n\r\n<pre class=\"brush: java\">\r\nassertThat(order.getCreatedAt())\r\n  .isCloseTo(ZonedDateTime.now(), \r\n    within(1, ChronoUnit.SECONDS));\r\n</pre>\r\n\r\n<p>Yes, this will work. But what if the placing takes more than one second? What if even nanoseconds matter?</p>\r\n\r\n<p>Sometimes, we need a solution that brings us one hundred percent certainty. Sometimes, we just need to control the clock.</p>\r\n\r\n<h2>Mocking the Clock</h2>\r\n\r\n<p>An obvious option is mocking. We already <a href=\"https://blog.ttulka.com/glass-box-testing-does-not-need-mocking\" target=\"_blank\">mock almost everything</a> (sadly), so why not mock time as well?</p>\r\n\r\n<p>Date and time in Java can be calculated dependently on a <code>java.time.Clock</code> instance:</p>\r\n\r\n<pre class=\"brush: java\">\r\nvar clock = Clock.systemUTC();\r\nvar zdt = ZonedDateTime.now(clock);\r\n</pre>\r\n\r\n<p>We can use this tactic for our advance:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Service\r\nclass OrderService {\r\n\r\n  private final Clock clock = Clock.systemUTC();\r\n\r\n  public Order place(String content) {\r\n    return Order.builder()\r\n      .id(UUID.randomUUID())\r\n      .createdAt(ZonedDateTime.now(clock))\r\n      .content(content)\r\n      .build();\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>When using the <a href=\"https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#0.2\" target=\"_blank\">Mockito inline maker</a> we can easily mock static methods:</p>\r\n\r\n<pre class=\"brush: java\">\r\nstatic Clock clock;\r\n\r\n@BeforeAll\r\nstatic void setupClock() {\r\n  clock = Clock.fixed(\r\n      Instant.parse(\"2020-12-01T10:05:23.653Z\"),\r\n      ZoneId.of(\"Europe/Prague\"));\r\n\r\n  Mockito.mockStatic(Clock.class)\r\n    .when(Clock::systemUTC).thenReturn(clock);\r\n}\r\n\r\n@Test\r\nvoid order_is_placed_at_current_time() {\r\n  var order = orderService.place(\"Test\");\r\n\r\n  assertThat(order.getCreatedAt())\r\n    .isEqualTo(ZonedDateTime.now(clock));\r\n}\r\n</pre>\r\n\r\n<p>This will work, but there is a lot of boilerplate code and an amount of black magic going on.</p>\r\n\r\n<p>Mocking static methods will definitely impact other tests that use the methods which means that just lost test isolation again. Can we do better?</p>\r\n\r\n<h2>Fixing the Clock</h2>\r\n\r\n<p>Why not just use the basic Spring feature and inject the clock as a bean dependency?</p>\r\n\r\n<pre class=\"brush: java\">\r\n@Service\r\nclass OrderService {\r\n\r\n  private final Clock clock;\r\n\r\n  public OrderService(Clock clock) {\r\n    this.clock = clock;\r\n  }\r\n\r\n  ...\r\n}\r\n</pre>\r\n\r\n<p>Now, when the clock became a dependency of the service, we have to provide a default instance to the Spring context:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@SpringBootApplication\r\npublic class ShoppingApplication {\r\n\r\n  public static void main(String[] args) {\r\n    SpringApplication.run(ShoppingApplication.class, args);\r\n  }\r\n\r\n  @Bean\r\n  Clock clock() {\r\n    return Clock.systemUTC();\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>We can change this instance arbitrarily, like providing a fixed clock for our tests:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@TestConfiguration\r\nclass FixedClockConfig {\r\n\r\n  @Primary\r\n  @Bean\r\n  Clock fixedClock() {\r\n    return Clock.fixed(\r\n        Instant.parse(\"2020-12-01T10:05:23.653Z\"),\r\n        ZoneId.of(\"Europe/Prague\"));\r\n  }\r\n}\r\n\r\n@SpringBootTest(classes = FixedClockConfig.class)\r\nclass OrderTests {\r\n\r\n  @Autowired\r\n  private Clock clock;\r\n\r\n  @Test\r\n  void order_is_placed_at_current_time() {\r\n    var order = orderService.place(\"Test\");\r\n\r\n    assertThat(order.getCreatedAt())\r\n      .isEqualTo(ZonedDateTime.now(clock));\r\n  }\r\n}\r\n</pre>\r\n\r\n<p>We can use the same test configuration in multiple tests:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@SpringBootTest(classes = ClockConfig.class)\r\nclass OrderTests {\r\n  ...\r\n}\r\n\r\n@SpringBootTest(classes = ClockConfig.class)\r\nclass DeliveryTests {\r\n  ...\r\n}\r\n</pre>\r\n\r\n<h2>Controlling the Clock</h2>\r\n\r\n<p>When a fixed clock is not enough, we can gain more control by mixing both worlds: injecting a mocked clock:</p>\r\n\r\n<pre class=\"brush: java\">\r\n@SpringBootTest\r\nclass OrderTests {\r\n\r\n  @MockBean\r\n  private Clock clock;\r\n\r\n  @BeforeEach\r\n  void setupClock() {\r\n    when(clock.getZone()).thenReturn(\r\n        ZoneId.of(\"Europe/Prague\"));\r\n  }\r\n\r\n  @Test\r\n  void order_is_placed_at_current_time() {\r\n    when(clock.instant()).thenReturn(\r\n        Instant.parse(\"2020-12-01T10:05:23.653Z\"));\r\n\r\n    var order = orderService.place(\"Test\");\r\n\r\n    assertThat(order.getCreatedAt())\r\n      .isEqualTo(ZonedDateTime.now(clock));\r\n  }\r\n}\r\n</pre>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>We have seen how to test date and time with Spring Boot and Mockito.</p>\r\n\r\n<p>Controlling time brings certainty into tests, makes them isolated from the environment, independent, and stable.</p>\r\n\r\n<p>The source code can be found on <a href=\"https://github.com/ttulka/spring-boot-samples/tree/master/testing-date-time\" target=\"_blank\">my GitHub</a>.</p>\r\n\r\n<p>Happy timing!</p>\r\n', 'Testing,Spring Boot,Java,Programming', 'false', 'false', 1, 1),
(103, 'rethinking-api-versioning-with-domain-driven-design', 1613716000, 'Rethinking API Versioning with Domain-Driven Design', '<p>How to manage breaking changes elegantly and get rid of version IDs for good by being nice to your clients.</p>', '<p>Change is inevitable. According to the <a href=\"https://agilemanifesto.org/principles.html\" target=\"_blank\">Agile manifesto</a>, we should welcome change. Is this also true for your API?</p>\r\n\r\n<p>Dealing with changes in APIs can be hard and there is no bullet-proof way of managing it. Whether you use URL path, parameter, or HTTP header versioning — all cause pretty much the same pain — the change still needs to be communicated back to the consumers. There is no technical solution for that. It is really a people problem.</p>\r\n\r\n<p>Another difficulty is the need to maintain multiple versions potentially forever.</p>\r\n\r\n<p>Is <i>such</i> a change really welcomed by developers? I doubt it.</p>\r\n\r\n<p>There are already attempts to make things better, like <a href=\"https://en.wikipedia.org/wiki/HATEOAS\" target=\"_blank\">HATEOAS</a>, but most of them are too impractical and/or difficult to develop.</p>\r\n\r\n\r\n<p>Although a shiny new approach by <a href=\"https://graphql.org\" target=\"_blank\">GraphQL</a> has been gaining popularity in the last few years, I believe that the simple plain REST will never die out.</p>\r\n\r\n<p>In this text, we will concentrate on endpoints such as:</p>\r\n\r\n<pre>\r\nGET /product/123\r\n{\r\n  \"name\": \"My Product\",\r\n  \"description\": \"Lorem ipsum...\",\r\n  \"price\": 14.5\r\n}\r\n</pre>\r\n\r\n<h2>What Is Wrong with APIs</h2>\r\n\r\n<p>Most Restful traditionally built APIs align a <em>resource</em> with a <em>data entity</em>.</p>\r\n\r\n<p>In the system that provides the <code>/product</code> resource from the example above, one can surely find a Restful controller <code>ProductController</code>, a data-transfer object <code>ProductDto</code>, a domain object <code>Product</code>, and of course, a database table <code>PRODUCTS</code>.</p>\r\n\r\n<p>This entity-driven system design practically means just exposing naked data to the client. In the worst case, even the raw database structure just as it is.</p>\r\n\r\n<p>This is unfortunate because data is usually very volatile. By exposing raw data to the client you basically let implementation details leak into the API.</p>\r\n\r\n<p>If the API is coupled to volatile details, the system becomes hard to change. Changes that are difficult to carry out are usually not very welcome.</p>\r\n\r\n<p>Instead of the data-driven approach, your <strong>API design should be behavior-driven</strong>. Don’t think about resources as mere data structures with particular CRUD operations. Rather, think about resources as the behavior of the system within particular use-cases.</p>\r\n\r\n<p>A rule of thumb would be the number of operations the client has to invoke to achieve a single goal being not greater than one.</p>\r\n\r\n<p>As Gregor Hohpe states in his Cloud Strategy: <i>“Don\'t build elaborate APIs that mimic the back-end system\'s design. Instead, build consumer-driven APIs that provide the service in a format that the front-ends prefer.”</i></p>\r\n\r\n<p>This is a piece of good advice. Not following it often leads to another big problem: <em>accidental coupling</em> between use-cases. Let me show you some example:</p>\r\n\r\n<pre>\r\nGET /product/123\r\n{\r\n  \"name\": \"My Product\",\r\n  \"description\": \"Lorem ipsum...\",\r\n  \"price\": 14.5,\r\n  \"availability\": \"soldout\",\r\n  \"delivery\": [\"PPL\", \"FedEx\"]\r\n}\r\n</pre>\r\n\r\n<p>In the data-driven API design, the producer must make assumptions about what to do in order to satisfy <i>all</i> consumers at once. In this particular case, the attributes <code>name</code>, <code>description</code>, and <code>price</code> are required by the Catalog page, while <code>availability</code> and <code>delivery</code> are needed only by the Checkout page.</p>\r\n\r\n<p>This approach not only forces clients to consume unneeded data and unnecessarily increase traffic. It also couples use-cases tightly to each other. Every change performed in order to satisfy the needs of one consumer will somehow impact all the others. If a consumer is not <a href=\"https://en.wikipedia.org/wiki/Robustness_principle\" target=\"_blank\">liberal</a> enough any minor unintentional change can break it.</p>\r\n\r\n<p class=\"note\">Postel’s law of conservative producers and liberal consumers also has its trade-offs. It makes the system more robust, but less precise and more complex (it breaks the separation of concerns).</p>\r\n\r\n<p>We can resolve the problem by adding a domain context (remember microservices?):</p>\r\n\r\n<pre>\r\nGET /catalog/product/123\r\n{\r\n  \"name\": \"My Product\",\r\n  \"description\": \"Lorem ipsum...\",\r\n  \"price\": 14.5\r\n}\r\n\r\nGET /checkout/product/123\r\n{\r\n  \"availability\": \"soldout\",\r\n  \"delivery\": [\"PPL\", \"FedEx\"]\r\n}\r\n</pre>\r\n\r\n<p>Now, as both use-cases are decoupled, each resource can be treated individually.</p>\r\n\r\n<p class=\"warning\">It is still wise to make changes with caution as existing users often use the system beyond its original design.</p>\r\n\r\n<h2>Evolutionary API Versioning</h2>\r\n\r\n<p>Roy Fielding, the author of REST, already gave us an <a href=\"https://www.infoq.com/articles/roy-fielding-on-versioning/\" target=\"_blank\">answer</a> to the question of how we can version our APIs: <i>“Don’t.”</i></p>\r\n\r\n<p>He explains: <i>“Versioning is used in ways that are informative rather than contractual.”</i> And further: <i>“REST is designed primarily to improve evolvability, the ability to change over time without starting over.”</i></p>\r\n\r\n<p>This makes sense, but the devil is always in the detail. How can we make our API <em>evolutionary</em>?</p>\r\n\r\n<p>Applying the Domain-driven design would be the first step. Moving away from data-centric APIs would mean considering endpoints such as:</p>\r\n\r\n<pre>\r\nGET /GetAllProducts\r\nGET /GetProductDetail/123\r\nGET /SearchProducts?name=...\r\n...\r\n</pre>\r\n\r\n<p>Say what you do, do what you say.</p>\r\n\r\n<p>How can it help us with managing breaking changes? When we are honest about our endpoints we can <strong>make the change explicit</strong>.</p>\r\n\r\n<p>Well, there is always some business motivation behind every change. If we communicate this motivation to the clients, they will gain better understanding of why the change was done and what it will bring them if they switch.</p>\r\n\r\n<pre>\r\nGET /GetProductsAtDiscount\r\nGET /GetPromoProducts\r\n...\r\n</pre>\r\n\r\n<p>Understanding the benefits will make the clients <i>want</i> to switch. Consider these two possibilities:</p>\r\n\r\n<pre>\r\n/v1/ponies => /v2/ponies\r\n</pre>\r\n\r\n<p>or</p>\r\n\r\n<pre>\r\n/ponies => /unicorns\r\n</pre>\r\n\r\n<p>Which one would be most tempting for users to switch to? I guess you get the point &#x1F984;</p>\r\n\r\n<p>Now, you can finally get rid of version IDs for good. Being nice to your clients pays off!</p>\r\n\r\n<p>One more thing. Breaking changes are sometimes inevitable; however, it should be a last resort.</p>\r\n\r\n<p>If you have to introduce breaking changes every time you release a new feature then you probably have a bigger problem to fix: <em>wrong abstraction</em>.</p>\r\n\r\n<p>A good API reflects its <em>purpose</em> well. Purpose tends to be stable and as such it is the best foundation for designing stable APIs.</p>\r\n\r\n<h2>Summary</h2>\r\n\r\n<p>There is no simple technical solution for API versioning. Instead of looking for one, we can apply the same principles of well-structured, decoupled, and maintainable software architectures:</p>\r\n\r\n<ul>\r\n  <li>A good API reflects its purpose.</li>\r\n  <li>Apply the Domain-driven design to your APIs.</li>\r\n  <li>Say what you do, do what you say.</li>\r\n  <li>Make the change explicit.</li>\r\n  <li>Don’t break your API, evolve it.</li>\r\n</ul>\r\n\r\n<p>Happy resting!</p>\r\n', 'API Design,DDD,REST', 'false', 'false', 1, 2);

-- --------------------------------------------------------

--
-- Table structure for table `Property`
--

DROP TABLE IF EXISTS `Property`;
CREATE TABLE `Property` (
  `name` varchar(50) NOT NULL,
  `value` varchar(100) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Dumping data for table `Property`
--

INSERT INTO `Property` (`name`, `value`) VALUES
('blogAuthor', 'Tomas Tulka - NET21 s.r.o.'),
('blogDescription', 'Random thoughts about programming and other stuff.'),
('blogTitle', 'Tomas Tulka\'s Blog');

--
-- Indexes for dumped tables
--

--
-- Indexes for table `Author`
--
ALTER TABLE `Author`
  ADD PRIMARY KEY (`id`);

--
-- Indexes for table `Category`
--
ALTER TABLE `Category`
  ADD PRIMARY KEY (`id`);

--
-- Indexes for table `Comment`
--
ALTER TABLE `Comment`
  ADD PRIMARY KEY (`id`),
  ADD KEY `comment_parent_idx` (`parentId`),
  ADD KEY `comment_post_idx` (`postId`);

--
-- Indexes for table `Post`
--
ALTER TABLE `Post`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `url` (`url`),
  ADD KEY `post_author_idx` (`authorId`),
  ADD KEY `post_category_idx` (`categoryId`);

--
-- Indexes for table `Property`
--
ALTER TABLE `Property`
  ADD PRIMARY KEY (`name`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `Author`
--
ALTER TABLE `Author`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=2;

--
-- AUTO_INCREMENT for table `Category`
--
ALTER TABLE `Category`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=3;

--
-- AUTO_INCREMENT for table `Comment`
--
ALTER TABLE `Comment`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=46;

--
-- AUTO_INCREMENT for table `Post`
--
ALTER TABLE `Post`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=104;

--
-- Constraints for dumped tables
--

--
-- Constraints for table `Comment`
--
ALTER TABLE `Comment`
  ADD CONSTRAINT `Comment_ibfk_1` FOREIGN KEY (`parentId`) REFERENCES `Comment` (`id`),
  ADD CONSTRAINT `Comment_ibfk_2` FOREIGN KEY (`postId`) REFERENCES `Post` (`id`);

--
-- Constraints for table `Post`
--
ALTER TABLE `Post`
  ADD CONSTRAINT `Post_ibfk_1` FOREIGN KEY (`authorId`) REFERENCES `Author` (`id`),
  ADD CONSTRAINT `Post_ibfk_2` FOREIGN KEY (`categoryId`) REFERENCES `Category` (`id`);
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
